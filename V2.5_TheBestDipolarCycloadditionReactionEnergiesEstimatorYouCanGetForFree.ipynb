{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2QLEtu33YX7",
        "outputId": "d05f1835-122f-47c2-aacf-c6e42ce6415f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rdkit\n",
            "  Downloading rdkit-2023.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.7/29.7 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.22.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (8.4.0)\n",
            "Installing collected packages: rdkit\n",
            "Successfully installed rdkit-2023.3.1\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install pandas\n",
        "!{sys.executable} -m pip install numpy\n",
        "!{sys.executable} -m pip install scikit-learn\n",
        "!{sys.executable} -m pip install torch\n",
        "!{sys.executable} -m pip install rdkit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "x8a_Hg7L3YYA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FSSRLOFl3gz_"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-sF2YeIG3YYB"
      },
      "outputs": [],
      "source": [
        "# Define a Reaction dataset class\n",
        "class ReactionDataset(Dataset):\n",
        "    def __init__(self, X, y_G_act, y_G_r):\n",
        "        self.X = X\n",
        "        self.y_G_act = y_G_act\n",
        "        self.y_G_r = y_G_r\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y_G_act[idx], self.y_G_r[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BDMUIgY93YYC"
      },
      "outputs": [],
      "source": [
        "# Read the dataset\n",
        "df = pd.read_csv('full_dataset.csv') \n",
        "\n",
        "# Prepare the feature matrix X and target variables y\n",
        "X_smiles = df['rxn_smiles']\n",
        "y_G_act = df['G_act']\n",
        "y_G_r = df['G_r']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yM-TMpZ53YYE",
        "outputId": "8810f82d-04b7-4fba-cbfc-f80aaac30084"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "torch.Size([5269, 3072])\n"
          ]
        }
      ],
      "source": [
        "# Define the encoding function\n",
        "def encode_smiles(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=1024)\n",
        "    bitstring = fp.ToBitString()\n",
        "    features = np.array([int(bit) for bit in bitstring], dtype=np.float32)\n",
        "    features_tensor = torch.tensor(features)\n",
        "    return features_tensor\n",
        "\n",
        "# Encode SMILES strings to fingerprints\n",
        "X_fingerprints = []\n",
        "\n",
        "for idx, smiles in enumerate(X_smiles):\n",
        "    reactants, products = smiles.split('>>')\n",
        "    reactant_fingerprints = [encode_smiles(reactant) for reactant in reactants.split('.')]\n",
        "    product_fingerprints = [encode_smiles(product) for product in products.split('.')]\n",
        "    X_fingerprints.append(reactant_fingerprints + product_fingerprints)\n",
        "\n",
        "# Convert the list of fingerprints and target values to tensors\n",
        "X_tensor = torch.stack([torch.cat(fingerprints) for fingerprints in X_fingerprints]).to(device)\n",
        "\n",
        "# Print the fingerprint tensor\n",
        "print(X_tensor)\n",
        "print(X_tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HjrjTdBg3YYF"
      },
      "outputs": [],
      "source": [
        "# Convert the data into PyTorch tensors\n",
        "y_G_act_tensor = torch.tensor(y_G_act.values, dtype=torch.float32).to(device)\n",
        "y_G_r_tensor = torch.tensor(y_G_r.values, dtype=torch.float32).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gyy7_PpA3YYG"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_G_act_train, y_G_act_test, y_G_r_train, y_G_r_test = train_test_split(X_tensor, y_G_act_tensor, y_G_r_tensor, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iIVe753c3YYH"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.dropout3 = nn.Dropout(0.2)\n",
        "        self.fc4_act = nn.Linear(64, 1)  # Output layer for G_act\n",
        "        self.fc4_r = nn.Linear(64, 1)  # Output layer for G_r\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.dropout3(x)\n",
        "        output_act = self.fc4_act(x)  # Output for G_act\n",
        "        output_r = self.fc4_r(x)  # Output for G_r\n",
        "        return output_act, output_r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buKX3MD_3YYI",
        "outputId": "a0fb067d-4485-438a-c58f-d3090c2b8122"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "NeuralNetwork(\n",
              "  (fc1): Linear(in_features=3072, out_features=128, bias=True)\n",
              "  (relu1): ReLU()\n",
              "  (dropout1): Dropout(p=0.2, inplace=False)\n",
              "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (relu2): ReLU()\n",
              "  (dropout2): Dropout(p=0.2, inplace=False)\n",
              "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (relu3): ReLU()\n",
              "  (dropout3): Dropout(p=0.2, inplace=False)\n",
              "  (fc4_act): Linear(in_features=64, out_features=1, bias=True)\n",
              "  (fc4_r): Linear(in_features=64, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Instantiate the model\n",
        "model = NeuralNetwork(X_train.shape[1])\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "vZV6PXeL3YYJ"
      },
      "outputs": [],
      "source": [
        "# Define the loss function and optimizer\n",
        "criterion = nn.SmoothL1Loss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "XkQm_Fh93YYK"
      },
      "outputs": [],
      "source": [
        "# Define the data loaders\n",
        "train_dataset = ReactionDataset(X_train, y_G_act_train, y_G_r_train)\n",
        "test_dataset = ReactionDataset(X_test, y_G_act_test, y_G_r_test)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_0Jhuh33YYL",
        "outputId": "464e56c5-0725-46e2-a76d-4416b7d7a797"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000, Loss: 4.703514603051272\n",
            "Epoch 2/1000, Loss: 4.525348285833995\n",
            "Epoch 3/1000, Loss: 4.467112788648317\n",
            "Epoch 4/1000, Loss: 4.434290223049395\n",
            "Epoch 5/1000, Loss: 4.379505493424156\n",
            "Epoch 6/1000, Loss: 4.372170851086125\n",
            "Epoch 7/1000, Loss: 4.329987495234518\n",
            "Epoch 8/1000, Loss: 4.377725120746728\n",
            "Epoch 9/1000, Loss: 4.320981148517493\n",
            "Epoch 10/1000, Loss: 4.245370138775218\n",
            "Epoch 11/1000, Loss: 4.192886227911169\n",
            "Epoch 12/1000, Loss: 4.216735446091854\n",
            "Epoch 13/1000, Loss: 4.143590708573659\n",
            "Epoch 14/1000, Loss: 4.145339216246749\n",
            "Epoch 15/1000, Loss: 4.0558641408429\n",
            "Epoch 16/1000, Loss: 4.072325966574929\n",
            "Epoch 17/1000, Loss: 4.140241066614787\n",
            "Epoch 18/1000, Loss: 4.085428750876225\n",
            "Epoch 19/1000, Loss: 4.06447248025374\n",
            "Epoch 20/1000, Loss: 4.148945943875746\n",
            "Epoch 21/1000, Loss: 4.015003953919266\n",
            "Epoch 22/1000, Loss: 4.0000294121828945\n",
            "Epoch 23/1000, Loss: 4.027165402065624\n",
            "Epoch 24/1000, Loss: 4.056331891002077\n",
            "Epoch 25/1000, Loss: 3.9512478889841023\n",
            "Epoch 26/1000, Loss: 3.9583747404994387\n",
            "Epoch 27/1000, Loss: 3.968171654325543\n",
            "Epoch 28/1000, Loss: 3.861735893018318\n",
            "Epoch 29/1000, Loss: 3.9655996506864373\n",
            "Epoch 30/1000, Loss: 3.8838876792878816\n",
            "Epoch 31/1000, Loss: 3.8492049484541924\n",
            "Epoch 32/1000, Loss: 3.807445578502886\n",
            "Epoch 33/1000, Loss: 3.960611838282961\n",
            "Epoch 34/1000, Loss: 3.8846205906434492\n",
            "Epoch 35/1000, Loss: 3.923027369109067\n",
            "Epoch 36/1000, Loss: 3.8686614921598723\n",
            "Epoch 37/1000, Loss: 3.947254074342323\n",
            "Epoch 38/1000, Loss: 3.8424309492111206\n",
            "Epoch 39/1000, Loss: 3.898464419625022\n",
            "Epoch 40/1000, Loss: 3.815402202533953\n",
            "Epoch 41/1000, Loss: 3.915728408278841\n",
            "Epoch 42/1000, Loss: 3.7975164001638237\n",
            "Epoch 43/1000, Loss: 3.8721300002300376\n",
            "Epoch 44/1000, Loss: 3.8022295489455713\n",
            "Epoch 45/1000, Loss: 3.8098254312168467\n",
            "Epoch 46/1000, Loss: 3.755158933726224\n",
            "Epoch 47/1000, Loss: 3.782543202241262\n",
            "Epoch 48/1000, Loss: 3.8305280624013958\n",
            "Epoch 49/1000, Loss: 3.7469468008388174\n",
            "Epoch 50/1000, Loss: 3.775864019538417\n",
            "Epoch 51/1000, Loss: 3.77359653783567\n",
            "Epoch 52/1000, Loss: 3.7671997800017847\n",
            "Epoch 53/1000, Loss: 3.8167167974240854\n",
            "Epoch 54/1000, Loss: 3.714940672571009\n",
            "Epoch 55/1000, Loss: 3.687503426363974\n",
            "Epoch 56/1000, Loss: 3.7364274100823835\n",
            "Epoch 57/1000, Loss: 3.7121953052101713\n",
            "Epoch 58/1000, Loss: 3.6073680772925867\n",
            "Epoch 59/1000, Loss: 3.7194503274830906\n",
            "Epoch 60/1000, Loss: 3.717176377773285\n",
            "Epoch 61/1000, Loss: 3.7067369493571194\n",
            "Epoch 62/1000, Loss: 3.6545922900691177\n",
            "Epoch 63/1000, Loss: 3.6108242219144646\n",
            "Epoch 64/1000, Loss: 3.741762632673437\n",
            "Epoch 65/1000, Loss: 3.7182835828174245\n",
            "Epoch 66/1000, Loss: 3.663786122293183\n",
            "Epoch 67/1000, Loss: 3.692743514523362\n",
            "Epoch 68/1000, Loss: 3.6001378640984045\n",
            "Epoch 69/1000, Loss: 3.6785961570161763\n",
            "Epoch 70/1000, Loss: 3.6758812882683496\n",
            "Epoch 71/1000, Loss: 3.6644928383104727\n",
            "Epoch 72/1000, Loss: 3.599732628374389\n",
            "Epoch 73/1000, Loss: 3.7353536161509426\n",
            "Epoch 74/1000, Loss: 3.660457954262242\n",
            "Epoch 75/1000, Loss: 3.6120178157633003\n",
            "Epoch 76/1000, Loss: 3.67522221623045\n",
            "Epoch 77/1000, Loss: 3.553751456015038\n",
            "Epoch 78/1000, Loss: 3.6204383860934866\n",
            "Epoch 79/1000, Loss: 3.6152248870242727\n",
            "Epoch 80/1000, Loss: 3.5420360438751453\n",
            "Epoch 81/1000, Loss: 3.6234886971386997\n",
            "Epoch 82/1000, Loss: 3.575144946575165\n",
            "Epoch 83/1000, Loss: 3.614050310669523\n",
            "Epoch 84/1000, Loss: 3.540017433238752\n",
            "Epoch 85/1000, Loss: 3.6396700035442007\n",
            "Epoch 86/1000, Loss: 3.5732996355403555\n",
            "Epoch 87/1000, Loss: 3.5467700886003897\n",
            "Epoch 88/1000, Loss: 3.629636965014718\n",
            "Epoch 89/1000, Loss: 3.5837604719581027\n",
            "Epoch 90/1000, Loss: 3.523848388231162\n",
            "Epoch 91/1000, Loss: 3.520753654566678\n",
            "Epoch 92/1000, Loss: 3.5719043612480164\n",
            "Epoch 93/1000, Loss: 3.551776844443697\n",
            "Epoch 94/1000, Loss: 3.540848676002387\n",
            "Epoch 95/1000, Loss: 3.4962240638154927\n",
            "Epoch 96/1000, Loss: 3.5702095808404866\n",
            "Epoch 97/1000, Loss: 3.4632635188825205\n",
            "Epoch 98/1000, Loss: 3.46120303327387\n",
            "Epoch 99/1000, Loss: 3.5072184963659807\n",
            "Epoch 100/1000, Loss: 3.506562854304458\n",
            "Epoch 101/1000, Loss: 3.578018408833128\n",
            "Epoch 102/1000, Loss: 3.5064435781854573\n",
            "Epoch 103/1000, Loss: 3.511895867911252\n",
            "Epoch 104/1000, Loss: 3.478558179103967\n",
            "Epoch 105/1000, Loss: 3.505288252324769\n",
            "Epoch 106/1000, Loss: 3.4624182744459673\n",
            "Epoch 107/1000, Loss: 3.4547497449499187\n",
            "Epoch 108/1000, Loss: 3.5346169634298845\n",
            "Epoch 109/1000, Loss: 3.5257075302528613\n",
            "Epoch 110/1000, Loss: 3.518428683280945\n",
            "Epoch 111/1000, Loss: 3.480785967725696\n",
            "Epoch 112/1000, Loss: 3.481767475605011\n",
            "Epoch 113/1000, Loss: 3.4874915155497463\n",
            "Epoch 114/1000, Loss: 3.445115181532773\n",
            "Epoch 115/1000, Loss: 3.460480635816401\n",
            "Epoch 116/1000, Loss: 3.5092406850872617\n",
            "Epoch 117/1000, Loss: 3.4605006431088303\n",
            "Epoch 118/1000, Loss: 3.4562036395072937\n",
            "Epoch 119/1000, Loss: 3.4273544531880002\n",
            "Epoch 120/1000, Loss: 3.418817971691941\n",
            "Epoch 121/1000, Loss: 3.4953586820400124\n",
            "Epoch 122/1000, Loss: 3.443462787252484\n",
            "Epoch 123/1000, Loss: 3.42944785862258\n",
            "Epoch 124/1000, Loss: 3.434801439444224\n",
            "Epoch 125/1000, Loss: 3.5420034166538352\n",
            "Epoch 126/1000, Loss: 3.442028627251134\n",
            "Epoch 127/1000, Loss: 3.500121869824149\n",
            "Epoch 128/1000, Loss: 3.5122353741616914\n",
            "Epoch 129/1000, Loss: 3.468637152151628\n",
            "Epoch 130/1000, Loss: 3.4851309169422495\n",
            "Epoch 131/1000, Loss: 3.4717153563643945\n",
            "Epoch 132/1000, Loss: 3.3806820710500083\n",
            "Epoch 133/1000, Loss: 3.450256838943019\n",
            "Epoch 134/1000, Loss: 3.4131798834511726\n",
            "Epoch 135/1000, Loss: 3.4494876355835884\n",
            "Epoch 136/1000, Loss: 3.4607489398031523\n",
            "Epoch 137/1000, Loss: 3.441109333977555\n",
            "Epoch 138/1000, Loss: 3.427623212337494\n",
            "Epoch 139/1000, Loss: 3.4636948722781558\n",
            "Epoch 140/1000, Loss: 3.459443439136852\n",
            "Epoch 141/1000, Loss: 3.4665029283725852\n",
            "Epoch 142/1000, Loss: 3.4144508531599334\n",
            "Epoch 143/1000, Loss: 3.428356474096125\n",
            "Epoch 144/1000, Loss: 3.417600539597598\n",
            "Epoch 145/1000, Loss: 3.4123318755265437\n",
            "Epoch 146/1000, Loss: 3.396037692373449\n",
            "Epoch 147/1000, Loss: 3.3934700272300025\n",
            "Epoch 148/1000, Loss: 3.3956496462677466\n",
            "Epoch 149/1000, Loss: 3.375191833033706\n",
            "Epoch 150/1000, Loss: 3.413570447401567\n",
            "Epoch 151/1000, Loss: 3.4524891430681404\n",
            "Epoch 152/1000, Loss: 3.414630716497248\n",
            "Epoch 153/1000, Loss: 3.4620432817574702\n",
            "Epoch 154/1000, Loss: 3.3290032003865098\n",
            "Epoch 155/1000, Loss: 3.440747985334107\n",
            "Epoch 156/1000, Loss: 3.3892956123207556\n",
            "Epoch 157/1000, Loss: 3.334239229108348\n",
            "Epoch 158/1000, Loss: 3.3961041515523736\n",
            "Epoch 159/1000, Loss: 3.36130673235113\n",
            "Epoch 160/1000, Loss: 3.3898162336060493\n",
            "Epoch 161/1000, Loss: 3.375128796606353\n",
            "Epoch 162/1000, Loss: 3.5075668782898872\n",
            "Epoch 163/1000, Loss: 3.444666329658393\n",
            "Epoch 164/1000, Loss: 3.434944895180789\n",
            "Epoch 165/1000, Loss: 3.374726860812216\n",
            "Epoch 166/1000, Loss: 3.454965311469454\n",
            "Epoch 167/1000, Loss: 3.4421336867592554\n",
            "Epoch 168/1000, Loss: 3.359796256730051\n",
            "Epoch 169/1000, Loss: 3.363946921897657\n",
            "Epoch 170/1000, Loss: 3.3404809922883003\n",
            "Epoch 171/1000, Loss: 3.3019237554434575\n",
            "Epoch 172/1000, Loss: 3.4663101091529382\n",
            "Epoch 173/1000, Loss: 3.3513355146754873\n",
            "Epoch 174/1000, Loss: 3.367996228463722\n",
            "Epoch 175/1000, Loss: 3.3537656231360002\n",
            "Epoch 176/1000, Loss: 3.369049765846946\n",
            "Epoch 177/1000, Loss: 3.3920516859401357\n",
            "Epoch 178/1000, Loss: 3.367961408513965\n",
            "Epoch 179/1000, Loss: 3.358760517654997\n",
            "Epoch 180/1000, Loss: 3.3057076172395186\n",
            "Epoch 181/1000, Loss: 3.3140305533553613\n",
            "Epoch 182/1000, Loss: 3.344127443703738\n",
            "Epoch 183/1000, Loss: 3.3922164982015435\n",
            "Epoch 184/1000, Loss: 3.3193170103159817\n",
            "Epoch 185/1000, Loss: 3.275863400011352\n",
            "Epoch 186/1000, Loss: 3.3843924294818533\n",
            "Epoch 187/1000, Loss: 3.357401526335514\n",
            "Epoch 188/1000, Loss: 3.4307686144655403\n",
            "Epoch 189/1000, Loss: 3.311979243249604\n",
            "Epoch 190/1000, Loss: 3.353960324417461\n",
            "Epoch 191/1000, Loss: 3.3024875593907908\n",
            "Epoch 192/1000, Loss: 3.3425342639287314\n",
            "Epoch 193/1000, Loss: 3.3214069695183723\n",
            "Epoch 194/1000, Loss: 3.308289829528693\n",
            "Epoch 195/1000, Loss: 3.2436367490074853\n",
            "Epoch 196/1000, Loss: 3.344868114500335\n",
            "Epoch 197/1000, Loss: 3.3314190452749077\n",
            "Epoch 198/1000, Loss: 3.3078044472318706\n",
            "Epoch 199/1000, Loss: 3.3798005671212166\n",
            "Epoch 200/1000, Loss: 3.2794709332061536\n",
            "Epoch 201/1000, Loss: 3.320424283092672\n",
            "Epoch 202/1000, Loss: 3.2764969179124543\n",
            "Epoch 203/1000, Loss: 3.3012644467931804\n",
            "Epoch 204/1000, Loss: 3.3704183155840095\n",
            "Epoch 205/1000, Loss: 3.3565357345523257\n",
            "Epoch 206/1000, Loss: 3.2863333423932395\n",
            "Epoch 207/1000, Loss: 3.298608801581643\n",
            "Epoch 208/1000, Loss: 3.3231178413737905\n",
            "Epoch 209/1000, Loss: 3.2938682364694998\n",
            "Epoch 210/1000, Loss: 3.2849527687737434\n",
            "Epoch 211/1000, Loss: 3.3455541693803035\n",
            "Epoch 212/1000, Loss: 3.2708280366478544\n",
            "Epoch 213/1000, Loss: 3.2817664886965896\n",
            "Epoch 214/1000, Loss: 3.305787326711597\n",
            "Epoch 215/1000, Loss: 3.308505020358346\n",
            "Epoch 216/1000, Loss: 3.3430712313363045\n",
            "Epoch 217/1000, Loss: 3.330347482002143\n",
            "Epoch 218/1000, Loss: 3.2938012011123425\n",
            "Epoch 219/1000, Loss: 3.2564347648259364\n",
            "Epoch 220/1000, Loss: 3.287403354139039\n",
            "Epoch 221/1000, Loss: 3.3081092310674265\n",
            "Epoch 222/1000, Loss: 3.2429238774559717\n",
            "Epoch 223/1000, Loss: 3.3092945525140474\n",
            "Epoch 224/1000, Loss: 3.290018273122383\n",
            "Epoch 225/1000, Loss: 3.3601511879400774\n",
            "Epoch 226/1000, Loss: 3.24717666163589\n",
            "Epoch 227/1000, Loss: 3.2517031069957847\n",
            "Epoch 228/1000, Loss: 3.2557558543754346\n",
            "Epoch 229/1000, Loss: 3.376717068932273\n",
            "Epoch 230/1000, Loss: 3.3593007109381934\n",
            "Epoch 231/1000, Loss: 3.2559136152267456\n",
            "Epoch 232/1000, Loss: 3.314990365143978\n",
            "Epoch 233/1000, Loss: 3.248199023983695\n",
            "Epoch 234/1000, Loss: 3.246683023192666\n",
            "Epoch 235/1000, Loss: 3.27935322486993\n",
            "Epoch 236/1000, Loss: 3.338381973179904\n",
            "Epoch 237/1000, Loss: 3.276857852935791\n",
            "Epoch 238/1000, Loss: 3.248466659675945\n",
            "Epoch 239/1000, Loss: 3.304998174761281\n",
            "Epoch 240/1000, Loss: 3.205938462055091\n",
            "Epoch 241/1000, Loss: 3.264296231847821\n",
            "Epoch 242/1000, Loss: 3.288535036823966\n",
            "Epoch 243/1000, Loss: 3.1906405289967856\n",
            "Epoch 244/1000, Loss: 3.2399129632747536\n",
            "Epoch 245/1000, Loss: 3.1595017837755606\n",
            "Epoch 246/1000, Loss: 3.243963228933739\n",
            "Epoch 247/1000, Loss: 3.2988484863078957\n",
            "Epoch 248/1000, Loss: 3.2177359588218457\n",
            "Epoch 249/1000, Loss: 3.2312138532147263\n",
            "Epoch 250/1000, Loss: 3.294173184669379\n",
            "Epoch 251/1000, Loss: 3.2188067400094234\n",
            "Epoch 252/1000, Loss: 3.260211944580078\n",
            "Epoch 253/1000, Loss: 3.265279494451754\n",
            "Epoch 254/1000, Loss: 3.33821461056218\n",
            "Epoch 255/1000, Loss: 3.234891597068671\n",
            "Epoch 256/1000, Loss: 3.2433381315433616\n",
            "Epoch 257/1000, Loss: 3.249683521010659\n",
            "Epoch 258/1000, Loss: 3.2404465368299773\n",
            "Epoch 259/1000, Loss: 3.299278797525348\n",
            "Epoch 260/1000, Loss: 3.1961484027631357\n",
            "Epoch 261/1000, Loss: 3.2603304566759053\n",
            "Epoch 262/1000, Loss: 3.272382393027797\n",
            "Epoch 263/1000, Loss: 3.239196105436845\n",
            "Epoch 264/1000, Loss: 3.2812678470756067\n",
            "Epoch 265/1000, Loss: 3.243554545171333\n",
            "Epoch 266/1000, Loss: 3.305016167236097\n",
            "Epoch 267/1000, Loss: 3.211701208894903\n",
            "Epoch 268/1000, Loss: 3.3049021489692456\n",
            "Epoch 269/1000, Loss: 3.2396953412980745\n",
            "Epoch 270/1000, Loss: 3.2164056319178957\n",
            "Epoch 271/1000, Loss: 3.270504467415087\n",
            "Epoch 272/1000, Loss: 3.22842344009515\n",
            "Epoch 273/1000, Loss: 3.2171383355603074\n",
            "Epoch 274/1000, Loss: 3.164274109132362\n",
            "Epoch 275/1000, Loss: 3.1991661209048647\n",
            "Epoch 276/1000, Loss: 3.21363667827664\n",
            "Epoch 277/1000, Loss: 3.238770873257608\n",
            "Epoch 278/1000, Loss: 3.260118740977663\n",
            "Epoch 279/1000, Loss: 3.2580347476583538\n",
            "Epoch 280/1000, Loss: 3.2716536034237254\n",
            "Epoch 281/1000, Loss: 3.268656419985222\n",
            "Epoch 282/1000, Loss: 3.2268657991380403\n",
            "Epoch 283/1000, Loss: 3.2593400568673103\n",
            "Epoch 284/1000, Loss: 3.211519653146917\n",
            "Epoch 285/1000, Loss: 3.2175491867643413\n",
            "Epoch 286/1000, Loss: 3.1963209907213845\n",
            "Epoch 287/1000, Loss: 3.180420330076507\n",
            "Epoch 288/1000, Loss: 3.1854337200973974\n",
            "Epoch 289/1000, Loss: 3.2238730598579752\n",
            "Epoch 290/1000, Loss: 3.2045460081461705\n",
            "Epoch 291/1000, Loss: 3.2793863144787876\n",
            "Epoch 292/1000, Loss: 3.277006198059429\n",
            "Epoch 293/1000, Loss: 3.213907660860004\n",
            "Epoch 294/1000, Loss: 3.21372295329065\n",
            "Epoch 295/1000, Loss: 3.1873072078733733\n",
            "Epoch 296/1000, Loss: 3.206620357253335\n",
            "Epoch 297/1000, Loss: 3.1992707387967543\n",
            "Epoch 298/1000, Loss: 3.2213901642597085\n",
            "Epoch 299/1000, Loss: 3.142024267803539\n",
            "Epoch 300/1000, Loss: 3.2208329312729114\n",
            "Epoch 301/1000, Loss: 3.2063476316856616\n",
            "Epoch 302/1000, Loss: 3.2404411308693164\n",
            "Epoch 303/1000, Loss: 3.2142158060362847\n",
            "Epoch 304/1000, Loss: 3.1332807793761743\n",
            "Epoch 305/1000, Loss: 3.1836286322637037\n",
            "Epoch 306/1000, Loss: 3.243530267115795\n",
            "Epoch 307/1000, Loss: 3.164752545681867\n",
            "Epoch 308/1000, Loss: 3.1535085141658783\n",
            "Epoch 309/1000, Loss: 3.204313023523851\n",
            "Epoch 310/1000, Loss: 3.186505391742244\n",
            "Epoch 311/1000, Loss: 3.134796173283548\n",
            "Epoch 312/1000, Loss: 3.2162129454540485\n",
            "Epoch 313/1000, Loss: 3.26110789270112\n",
            "Epoch 314/1000, Loss: 3.1529513290434172\n",
            "Epoch 315/1000, Loss: 3.1790831341887964\n",
            "Epoch 316/1000, Loss: 3.240108361749938\n",
            "Epoch 317/1000, Loss: 3.1332435029925723\n",
            "Epoch 318/1000, Loss: 3.2224154779405305\n",
            "Epoch 319/1000, Loss: 3.253665209719629\n",
            "Epoch 320/1000, Loss: 3.195302309411945\n",
            "Epoch 321/1000, Loss: 3.2131010727448897\n",
            "Epoch 322/1000, Loss: 3.260761074947588\n",
            "Epoch 323/1000, Loss: 3.1306185776537117\n",
            "Epoch 324/1000, Loss: 3.149222381187208\n",
            "Epoch 325/1000, Loss: 3.280238910154863\n",
            "Epoch 326/1000, Loss: 3.212574008739356\n",
            "Epoch 327/1000, Loss: 3.2598747592983823\n",
            "Epoch 328/1000, Loss: 3.2343140948902476\n",
            "Epoch 329/1000, Loss: 3.193463641585726\n",
            "Epoch 330/1000, Loss: 3.1904650005427273\n",
            "Epoch 331/1000, Loss: 3.14823160478563\n",
            "Epoch 332/1000, Loss: 3.2326972340092515\n",
            "Epoch 333/1000, Loss: 3.1420373058680333\n",
            "Epoch 334/1000, Loss: 3.167396093859817\n",
            "Epoch 335/1000, Loss: 3.161488700996746\n",
            "Epoch 336/1000, Loss: 3.22274044788245\n",
            "Epoch 337/1000, Loss: 3.151856648199486\n",
            "Epoch 338/1000, Loss: 3.1622977455457053\n",
            "Epoch 339/1000, Loss: 3.1242669567917334\n",
            "Epoch 340/1000, Loss: 3.1973505182699724\n",
            "Epoch 341/1000, Loss: 3.199634151025252\n",
            "Epoch 342/1000, Loss: 3.183803421981407\n",
            "Epoch 343/1000, Loss: 3.1837362863800744\n",
            "Epoch 344/1000, Loss: 3.180370355194265\n",
            "Epoch 345/1000, Loss: 3.201428650003491\n",
            "Epoch 346/1000, Loss: 3.1733900435043103\n",
            "Epoch 347/1000, Loss: 3.177902146722331\n",
            "Epoch 348/1000, Loss: 3.196681736093579\n",
            "Epoch 349/1000, Loss: 3.167736434575283\n",
            "Epoch 350/1000, Loss: 3.140190758488395\n",
            "Epoch 351/1000, Loss: 3.1942037307854854\n",
            "Epoch 352/1000, Loss: 3.1684084823637297\n",
            "Epoch 353/1000, Loss: 3.256319640260754\n",
            "Epoch 354/1000, Loss: 3.1659121079878374\n",
            "Epoch 355/1000, Loss: 3.204779461477742\n",
            "Epoch 356/1000, Loss: 3.173096924117117\n",
            "Epoch 357/1000, Loss: 3.090922599489039\n",
            "Epoch 358/1000, Loss: 3.198999580108758\n",
            "Epoch 359/1000, Loss: 3.153341700633367\n",
            "Epoch 360/1000, Loss: 3.1154028059858265\n",
            "Epoch 361/1000, Loss: 3.1762020804665307\n",
            "Epoch 362/1000, Loss: 3.2026496999191516\n",
            "Epoch 363/1000, Loss: 3.10947599736127\n",
            "Epoch 364/1000, Loss: 3.164517800013224\n",
            "Epoch 365/1000, Loss: 3.1218306927969963\n",
            "Epoch 366/1000, Loss: 3.1972355102047776\n",
            "Epoch 367/1000, Loss: 3.1380031126918215\n",
            "Epoch 368/1000, Loss: 3.2071658517375137\n",
            "Epoch 369/1000, Loss: 3.095864404331554\n",
            "Epoch 370/1000, Loss: 3.2004370671330076\n",
            "Epoch 371/1000, Loss: 3.1489043660236127\n",
            "Epoch 372/1000, Loss: 3.192598109895533\n",
            "Epoch 373/1000, Loss: 3.1535000421784143\n",
            "Epoch 374/1000, Loss: 3.124759013002569\n",
            "Epoch 375/1000, Loss: 3.1525363885995112\n",
            "Epoch 376/1000, Loss: 3.1764129156416114\n",
            "Epoch 377/1000, Loss: 3.1056547851273506\n",
            "Epoch 378/1000, Loss: 3.1348822152975835\n",
            "Epoch 379/1000, Loss: 3.137590112108173\n",
            "Epoch 380/1000, Loss: 3.13335346994978\n",
            "Epoch 381/1000, Loss: 3.2453192607922987\n",
            "Epoch 382/1000, Loss: 3.1368367870648703\n",
            "Epoch 383/1000, Loss: 3.1748397567055444\n",
            "Epoch 384/1000, Loss: 3.165300280758829\n",
            "Epoch 385/1000, Loss: 3.2134462721420056\n",
            "Epoch 386/1000, Loss: 3.1237760944799944\n",
            "Epoch 387/1000, Loss: 3.1572321111505683\n",
            "Epoch 388/1000, Loss: 3.087696825916117\n",
            "Epoch 389/1000, Loss: 3.1623059168006433\n",
            "Epoch 390/1000, Loss: 3.127054750919342\n",
            "Epoch 391/1000, Loss: 3.201340335788149\n",
            "Epoch 392/1000, Loss: 3.1469347793044466\n",
            "Epoch 393/1000, Loss: 3.143398919791886\n",
            "Epoch 394/1000, Loss: 3.1155181870316015\n",
            "Epoch 395/1000, Loss: 3.1516584049571645\n",
            "Epoch 396/1000, Loss: 3.1119896173477173\n",
            "Epoch 397/1000, Loss: 3.140189292755994\n",
            "Epoch 398/1000, Loss: 3.1945621010028953\n",
            "Epoch 399/1000, Loss: 3.098247838742805\n",
            "Epoch 400/1000, Loss: 3.1470947283687014\n",
            "Epoch 401/1000, Loss: 3.153975074038361\n",
            "Epoch 402/1000, Loss: 3.118760945218982\n",
            "Epoch 403/1000, Loss: 3.2379398689125525\n",
            "Epoch 404/1000, Loss: 3.2022055062380703\n",
            "Epoch 405/1000, Loss: 3.2278609889926333\n",
            "Epoch 406/1000, Loss: 3.1050387160344557\n",
            "Epoch 407/1000, Loss: 3.1282480250705373\n",
            "Epoch 408/1000, Loss: 3.0866173072294756\n",
            "Epoch 409/1000, Loss: 3.157821713071881\n",
            "Epoch 410/1000, Loss: 3.171959226781672\n",
            "Epoch 411/1000, Loss: 3.1180083814895516\n",
            "Epoch 412/1000, Loss: 3.1153776519226306\n",
            "Epoch 413/1000, Loss: 3.148379759355025\n",
            "Epoch 414/1000, Loss: 3.155917220946514\n",
            "Epoch 415/1000, Loss: 3.076857126120365\n",
            "Epoch 416/1000, Loss: 3.107517040137089\n",
            "Epoch 417/1000, Loss: 3.1417194234602377\n",
            "Epoch 418/1000, Loss: 3.15802506515474\n",
            "Epoch 419/1000, Loss: 3.0779051600080547\n",
            "Epoch 420/1000, Loss: 3.123864928881327\n",
            "Epoch 421/1000, Loss: 3.1285305709549873\n",
            "Epoch 422/1000, Loss: 3.1634433395934827\n",
            "Epoch 423/1000, Loss: 3.1087854540709294\n",
            "Epoch 424/1000, Loss: 3.1023740127231134\n",
            "Epoch 425/1000, Loss: 3.069360922683369\n",
            "Epoch 426/1000, Loss: 3.1785559780669934\n",
            "Epoch 427/1000, Loss: 3.1340717230782364\n",
            "Epoch 428/1000, Loss: 3.0514501640290925\n",
            "Epoch 429/1000, Loss: 3.0932345245823716\n",
            "Epoch 430/1000, Loss: 3.1047520836194358\n",
            "Epoch 431/1000, Loss: 3.1157694910511826\n",
            "Epoch 432/1000, Loss: 3.1424453701033737\n",
            "Epoch 433/1000, Loss: 3.1322419959487338\n",
            "Epoch 434/1000, Loss: 3.100401573108904\n",
            "Epoch 435/1000, Loss: 3.1360898261720482\n",
            "Epoch 436/1000, Loss: 3.154390871524811\n",
            "Epoch 437/1000, Loss: 3.0836027680021343\n",
            "Epoch 438/1000, Loss: 3.0633880748893274\n",
            "Epoch 439/1000, Loss: 3.152522713849039\n",
            "Epoch 440/1000, Loss: 3.086766596093322\n",
            "Epoch 441/1000, Loss: 3.102040682778214\n",
            "Epoch 442/1000, Loss: 3.128318350423466\n",
            "Epoch 443/1000, Loss: 3.097872430627996\n",
            "Epoch 444/1000, Loss: 3.116708871090051\n",
            "Epoch 445/1000, Loss: 3.1101645746014337\n",
            "Epoch 446/1000, Loss: 3.064653490528916\n",
            "Epoch 447/1000, Loss: 3.166895517797181\n",
            "Epoch 448/1000, Loss: 3.160917376026963\n",
            "Epoch 449/1000, Loss: 3.0710245497298962\n",
            "Epoch 450/1000, Loss: 3.1118481466264436\n",
            "Epoch 451/1000, Loss: 3.1718534050565776\n",
            "Epoch 452/1000, Loss: 3.0673252741495767\n",
            "Epoch 453/1000, Loss: 3.1530550131292054\n",
            "Epoch 454/1000, Loss: 3.111382721048413\n",
            "Epoch 455/1000, Loss: 3.086434768004851\n",
            "Epoch 456/1000, Loss: 3.151162935025764\n",
            "Epoch 457/1000, Loss: 3.1405288125529434\n",
            "Epoch 458/1000, Loss: 3.092687883160331\n",
            "Epoch 459/1000, Loss: 3.073953771229946\n",
            "Epoch 460/1000, Loss: 3.0685025905117844\n",
            "Epoch 461/1000, Loss: 3.1176971368717425\n",
            "Epoch 462/1000, Loss: 3.0672171450022496\n",
            "Epoch 463/1000, Loss: 3.1592885851860046\n",
            "Epoch 464/1000, Loss: 3.117245674133301\n",
            "Epoch 465/1000, Loss: 3.1304961388761345\n",
            "Epoch 466/1000, Loss: 3.1209083036942915\n",
            "Epoch 467/1000, Loss: 3.150616529313001\n",
            "Epoch 468/1000, Loss: 3.0660550567236813\n",
            "Epoch 469/1000, Loss: 3.1072357540780846\n",
            "Epoch 470/1000, Loss: 3.0719443053910225\n",
            "Epoch 471/1000, Loss: 3.1253010666731633\n",
            "Epoch 472/1000, Loss: 3.0997054179509482\n",
            "Epoch 473/1000, Loss: 3.0773628308917536\n",
            "Epoch 474/1000, Loss: 3.0682467113841665\n",
            "Epoch 475/1000, Loss: 3.0567943462819764\n",
            "Epoch 476/1000, Loss: 3.1360882087187334\n",
            "Epoch 477/1000, Loss: 3.046999164602973\n",
            "Epoch 478/1000, Loss: 3.1353074619264314\n",
            "Epoch 479/1000, Loss: 3.1293409340309375\n",
            "Epoch 480/1000, Loss: 3.095424768599597\n",
            "Epoch 481/1000, Loss: 3.144450955318682\n",
            "Epoch 482/1000, Loss: 3.048595338156729\n",
            "Epoch 483/1000, Loss: 3.0468430573290046\n",
            "Epoch 484/1000, Loss: 3.0814627643787498\n",
            "Epoch 485/1000, Loss: 3.0153612060980364\n",
            "Epoch 486/1000, Loss: 3.0823209538604273\n",
            "Epoch 487/1000, Loss: 3.0922203795476393\n",
            "Epoch 488/1000, Loss: 3.0488063331806297\n",
            "Epoch 489/1000, Loss: 3.032847650123365\n",
            "Epoch 490/1000, Loss: 3.065238605846058\n",
            "Epoch 491/1000, Loss: 3.0682469678647593\n",
            "Epoch 492/1000, Loss: 3.076314850287004\n",
            "Epoch 493/1000, Loss: 3.0975319201296028\n",
            "Epoch 494/1000, Loss: 3.0973132370096264\n",
            "Epoch 495/1000, Loss: 3.033199345523661\n",
            "Epoch 496/1000, Loss: 3.1296551498499783\n",
            "Epoch 497/1000, Loss: 3.138314274224368\n",
            "Epoch 498/1000, Loss: 3.097827819260684\n",
            "Epoch 499/1000, Loss: 3.068676859140396\n",
            "Epoch 500/1000, Loss: 3.089618094039686\n",
            "Epoch 501/1000, Loss: 3.065005044142405\n",
            "Epoch 502/1000, Loss: 3.045802679928866\n",
            "Epoch 503/1000, Loss: 3.037551594502998\n",
            "Epoch 504/1000, Loss: 3.0851542913552485\n",
            "Epoch 505/1000, Loss: 3.072497881723173\n",
            "Epoch 506/1000, Loss: 3.061045844446529\n",
            "Epoch 507/1000, Loss: 3.1049706719138404\n",
            "Epoch 508/1000, Loss: 3.1288489988355925\n",
            "Epoch 509/1000, Loss: 3.086805480899233\n",
            "Epoch 510/1000, Loss: 3.0163386021599625\n",
            "Epoch 511/1000, Loss: 3.053791444409977\n",
            "Epoch 512/1000, Loss: 3.1375237504641214\n",
            "Epoch 513/1000, Loss: 3.023359889333898\n",
            "Epoch 514/1000, Loss: 3.110622320211295\n",
            "Epoch 515/1000, Loss: 3.136958163796049\n",
            "Epoch 516/1000, Loss: 3.091879226944663\n",
            "Epoch 517/1000, Loss: 3.094722420880289\n",
            "Epoch 518/1000, Loss: 3.0898608845291715\n",
            "Epoch 519/1000, Loss: 3.045483068083272\n",
            "Epoch 520/1000, Loss: 2.9892753001415366\n",
            "Epoch 521/1000, Loss: 3.062987607536894\n",
            "Epoch 522/1000, Loss: 3.103893224037055\n",
            "Epoch 523/1000, Loss: 3.0493570891293613\n",
            "Epoch 524/1000, Loss: 3.095110867962693\n",
            "Epoch 525/1000, Loss: 3.0294083474260387\n",
            "Epoch 526/1000, Loss: 3.0713232267986643\n",
            "Epoch 527/1000, Loss: 3.0673596290024845\n",
            "Epoch 528/1000, Loss: 3.0752338830268746\n",
            "Epoch 529/1000, Loss: 3.0779059700893634\n",
            "Epoch 530/1000, Loss: 3.1307985656189197\n",
            "Epoch 531/1000, Loss: 3.1125279782396373\n",
            "Epoch 532/1000, Loss: 3.073910649978753\n",
            "Epoch 533/1000, Loss: 3.023570412939245\n",
            "Epoch 534/1000, Loss: 3.053236000465624\n",
            "Epoch 535/1000, Loss: 3.069171358238567\n",
            "Epoch 536/1000, Loss: 3.0623470562877078\n",
            "Epoch 537/1000, Loss: 3.0719067625927203\n",
            "Epoch 538/1000, Loss: 3.067694933125467\n",
            "Epoch 539/1000, Loss: 3.0489036892399644\n",
            "Epoch 540/1000, Loss: 3.078168421080618\n",
            "Epoch 541/1000, Loss: 2.9953265524271764\n",
            "Epoch 542/1000, Loss: 3.0475377810723856\n",
            "Epoch 543/1000, Loss: 3.0854232880202206\n",
            "Epoch 544/1000, Loss: 3.066219153729352\n",
            "Epoch 545/1000, Loss: 3.035412260980317\n",
            "Epoch 546/1000, Loss: 3.055203929091945\n",
            "Epoch 547/1000, Loss: 3.017309855331074\n",
            "Epoch 548/1000, Loss: 3.200738254821662\n",
            "Epoch 549/1000, Loss: 3.071056281978434\n",
            "Epoch 550/1000, Loss: 3.0157725738756582\n",
            "Epoch 551/1000, Loss: 3.111297832293944\n",
            "Epoch 552/1000, Loss: 3.1131265912995194\n",
            "Epoch 553/1000, Loss: 3.0830671046719407\n",
            "Epoch 554/1000, Loss: 3.0460333029429116\n",
            "Epoch 555/1000, Loss: 3.0662083156181104\n",
            "Epoch 556/1000, Loss: 3.0274576526699644\n",
            "Epoch 557/1000, Loss: 3.068081187479424\n",
            "Epoch 558/1000, Loss: 3.0654477431918634\n",
            "Epoch 559/1000, Loss: 3.044335712086071\n",
            "Epoch 560/1000, Loss: 3.0589477410822203\n",
            "Epoch 561/1000, Loss: 3.0566667074506935\n",
            "Epoch 562/1000, Loss: 3.0491171993992547\n",
            "Epoch 563/1000, Loss: 3.126885318394863\n",
            "Epoch 564/1000, Loss: 3.0799293933492717\n",
            "Epoch 565/1000, Loss: 3.0360811840404165\n",
            "Epoch 566/1000, Loss: 3.0902058518294133\n",
            "Epoch 567/1000, Loss: 3.017034761833422\n",
            "Epoch 568/1000, Loss: 3.040825568365328\n",
            "Epoch 569/1000, Loss: 3.0449130246133516\n",
            "Epoch 570/1000, Loss: 3.0672592692302936\n",
            "Epoch 571/1000, Loss: 3.0829779541853704\n",
            "Epoch 572/1000, Loss: 3.0893082094914988\n",
            "Epoch 573/1000, Loss: 3.0286467310154075\n",
            "Epoch 574/1000, Loss: 3.1235284317623484\n",
            "Epoch 575/1000, Loss: 3.0630680078809913\n",
            "Epoch 576/1000, Loss: 3.0137086230697054\n",
            "Epoch 577/1000, Loss: 3.0422336757183075\n",
            "Epoch 578/1000, Loss: 3.092357971451499\n",
            "Epoch 579/1000, Loss: 3.0632716128320405\n",
            "Epoch 580/1000, Loss: 3.049672502459902\n",
            "Epoch 581/1000, Loss: 3.0792678531372184\n",
            "Epoch 582/1000, Loss: 3.087640318003568\n",
            "Epoch 583/1000, Loss: 3.0049577463756907\n",
            "Epoch 584/1000, Loss: 3.04927238370433\n",
            "Epoch 585/1000, Loss: 3.049958510832353\n",
            "Epoch 586/1000, Loss: 3.0766352812449136\n",
            "Epoch 587/1000, Loss: 3.014027791492867\n",
            "Epoch 588/1000, Loss: 2.9887957320068823\n",
            "Epoch 589/1000, Loss: 3.000482032696406\n",
            "Epoch 590/1000, Loss: 3.0313787604823257\n",
            "Epoch 591/1000, Loss: 3.109414013046207\n",
            "Epoch 592/1000, Loss: 3.036010406234048\n",
            "Epoch 593/1000, Loss: 3.0727529010989447\n",
            "Epoch 594/1000, Loss: 3.0718069491964397\n",
            "Epoch 595/1000, Loss: 3.0226161741849147\n",
            "Epoch 596/1000, Loss: 2.946076636061524\n",
            "Epoch 597/1000, Loss: 3.15287927515579\n",
            "Epoch 598/1000, Loss: 3.031416208455057\n",
            "Epoch 599/1000, Loss: 3.0034632023536796\n",
            "Epoch 600/1000, Loss: 2.9865899284680686\n",
            "Epoch 601/1000, Loss: 3.0691139499346414\n",
            "Epoch 602/1000, Loss: 3.117764454899412\n",
            "Epoch 603/1000, Loss: 3.0574089541579736\n",
            "Epoch 604/1000, Loss: 3.0241176675666464\n",
            "Epoch 605/1000, Loss: 2.937088977206837\n",
            "Epoch 606/1000, Loss: 3.1149239124673787\n",
            "Epoch 607/1000, Loss: 3.0336379824262676\n",
            "Epoch 608/1000, Loss: 3.045450598904581\n",
            "Epoch 609/1000, Loss: 3.041500293847286\n",
            "Epoch 610/1000, Loss: 3.0242443626577202\n",
            "Epoch 611/1000, Loss: 3.020960347218947\n",
            "Epoch 612/1000, Loss: 2.9918322003248967\n",
            "Epoch 613/1000, Loss: 3.027177946134047\n",
            "Epoch 614/1000, Loss: 2.949177285938552\n",
            "Epoch 615/1000, Loss: 3.020299013816949\n",
            "Epoch 616/1000, Loss: 3.038113510066813\n",
            "Epoch 617/1000, Loss: 3.035295068314581\n",
            "Epoch 618/1000, Loss: 2.998594472805659\n",
            "Epoch 619/1000, Loss: 3.07279127294367\n",
            "Epoch 620/1000, Loss: 3.042260956583601\n",
            "Epoch 621/1000, Loss: 3.0184911805571932\n",
            "Epoch 622/1000, Loss: 3.069682981931802\n",
            "Epoch 623/1000, Loss: 3.068784655946674\n",
            "Epoch 624/1000, Loss: 3.0617818651777324\n",
            "Epoch 625/1000, Loss: 3.047898323246927\n",
            "Epoch 626/1000, Loss: 3.05432413744204\n",
            "Epoch 627/1000, Loss: 3.052035854621367\n",
            "Epoch 628/1000, Loss: 3.0346392393112183\n",
            "Epoch 629/1000, Loss: 2.978528914126483\n",
            "Epoch 630/1000, Loss: 2.9637717749133254\n",
            "Epoch 631/1000, Loss: 2.9946394472411186\n",
            "Epoch 632/1000, Loss: 3.074757238229116\n",
            "Epoch 633/1000, Loss: 3.058487492980379\n",
            "Epoch 634/1000, Loss: 3.0195124438314727\n",
            "Epoch 635/1000, Loss: 3.055053801247568\n",
            "Epoch 636/1000, Loss: 3.0236272197781187\n",
            "Epoch 637/1000, Loss: 3.0755766539862663\n",
            "Epoch 638/1000, Loss: 3.071553405487176\n",
            "Epoch 639/1000, Loss: 3.0038469578280593\n",
            "Epoch 640/1000, Loss: 3.0491540161046116\n",
            "Epoch 641/1000, Loss: 3.0567581183982617\n",
            "Epoch 642/1000, Loss: 2.952990741440744\n",
            "Epoch 643/1000, Loss: 2.996295786265171\n",
            "Epoch 644/1000, Loss: 3.029471081314665\n",
            "Epoch 645/1000, Loss: 3.036794870188742\n",
            "Epoch 646/1000, Loss: 3.0416208803653717\n",
            "Epoch 647/1000, Loss: 3.044253838784767\n",
            "Epoch 648/1000, Loss: 2.972307888847409\n",
            "Epoch 649/1000, Loss: 3.0053923590616747\n",
            "Epoch 650/1000, Loss: 3.0170229854005757\n",
            "Epoch 651/1000, Loss: 2.9464857930486854\n",
            "Epoch 652/1000, Loss: 2.950238551154281\n",
            "Epoch 653/1000, Loss: 2.9794870804656637\n",
            "Epoch 654/1000, Loss: 3.043555757313064\n",
            "Epoch 655/1000, Loss: 3.0385670192313916\n",
            "Epoch 656/1000, Loss: 3.0003644860152043\n",
            "Epoch 657/1000, Loss: 3.0371878074877188\n",
            "Epoch 658/1000, Loss: 2.9818419190970333\n",
            "Epoch 659/1000, Loss: 3.0398171218958767\n",
            "Epoch 660/1000, Loss: 3.055914550116568\n",
            "Epoch 661/1000, Loss: 3.0645131956447256\n",
            "Epoch 662/1000, Loss: 2.9665942986806235\n",
            "Epoch 663/1000, Loss: 2.9936592669198006\n",
            "Epoch 664/1000, Loss: 3.0176131264729933\n",
            "Epoch 665/1000, Loss: 3.0394856279546563\n",
            "Epoch 666/1000, Loss: 3.05918329502597\n",
            "Epoch 667/1000, Loss: 3.008479958230799\n",
            "Epoch 668/1000, Loss: 3.071333182580543\n",
            "Epoch 669/1000, Loss: 3.0287906635891306\n",
            "Epoch 670/1000, Loss: 2.9837717775142556\n",
            "Epoch 671/1000, Loss: 3.0679394187349263\n",
            "Epoch 672/1000, Loss: 3.0161268295663777\n",
            "Epoch 673/1000, Loss: 2.970868347269116\n",
            "Epoch 674/1000, Loss: 2.9997886726350496\n",
            "Epoch 675/1000, Loss: 3.050245176662098\n",
            "Epoch 676/1000, Loss: 2.9781714497190532\n",
            "Epoch 677/1000, Loss: 3.007737395438281\n",
            "Epoch 678/1000, Loss: 2.998480799523267\n",
            "Epoch 679/1000, Loss: 2.9800578409975227\n",
            "Epoch 680/1000, Loss: 2.9898009679534217\n",
            "Epoch 681/1000, Loss: 3.058305449558027\n",
            "Epoch 682/1000, Loss: 3.0470092495282493\n",
            "Epoch 683/1000, Loss: 2.9513975688905427\n",
            "Epoch 684/1000, Loss: 2.997562935858062\n",
            "Epoch 685/1000, Loss: 2.9694983941135984\n",
            "Epoch 686/1000, Loss: 3.0236443317297734\n",
            "Epoch 687/1000, Loss: 3.008811304063508\n",
            "Epoch 688/1000, Loss: 2.9611942659724844\n",
            "Epoch 689/1000, Loss: 2.9810684847109243\n",
            "Epoch 690/1000, Loss: 2.960950839700121\n",
            "Epoch 691/1000, Loss: 3.031436688972242\n",
            "Epoch 692/1000, Loss: 3.0579113653211882\n",
            "Epoch 693/1000, Loss: 3.015454596642292\n",
            "Epoch 694/1000, Loss: 3.0451754331588745\n",
            "Epoch 695/1000, Loss: 3.020596998207497\n",
            "Epoch 696/1000, Loss: 2.9924709110549004\n",
            "Epoch 697/1000, Loss: 3.0229075334288855\n",
            "Epoch 698/1000, Loss: 2.9771715312293083\n",
            "Epoch 699/1000, Loss: 3.028466930895141\n",
            "Epoch 700/1000, Loss: 2.998985310395559\n",
            "Epoch 701/1000, Loss: 2.9477374174378137\n",
            "Epoch 702/1000, Loss: 2.9251303428953346\n",
            "Epoch 703/1000, Loss: 2.9344036561070066\n",
            "Epoch 704/1000, Loss: 3.0067469333157395\n",
            "Epoch 705/1000, Loss: 2.970602431080558\n",
            "Epoch 706/1000, Loss: 3.0481215516726174\n",
            "Epoch 707/1000, Loss: 2.9911556722539845\n",
            "Epoch 708/1000, Loss: 2.964002670663776\n",
            "Epoch 709/1000, Loss: 3.021451269135331\n",
            "Epoch 710/1000, Loss: 2.9265754692482226\n",
            "Epoch 711/1000, Loss: 3.043449279936877\n",
            "Epoch 712/1000, Loss: 3.004729601469907\n",
            "Epoch 713/1000, Loss: 2.926776759552233\n",
            "Epoch 714/1000, Loss: 2.967776627251596\n",
            "Epoch 715/1000, Loss: 2.9227111113793924\n",
            "Epoch 716/1000, Loss: 3.0452058550083274\n",
            "Epoch 717/1000, Loss: 3.0265839993953705\n",
            "Epoch 718/1000, Loss: 2.9759544296698137\n",
            "Epoch 719/1000, Loss: 3.002115828521324\n",
            "Epoch 720/1000, Loss: 2.97554464412458\n",
            "Epoch 721/1000, Loss: 2.891423028526884\n",
            "Epoch 722/1000, Loss: 3.014840006828308\n",
            "Epoch 723/1000, Loss: 2.945569039294214\n",
            "Epoch 724/1000, Loss: 3.024358514583472\n",
            "Epoch 725/1000, Loss: 3.0891437223463347\n",
            "Epoch 726/1000, Loss: 3.000337571808786\n",
            "Epoch 727/1000, Loss: 2.973840099392515\n",
            "Epoch 728/1000, Loss: 3.0139611273100884\n",
            "Epoch 729/1000, Loss: 2.9903985993428663\n",
            "Epoch 730/1000, Loss: 3.027974880102909\n",
            "Epoch 731/1000, Loss: 2.9604187020749757\n",
            "Epoch 732/1000, Loss: 2.940511108347864\n",
            "Epoch 733/1000, Loss: 3.012346621715661\n",
            "Epoch 734/1000, Loss: 3.040284940690705\n",
            "Epoch 735/1000, Loss: 2.954187319134221\n",
            "Epoch 736/1000, Loss: 3.0405706564585366\n",
            "Epoch 737/1000, Loss: 3.0822714783928613\n",
            "Epoch 738/1000, Loss: 3.0418863982865303\n",
            "Epoch 739/1000, Loss: 2.9874972186305304\n",
            "Epoch 740/1000, Loss: 3.0081413008949975\n",
            "Epoch 741/1000, Loss: 2.96532505570036\n",
            "Epoch 742/1000, Loss: 2.9575966343735205\n",
            "Epoch 743/1000, Loss: 2.9902525489980523\n",
            "Epoch 744/1000, Loss: 3.0094206044168184\n",
            "Epoch 745/1000, Loss: 2.983612936554533\n",
            "Epoch 746/1000, Loss: 3.041654662652449\n",
            "Epoch 747/1000, Loss: 2.9586575202869647\n",
            "Epoch 748/1000, Loss: 3.058665712674459\n",
            "Epoch 749/1000, Loss: 2.9290226652766718\n",
            "Epoch 750/1000, Loss: 2.966547733003443\n",
            "Epoch 751/1000, Loss: 2.9953894804824484\n",
            "Epoch 752/1000, Loss: 2.972633210998593\n",
            "Epoch 753/1000, Loss: 2.963856353001161\n",
            "Epoch 754/1000, Loss: 2.946849604447683\n",
            "Epoch 755/1000, Loss: 2.9472329381740456\n",
            "Epoch 756/1000, Loss: 2.991249140464898\n",
            "Epoch 757/1000, Loss: 3.055139380874056\n",
            "Epoch 758/1000, Loss: 2.9653707002148484\n",
            "Epoch 759/1000, Loss: 2.9885628819465637\n",
            "Epoch 760/1000, Loss: 3.019008510943615\n",
            "Epoch 761/1000, Loss: 3.045031359701446\n",
            "Epoch 762/1000, Loss: 2.9721128218101733\n",
            "Epoch 763/1000, Loss: 3.0113149917486943\n",
            "Epoch 764/1000, Loss: 2.998146659497059\n",
            "Epoch 765/1000, Loss: 3.0014154784607165\n",
            "Epoch 766/1000, Loss: 3.023513172612046\n",
            "Epoch 767/1000, Loss: 2.991624442013827\n",
            "Epoch 768/1000, Loss: 2.995257127465624\n",
            "Epoch 769/1000, Loss: 2.96376464583657\n",
            "Epoch 770/1000, Loss: 3.00562174392469\n",
            "Epoch 771/1000, Loss: 3.0025142882809495\n",
            "Epoch 772/1000, Loss: 2.9540691149957254\n",
            "Epoch 773/1000, Loss: 2.9115855341607872\n",
            "Epoch 774/1000, Loss: 3.083632205471848\n",
            "Epoch 775/1000, Loss: 2.985336184501648\n",
            "Epoch 776/1000, Loss: 3.0402251698754053\n",
            "Epoch 777/1000, Loss: 3.0235369620901165\n",
            "Epoch 778/1000, Loss: 2.9689379424759834\n",
            "Epoch 779/1000, Loss: 2.9735024228240503\n",
            "Epoch 780/1000, Loss: 2.965480204784509\n",
            "Epoch 781/1000, Loss: 2.9208152267065914\n",
            "Epoch 782/1000, Loss: 3.010817475391157\n",
            "Epoch 783/1000, Loss: 2.9420889850818748\n",
            "Epoch 784/1000, Loss: 2.956862912936644\n",
            "Epoch 785/1000, Loss: 2.903592173800324\n",
            "Epoch 786/1000, Loss: 3.0094355525392475\n",
            "Epoch 787/1000, Loss: 3.002674055821968\n",
            "Epoch 788/1000, Loss: 3.0260120485768174\n",
            "Epoch 789/1000, Loss: 2.9546804852557904\n",
            "Epoch 790/1000, Loss: 3.0014658696723706\n",
            "Epoch 791/1000, Loss: 2.9533909766963036\n",
            "Epoch 792/1000, Loss: 2.9756311936811968\n",
            "Epoch 793/1000, Loss: 3.036687908750592\n",
            "Epoch 794/1000, Loss: 2.9904598422122723\n",
            "Epoch 795/1000, Loss: 3.0231915071155084\n",
            "Epoch 796/1000, Loss: 2.9452560345331826\n",
            "Epoch 797/1000, Loss: 3.0310784849253567\n",
            "Epoch 798/1000, Loss: 2.965391336968451\n",
            "Epoch 799/1000, Loss: 3.036670457233082\n",
            "Epoch 800/1000, Loss: 2.9481693018566477\n",
            "Epoch 801/1000, Loss: 3.0139390248240847\n",
            "Epoch 802/1000, Loss: 3.0100555447014896\n",
            "Epoch 803/1000, Loss: 2.983210318919384\n",
            "Epoch 804/1000, Loss: 2.9873547436613026\n",
            "Epoch 805/1000, Loss: 3.0209871259602634\n",
            "Epoch 806/1000, Loss: 2.9811415500713117\n",
            "Epoch 807/1000, Loss: 2.993106840234814\n",
            "Epoch 808/1000, Loss: 2.9740916743423\n",
            "Epoch 809/1000, Loss: 2.920936542930025\n",
            "Epoch 810/1000, Loss: 3.0048744732683357\n",
            "Epoch 811/1000, Loss: 3.0060090431661317\n",
            "Epoch 812/1000, Loss: 3.0046631197134652\n",
            "Epoch 813/1000, Loss: 2.994850765575062\n",
            "Epoch 814/1000, Loss: 2.9788959965561377\n",
            "Epoch 815/1000, Loss: 2.924947299740531\n",
            "Epoch 816/1000, Loss: 2.9498180503194984\n",
            "Epoch 817/1000, Loss: 2.982018333492857\n",
            "Epoch 818/1000, Loss: 2.9620808615829004\n",
            "Epoch 819/1000, Loss: 2.9819522138797874\n",
            "Epoch 820/1000, Loss: 2.9704088433222338\n",
            "Epoch 821/1000, Loss: 2.9339805407957598\n",
            "Epoch 822/1000, Loss: 2.941032100807537\n",
            "Epoch 823/1000, Loss: 3.0459553879318815\n",
            "Epoch 824/1000, Loss: 2.9841480002258765\n",
            "Epoch 825/1000, Loss: 2.9751360895055714\n",
            "Epoch 826/1000, Loss: 2.9647332041552574\n",
            "Epoch 827/1000, Loss: 2.957385983430978\n",
            "Epoch 828/1000, Loss: 2.9345808164639906\n",
            "Epoch 829/1000, Loss: 2.948971703197017\n",
            "Epoch 830/1000, Loss: 3.013043345827045\n",
            "Epoch 831/1000, Loss: 2.931985532695597\n",
            "Epoch 832/1000, Loss: 2.9003281521074697\n",
            "Epoch 833/1000, Loss: 2.9826530489054592\n",
            "Epoch 834/1000, Loss: 2.9843342340353765\n",
            "Epoch 835/1000, Loss: 2.907877628550385\n",
            "Epoch 836/1000, Loss: 2.939039232152881\n",
            "Epoch 837/1000, Loss: 2.9346531855337545\n",
            "Epoch 838/1000, Loss: 3.056553383668264\n",
            "Epoch 839/1000, Loss: 2.954112323847684\n",
            "Epoch 840/1000, Loss: 2.884982030500065\n",
            "Epoch 841/1000, Loss: 2.9550728942408706\n",
            "Epoch 842/1000, Loss: 2.980376065680475\n",
            "Epoch 843/1000, Loss: 2.9905081860946887\n",
            "Epoch 844/1000, Loss: 3.0077678281249423\n",
            "Epoch 845/1000, Loss: 2.9527605988762597\n",
            "Epoch 846/1000, Loss: 2.942373825745149\n",
            "Epoch 847/1000, Loss: 3.010975434924617\n",
            "Epoch 848/1000, Loss: 2.9501626563794687\n",
            "Epoch 849/1000, Loss: 2.933801666353688\n",
            "Epoch 850/1000, Loss: 2.9383624638571884\n",
            "Epoch 851/1000, Loss: 2.9659261893142355\n",
            "Epoch 852/1000, Loss: 2.9488055823427257\n",
            "Epoch 853/1000, Loss: 2.970787606456063\n",
            "Epoch 854/1000, Loss: 2.886121265815966\n",
            "Epoch 855/1000, Loss: 2.9510758257273473\n",
            "Epoch 856/1000, Loss: 2.9576849630384734\n",
            "Epoch 857/1000, Loss: 2.9225735953359893\n",
            "Epoch 858/1000, Loss: 2.956624765287746\n",
            "Epoch 859/1000, Loss: 2.9827471303217337\n",
            "Epoch 860/1000, Loss: 2.930742641290029\n",
            "Epoch 861/1000, Loss: 2.994111183014783\n",
            "Epoch 862/1000, Loss: 2.9085044580878634\n",
            "Epoch 863/1000, Loss: 2.9319668520580637\n",
            "Epoch 864/1000, Loss: 2.9913805678035272\n",
            "Epoch 865/1000, Loss: 2.8699854151769117\n",
            "Epoch 866/1000, Loss: 2.9453515320113213\n",
            "Epoch 867/1000, Loss: 2.961055448561004\n",
            "Epoch 868/1000, Loss: 2.9932900215640212\n",
            "Epoch 869/1000, Loss: 2.968980464068326\n",
            "Epoch 870/1000, Loss: 2.930771086252097\n",
            "Epoch 871/1000, Loss: 2.896430268432155\n",
            "Epoch 872/1000, Loss: 2.9074729653921993\n",
            "Epoch 873/1000, Loss: 3.0099372267723083\n",
            "Epoch 874/1000, Loss: 2.987993157271183\n",
            "Epoch 875/1000, Loss: 2.9350169120412883\n",
            "Epoch 876/1000, Loss: 2.993760052955512\n",
            "Epoch 877/1000, Loss: 2.931850371938763\n",
            "Epoch 878/1000, Loss: 2.9871262519648583\n",
            "Epoch 879/1000, Loss: 2.912873002615842\n",
            "Epoch 880/1000, Loss: 2.963627649076057\n",
            "Epoch 881/1000, Loss: 2.9868292673067613\n",
            "Epoch 882/1000, Loss: 2.948026539701404\n",
            "Epoch 883/1000, Loss: 2.937965989112854\n",
            "Epoch 884/1000, Loss: 2.995395417466308\n",
            "Epoch 885/1000, Loss: 2.951226395187956\n",
            "Epoch 886/1000, Loss: 2.9676971471670903\n",
            "Epoch 887/1000, Loss: 2.9531114227844006\n",
            "Epoch 888/1000, Loss: 2.9144364161924883\n",
            "Epoch 889/1000, Loss: 2.9542532364527383\n",
            "Epoch 890/1000, Loss: 3.0003520051638284\n",
            "Epoch 891/1000, Loss: 2.90892073060527\n",
            "Epoch 892/1000, Loss: 2.940352608760198\n",
            "Epoch 893/1000, Loss: 2.9652759703722866\n",
            "Epoch 894/1000, Loss: 2.9046165997331794\n",
            "Epoch 895/1000, Loss: 2.9510306029608757\n",
            "Epoch 896/1000, Loss: 2.967962324619293\n",
            "Epoch 897/1000, Loss: 2.959119563752955\n",
            "Epoch 898/1000, Loss: 3.002773400508996\n",
            "Epoch 899/1000, Loss: 2.963502759283239\n",
            "Epoch 900/1000, Loss: 2.9874606412468534\n",
            "Epoch 901/1000, Loss: 2.9179178656953755\n",
            "Epoch 902/1000, Loss: 2.9450701357740345\n",
            "Epoch 903/1000, Loss: 2.8919446974089653\n",
            "Epoch 904/1000, Loss: 2.9307906645717043\n",
            "Epoch 905/1000, Loss: 2.926190910014239\n",
            "Epoch 906/1000, Loss: 2.9969311835187855\n",
            "Epoch 907/1000, Loss: 2.8983003262317544\n",
            "Epoch 908/1000, Loss: 2.9737586532578324\n",
            "Epoch 909/1000, Loss: 2.975408215414394\n",
            "Epoch 910/1000, Loss: 2.946455450672092\n",
            "Epoch 911/1000, Loss: 2.9483827271244745\n",
            "Epoch 912/1000, Loss: 2.9935476472883513\n",
            "Epoch 913/1000, Loss: 2.924666276483825\n",
            "Epoch 914/1000, Loss: 2.942558471000556\n",
            "Epoch 915/1000, Loss: 2.9169593742399504\n",
            "Epoch 916/1000, Loss: 2.9736962155862288\n",
            "Epoch 917/1000, Loss: 2.91935193809596\n",
            "Epoch 918/1000, Loss: 2.9149004726698906\n",
            "Epoch 919/1000, Loss: 2.925076354633678\n",
            "Epoch 920/1000, Loss: 2.939752484812881\n",
            "Epoch 921/1000, Loss: 2.926492940295826\n",
            "Epoch 922/1000, Loss: 3.0392404370235675\n",
            "Epoch 923/1000, Loss: 2.9016524824229153\n",
            "Epoch 924/1000, Loss: 3.0198058919473127\n",
            "Epoch 925/1000, Loss: 2.9860347852562414\n",
            "Epoch 926/1000, Loss: 2.8732164122841577\n",
            "Epoch 927/1000, Loss: 2.9754151593555105\n",
            "Epoch 928/1000, Loss: 2.952860629016703\n",
            "Epoch 929/1000, Loss: 2.9529272706219643\n",
            "Epoch 930/1000, Loss: 2.93384056651231\n",
            "Epoch 931/1000, Loss: 2.9412680394721753\n",
            "Epoch 932/1000, Loss: 2.9412519507335895\n",
            "Epoch 933/1000, Loss: 2.9423253617503424\n",
            "Epoch 934/1000, Loss: 2.9358022754842583\n",
            "Epoch 935/1000, Loss: 2.934265232447422\n",
            "Epoch 936/1000, Loss: 2.873151594942266\n",
            "Epoch 937/1000, Loss: 2.932420687241988\n",
            "Epoch 938/1000, Loss: 2.9080491345940214\n",
            "Epoch 939/1000, Loss: 2.905779964996107\n",
            "Epoch 940/1000, Loss: 2.9243044040419837\n",
            "Epoch 941/1000, Loss: 2.8947508822787893\n",
            "Epoch 942/1000, Loss: 2.932654619216919\n",
            "Epoch 943/1000, Loss: 2.956914537783825\n",
            "Epoch 944/1000, Loss: 2.9029831127686934\n",
            "Epoch 945/1000, Loss: 2.9688634213173026\n",
            "Epoch 946/1000, Loss: 2.888782305247856\n",
            "Epoch 947/1000, Loss: 2.9688327917546937\n",
            "Epoch 948/1000, Loss: 2.988243551868381\n",
            "Epoch 949/1000, Loss: 2.945297495885329\n",
            "Epoch 950/1000, Loss: 2.938119487328963\n",
            "Epoch 951/1000, Loss: 2.9142632656025165\n",
            "Epoch 952/1000, Loss: 2.947843805407033\n",
            "Epoch 953/1000, Loss: 2.9328915530985054\n",
            "Epoch 954/1000, Loss: 2.9429481201099628\n",
            "Epoch 955/1000, Loss: 2.9339978017590265\n",
            "Epoch 956/1000, Loss: 2.96558373263388\n",
            "Epoch 957/1000, Loss: 2.968062753930236\n",
            "Epoch 958/1000, Loss: 2.939175792715766\n",
            "Epoch 959/1000, Loss: 2.9461557016228186\n",
            "Epoch 960/1000, Loss: 2.9199180034073917\n",
            "Epoch 961/1000, Loss: 2.906177937081366\n",
            "Epoch 962/1000, Loss: 2.8596893585089482\n",
            "Epoch 963/1000, Loss: 2.99567572907968\n",
            "Epoch 964/1000, Loss: 2.946422199408213\n",
            "Epoch 965/1000, Loss: 2.9671689479640038\n",
            "Epoch 966/1000, Loss: 2.9668659911011206\n",
            "Epoch 967/1000, Loss: 2.8933766881624856\n",
            "Epoch 968/1000, Loss: 3.0145150730104158\n",
            "Epoch 969/1000, Loss: 2.9049296631957544\n",
            "Epoch 970/1000, Loss: 2.901264559138905\n",
            "Epoch 971/1000, Loss: 2.878206671187372\n",
            "Epoch 972/1000, Loss: 2.951065197135463\n",
            "Epoch 973/1000, Loss: 2.884975175062815\n",
            "Epoch 974/1000, Loss: 2.9112352512099524\n",
            "Epoch 975/1000, Loss: 2.9619688193003335\n",
            "Epoch 976/1000, Loss: 2.9483036091833403\n",
            "Epoch 977/1000, Loss: 2.919845961260073\n",
            "Epoch 978/1000, Loss: 2.853379376006849\n",
            "Epoch 979/1000, Loss: 2.8798774878184\n",
            "Epoch 980/1000, Loss: 2.937384085221724\n",
            "Epoch 981/1000, Loss: 2.9991661364381965\n",
            "Epoch 982/1000, Loss: 2.8802609886183883\n",
            "Epoch 983/1000, Loss: 2.9474981661998862\n",
            "Epoch 984/1000, Loss: 2.8929355478647985\n",
            "Epoch 985/1000, Loss: 2.9251810465798234\n",
            "Epoch 986/1000, Loss: 2.957731820417173\n",
            "Epoch 987/1000, Loss: 2.925907729250012\n",
            "Epoch 988/1000, Loss: 2.9364411559971897\n",
            "Epoch 989/1000, Loss: 2.9768880741162733\n",
            "Epoch 990/1000, Loss: 2.959655034722704\n",
            "Epoch 991/1000, Loss: 2.8612691678784112\n",
            "Epoch 992/1000, Loss: 2.9366271965431445\n",
            "Epoch 993/1000, Loss: 2.91092818162658\n",
            "Epoch 994/1000, Loss: 2.9509851372603215\n",
            "Epoch 995/1000, Loss: 2.8550413052241006\n",
            "Epoch 996/1000, Loss: 2.881050759192669\n",
            "Epoch 997/1000, Loss: 2.9248385393258296\n",
            "Epoch 998/1000, Loss: 2.9016147174618463\n",
            "Epoch 999/1000, Loss: 2.910395290815469\n",
            "Epoch 1000/1000, Loss: 2.9213788536461918\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "num_epochs = 1000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for inputs, targets_G_act, targets_G_r in train_dataloader:\n",
        "        targets_G_act = targets_G_act.to(device)\n",
        "        targets_G_r = targets_G_r.to(device)\n",
        "        inputs = inputs.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        outputs_G_act, outputs_G_r = outputs  # Separate the outputs into two tensors\n",
        "\n",
        "        loss_G_act = criterion(outputs_G_act, targets_G_act.unsqueeze(1))\n",
        "        loss_G_r = criterion(outputs_G_r, targets_G_r.unsqueeze(1))\n",
        "        loss = loss_G_act + loss_G_r\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "KCPx_9YD3YYM",
        "outputId": "d7a11707-5b6f-40d4-f4b3-b02f54d936dd"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-b66e5ca7e7e7>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtargets_G_act\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets_G_act\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtargets_G_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets_G_r\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0moutputs_G_act\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs_G_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m  \u001b[0;31m# Separate the outputs into two tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-b27aead34759>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the testing data\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_loss_G_act = 0.0\n",
        "    test_loss_G_r = 0.0\n",
        "    total_mse = 0.0\n",
        "    total_mae = 0.0\n",
        "    for inputs, targets_G_act, targets_G_r in test_dataloader:\n",
        "        targets_G_act = targets_G_act.to(device)\n",
        "        targets_G_r = targets_G_r.to(device)\n",
        "        outputs = model(inputs)\n",
        "        outputs_G_act, outputs_G_r = outputs  # Separate the outputs into two tensors\n",
        "\n",
        "        loss_G_act = criterion(outputs_G_act, targets_G_act.unsqueeze(1))\n",
        "        loss_G_r = criterion(outputs_G_r, targets_G_r.unsqueeze(1))\n",
        "\n",
        "        test_loss_G_act += loss_G_act.item()\n",
        "        test_loss_G_r += loss_G_r.item()\n",
        "\n",
        "        mse_G_act = mean_squared_error(targets_G_act.cpu(), outputs_G_act.cpu().detach())\n",
        "        mae_G_act = mean_absolute_error(targets_G_act.cpu(), outputs_G_act.cpu().detach())\n",
        "        mse_G_r = mean_squared_error(targets_G_r.cpu(), outputs_G_r.cpu().detach())\n",
        "        mae_G_r = mean_absolute_error(targets_G_r.cpu(), outputs_G_r.cpu().detach())\n",
        "\n",
        "        total_mse += mse_G_act.item() + mse_G_r.item()\n",
        "        total_mae += mae_G_act.item() + mae_G_r.item()\n",
        "\n",
        "    average_test_loss_G_act = test_loss_G_act / len(test_dataloader)\n",
        "    average_test_loss_G_r = test_loss_G_r / len(test_dataloader)\n",
        "\n",
        "    avg_mse = total_mse / len(test_dataloader)\n",
        "    avg_mae = total_mae / len(test_dataloader)\n",
        "\n",
        "    print(f\"Average Test Loss G_act: {average_test_loss_G_act}\")\n",
        "    print(f\"Average Test Loss G_r: {average_test_loss_G_r}\")\n",
        "\n",
        "    print(f\"Mean Squared Error (MSE): {avg_mse:.4f}\")\n",
        "    print(f\"Mean Absolute Error (MAE): {avg_mae:.4f}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "O5m2FeYt3YYN"
      },
      "source": [
        "# For showing how it works, we will use the following example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxjpuns03YYP",
        "outputId": "7874054f-a26c-48a7-88bd-49d582d29b25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "G_act 0     15.875896\n",
            "1     15.155477\n",
            "2     18.013824\n",
            "3     23.687079\n",
            "4     23.438094\n",
            "5     20.969801\n",
            "6     11.722625\n",
            "7      7.268044\n",
            "8      6.289673\n",
            "9     16.044475\n",
            "10    13.604005\n",
            "Name: G_act, dtype: float64 G_r 0    -51.881526\n",
            "1    -51.398681\n",
            "2    -66.822349\n",
            "3    -62.481289\n",
            "4    -84.836459\n",
            "5    -57.785046\n",
            "6    -63.721331\n",
            "7    -86.996102\n",
            "8    -86.799671\n",
            "9    -50.409197\n",
            "10   -51.008126\n",
            "Name: G_r, dtype: float64  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Read the dataset\n",
        "df = pd.read_csv('debuging_dataset.csv')\n",
        "\n",
        "# Extract the SMILES strings\n",
        "smiles = df['rxn_smiles']\n",
        "\n",
        "print(f\"G_act {df['G_act']} G_r {df['G_r']}  \")\n",
        "\n",
        "# Encode SMILES strings to fingerprints\n",
        "X_fingerprints = []\n",
        "\n",
        "for idx, smiles in enumerate(smiles):\n",
        "    reactants, products = smiles.split('>>')\n",
        "    reactant_fingerprints = [encode_smiles(reactant) for reactant in reactants.split('.')]\n",
        "    product_fingerprints = [encode_smiles(product) for product in products.split('.')]\n",
        "    X_fingerprints.append(reactant_fingerprints + product_fingerprints)\n",
        "\n",
        "# Convert the list of fingerprints and target values to tensors\n",
        "X_test = torch.stack([torch.cat(fingerprints) for fingerprints in X_fingerprints]).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LCGNqUu3YYQ",
        "outputId": "4f051ed1-8bf7-4154-b801-77d6120f9c4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions for G_act:\n",
            "15.528451\n",
            "15.004355\n",
            "18.296814\n",
            "22.711151\n",
            "23.070389\n",
            "20.372826\n",
            "11.329659\n",
            "7.5581665\n",
            "7.8600636\n",
            "15.880939\n",
            "13.679186\n",
            "Predictions for G_r:\n",
            "-50.93228\n",
            "-51.38302\n",
            "-65.1774\n",
            "-61.78781\n",
            "-82.55413\n",
            "-56.837776\n",
            "-64.16272\n",
            "-86.10514\n",
            "-77.14561\n",
            "-48.911022\n",
            "-48.26027\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Make predictions on test inputs\n",
        "with torch.no_grad():\n",
        "    test_inputs = X_test  \n",
        "    predictions_G_act, predictions_G_r = model(test_inputs)  # Separate the predictions into two tensors\n",
        "\n",
        "# Convert predictions to numpy arrays\n",
        "predictions_G_act = predictions_G_act.detach().cpu().numpy()\n",
        "predictions_G_r = predictions_G_r.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "print(\"Predictions for G_act:\")\n",
        "for pred in predictions_G_act:\n",
        "    print(pred[0])\n",
        "\n",
        "print(\"Predictions for G_r:\")\n",
        "for pred in predictions_G_r:\n",
        "    print(pred[0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
