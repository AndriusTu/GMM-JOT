{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\python39\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\andri\\appdata\\roaming\\python\\python39\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python39\\lib\\site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\python39\\lib\\site-packages (from pandas) (1.24.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\andri\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\python39\\lib\\site-packages (1.24.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\python39\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\python39\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\python39\\lib\\site-packages (from scikit-learn) (1.24.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\python39\\lib\\site-packages (from scikit-learn) (1.8.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python39\\lib\\site-packages (from scikit-learn) (3.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\python39\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\python39\\lib\\site-packages (from torch) (4.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rdkit in c:\\python39\\lib\\site-packages (2023.3.1)\n",
      "Requirement already satisfied: Pillow in c:\\python39\\lib\\site-packages (from rdkit) (9.4.0)\n",
      "Requirement already satisfied: numpy in c:\\python39\\lib\\site-packages (from rdkit) (1.24.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install scikit-learn\n",
    "!{sys.executable} -m pip install torch\n",
    "!{sys.executable} -m pip install rdkit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Reaction dataset class\n",
    "class ReactionDataset(Dataset):\n",
    "    def __init__(self, X, y_G_act, y_G_r):\n",
    "        self.X = X\n",
    "        self.y_G_act = y_G_act\n",
    "        self.y_G_r = y_G_r\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y_G_act[idx], self.y_G_r[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "df = pd.read_csv('full_dataset.csv') \n",
    "\n",
    "# Prepare the feature matrix X and target variables y\n",
    "X_smiles = df['rxn_smiles']\n",
    "y_G_act = df['G_act']\n",
    "y_G_r = df['G_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([5269, 3072])\n"
     ]
    }
   ],
   "source": [
    "# Define the encoding function\n",
    "def encode_smiles(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=1024)\n",
    "    bitstring = fp.ToBitString()\n",
    "    features = np.array([int(bit) for bit in bitstring], dtype=np.float32)\n",
    "    features_tensor = torch.tensor(features)\n",
    "    return features_tensor\n",
    "\n",
    "# Encode SMILES strings to fingerprints\n",
    "X_fingerprints = []\n",
    "\n",
    "for idx, smiles in enumerate(X_smiles):\n",
    "    reactants, products = smiles.split('>>')\n",
    "    reactant_fingerprints = [encode_smiles(reactant) for reactant in reactants.split('.')]\n",
    "    product_fingerprints = [encode_smiles(product) for product in products.split('.')]\n",
    "    X_fingerprints.append(reactant_fingerprints + product_fingerprints)\n",
    "\n",
    "# Convert the list of fingerprints and target values to tensors\n",
    "X_tensor = torch.stack([torch.cat(fingerprints) for fingerprints in X_fingerprints])\n",
    "\n",
    "# Print the fingerprint tensor\n",
    "print(X_tensor)\n",
    "print(X_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data into PyTorch tensors\n",
    "y_G_act_tensor = torch.tensor(y_G_act.values, dtype=torch.float32)\n",
    "y_G_r_tensor = torch.tensor(y_G_r.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_G_act_train, y_G_act_test, y_G_r_train, y_G_r_test = train_test_split(X_tensor, y_G_act_tensor, y_G_r_tensor, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc3_act = nn.Linear(64, 1)  # Output layer for G_act\n",
    "        self.fc3_r = nn.Linear(64, 1)  # Output layer for G_r\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        output_act = self.fc3_act(x)  # Output for G_act\n",
    "        output_r = self.fc3_r(x)  # Output for G_r\n",
    "        return output_act, output_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = NeuralNetwork(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.SmoothL1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data loaders\n",
    "train_dataset = ReactionDataset(X_train, y_G_act_train, y_G_r_train)\n",
    "test_dataset = ReactionDataset(X_test, y_G_act_test, y_G_r_test)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 30.45531959244699\n",
      "Epoch 2/1000, Loss: 19.1097511956186\n",
      "Epoch 3/1000, Loss: 13.007705002120048\n",
      "Epoch 4/1000, Loss: 11.293037862488717\n",
      "Epoch 5/1000, Loss: 10.222045212080985\n",
      "Epoch 6/1000, Loss: 9.683159777612397\n",
      "Epoch 7/1000, Loss: 9.374874750773111\n",
      "Epoch 8/1000, Loss: 8.91071718750578\n",
      "Epoch 9/1000, Loss: 8.63733368208914\n",
      "Epoch 10/1000, Loss: 8.315470919464573\n",
      "Epoch 11/1000, Loss: 8.135114785396691\n",
      "Epoch 12/1000, Loss: 7.986842505859606\n",
      "Epoch 13/1000, Loss: 7.905284968289462\n",
      "Epoch 14/1000, Loss: 7.672869888218966\n",
      "Epoch 15/1000, Loss: 7.726050250457995\n",
      "Epoch 16/1000, Loss: 7.437041835351423\n",
      "Epoch 17/1000, Loss: 7.472227949084657\n",
      "Epoch 18/1000, Loss: 7.313329494360722\n",
      "Epoch 19/1000, Loss: 7.2551981066212505\n",
      "Epoch 20/1000, Loss: 7.212780616500161\n",
      "Epoch 21/1000, Loss: 7.133050362269084\n",
      "Epoch 22/1000, Loss: 7.052901990485914\n",
      "Epoch 23/1000, Loss: 6.877286119894548\n",
      "Epoch 24/1000, Loss: 6.751619125857498\n",
      "Epoch 25/1000, Loss: 6.884031884598009\n",
      "Epoch 26/1000, Loss: 6.793733860507156\n",
      "Epoch 27/1000, Loss: 6.83867487040433\n",
      "Epoch 28/1000, Loss: 6.710267280087327\n",
      "Epoch 29/1000, Loss: 6.654512268124205\n",
      "Epoch 30/1000, Loss: 6.624272548791134\n",
      "Epoch 31/1000, Loss: 6.548261573820403\n",
      "Epoch 32/1000, Loss: 6.450510834202622\n",
      "Epoch 33/1000, Loss: 6.493139664332072\n",
      "Epoch 34/1000, Loss: 6.388059493267175\n",
      "Epoch 35/1000, Loss: 6.400763786200321\n",
      "Epoch 36/1000, Loss: 6.317008571191267\n",
      "Epoch 37/1000, Loss: 6.298196362726616\n",
      "Epoch 38/1000, Loss: 6.16663670539856\n",
      "Epoch 39/1000, Loss: 6.280972292928984\n",
      "Epoch 40/1000, Loss: 6.236719409624736\n",
      "Epoch 41/1000, Loss: 5.967639726219756\n",
      "Epoch 42/1000, Loss: 6.097574183435151\n",
      "Epoch 43/1000, Loss: 6.029772758483887\n",
      "Epoch 44/1000, Loss: 6.070136989607955\n",
      "Epoch 45/1000, Loss: 5.946094039714698\n",
      "Epoch 46/1000, Loss: 6.074651107643589\n",
      "Epoch 47/1000, Loss: 5.857948693362149\n",
      "Epoch 48/1000, Loss: 5.882644360715693\n",
      "Epoch 49/1000, Loss: 5.8536007548823505\n",
      "Epoch 50/1000, Loss: 5.839145075191151\n",
      "Epoch 51/1000, Loss: 5.896291111454819\n",
      "Epoch 52/1000, Loss: 5.839921293836651\n",
      "Epoch 53/1000, Loss: 5.7438891222982695\n",
      "Epoch 54/1000, Loss: 5.720099579204213\n",
      "Epoch 55/1000, Loss: 5.752699154796022\n",
      "Epoch 56/1000, Loss: 5.640093951514273\n",
      "Epoch 57/1000, Loss: 5.603890493060604\n",
      "Epoch 58/1000, Loss: 5.465723131642197\n",
      "Epoch 59/1000, Loss: 5.618737282175006\n",
      "Epoch 60/1000, Loss: 5.5259347341277385\n",
      "Epoch 61/1000, Loss: 5.590280989805858\n",
      "Epoch 62/1000, Loss: 5.506491276350888\n",
      "Epoch 63/1000, Loss: 5.548685529015281\n",
      "Epoch 64/1000, Loss: 5.395102435892278\n",
      "Epoch 65/1000, Loss: 5.380714098612468\n",
      "Epoch 66/1000, Loss: 5.360000245498888\n",
      "Epoch 67/1000, Loss: 5.360948978048382\n",
      "Epoch 68/1000, Loss: 5.316888209545251\n",
      "Epoch 69/1000, Loss: 5.283093844399308\n",
      "Epoch 70/1000, Loss: 5.265091027274276\n",
      "Epoch 71/1000, Loss: 5.222093005975087\n",
      "Epoch 72/1000, Loss: 5.224366045359409\n",
      "Epoch 73/1000, Loss: 5.091859049869306\n",
      "Epoch 74/1000, Loss: 5.287514197103905\n",
      "Epoch 75/1000, Loss: 5.208883500460423\n",
      "Epoch 76/1000, Loss: 5.121219302668716\n",
      "Epoch 77/1000, Loss: 5.083711701812166\n",
      "Epoch 78/1000, Loss: 5.021652973059452\n",
      "Epoch 79/1000, Loss: 5.029857572280999\n",
      "Epoch 80/1000, Loss: 4.982408707792109\n",
      "Epoch 81/1000, Loss: 5.017928399822929\n",
      "Epoch 82/1000, Loss: 5.019869410630428\n",
      "Epoch 83/1000, Loss: 4.968143806313023\n",
      "Epoch 84/1000, Loss: 4.902233439864534\n",
      "Epoch 85/1000, Loss: 5.0464922496766755\n",
      "Epoch 86/1000, Loss: 5.026725881027453\n",
      "Epoch 87/1000, Loss: 4.945725034583699\n",
      "Epoch 88/1000, Loss: 4.8269483898625225\n",
      "Epoch 89/1000, Loss: 4.887025273207462\n",
      "Epoch 90/1000, Loss: 4.893757341486035\n",
      "Epoch 91/1000, Loss: 4.918861302462491\n",
      "Epoch 92/1000, Loss: 4.9315226583769824\n",
      "Epoch 93/1000, Loss: 4.907638981486812\n",
      "Epoch 94/1000, Loss: 4.8851240367600415\n",
      "Epoch 95/1000, Loss: 4.862064301967621\n",
      "Epoch 96/1000, Loss: 4.800465885436896\n",
      "Epoch 97/1000, Loss: 4.919383193507339\n",
      "Epoch 98/1000, Loss: 4.795114340204181\n",
      "Epoch 99/1000, Loss: 4.817605789863702\n",
      "Epoch 100/1000, Loss: 4.802502695358161\n",
      "Epoch 101/1000, Loss: 4.828662294330019\n",
      "Epoch 102/1000, Loss: 4.82630993200071\n",
      "Epoch 103/1000, Loss: 4.739007312240022\n",
      "Epoch 104/1000, Loss: 4.699412083987034\n",
      "Epoch 105/1000, Loss: 4.666390247417219\n",
      "Epoch 106/1000, Loss: 4.7101728536865926\n",
      "Epoch 107/1000, Loss: 4.76356480699597\n",
      "Epoch 108/1000, Loss: 4.611398422356808\n",
      "Epoch 109/1000, Loss: 4.635272334922444\n",
      "Epoch 110/1000, Loss: 4.662643194198608\n",
      "Epoch 111/1000, Loss: 4.668088125460075\n",
      "Epoch 112/1000, Loss: 4.641932072061481\n",
      "Epoch 113/1000, Loss: 4.676234989455252\n",
      "Epoch 114/1000, Loss: 4.562220887704329\n",
      "Epoch 115/1000, Loss: 4.611989126060948\n",
      "Epoch 116/1000, Loss: 4.553161863124732\n",
      "Epoch 117/1000, Loss: 4.573701892838334\n",
      "Epoch 118/1000, Loss: 4.637777812553175\n",
      "Epoch 119/1000, Loss: 4.541502519087358\n",
      "Epoch 120/1000, Loss: 4.503369502948992\n",
      "Epoch 121/1000, Loss: 4.574593571099368\n",
      "Epoch 122/1000, Loss: 4.580511385744268\n",
      "Epoch 123/1000, Loss: 4.579145852363471\n",
      "Epoch 124/1000, Loss: 4.5112376231135745\n",
      "Epoch 125/1000, Loss: 4.465877532958984\n",
      "Epoch 126/1000, Loss: 4.585899351221142\n",
      "Epoch 127/1000, Loss: 4.517586079510775\n",
      "Epoch 128/1000, Loss: 4.570451125954136\n",
      "Epoch 129/1000, Loss: 4.4763946623513196\n",
      "Epoch 130/1000, Loss: 4.439070067622445\n",
      "Epoch 131/1000, Loss: 4.514098463636456\n",
      "Epoch 132/1000, Loss: 4.419461490529956\n",
      "Epoch 133/1000, Loss: 4.48246497818918\n",
      "Epoch 134/1000, Loss: 4.516344377488801\n",
      "Epoch 135/1000, Loss: 4.374406894048055\n",
      "Epoch 136/1000, Loss: 4.431361509091927\n",
      "Epoch 137/1000, Loss: 4.4566980510046985\n",
      "Epoch 138/1000, Loss: 4.416246011401668\n",
      "Epoch 139/1000, Loss: 4.5412151271646675\n",
      "Epoch 140/1000, Loss: 4.421223360480684\n",
      "Epoch 141/1000, Loss: 4.519591177954818\n",
      "Epoch 142/1000, Loss: 4.299180605194786\n",
      "Epoch 143/1000, Loss: 4.3633750026876275\n",
      "Epoch 144/1000, Loss: 4.362443645795186\n",
      "Epoch 145/1000, Loss: 4.378424200144681\n",
      "Epoch 146/1000, Loss: 4.382997660925894\n",
      "Epoch 147/1000, Loss: 4.411810620264574\n",
      "Epoch 148/1000, Loss: 4.320935263778225\n",
      "Epoch 149/1000, Loss: 4.3443435954325125\n",
      "Epoch 150/1000, Loss: 4.28820900664185\n",
      "Epoch 151/1000, Loss: 4.408210156541882\n",
      "Epoch 152/1000, Loss: 4.256019119060401\n",
      "Epoch 153/1000, Loss: 4.321033495845216\n",
      "Epoch 154/1000, Loss: 4.350342506712133\n",
      "Epoch 155/1000, Loss: 4.293649124376701\n",
      "Epoch 156/1000, Loss: 4.411368799932076\n",
      "Epoch 157/1000, Loss: 4.363072277921619\n",
      "Epoch 158/1000, Loss: 4.267685581337322\n",
      "Epoch 159/1000, Loss: 4.304777725176378\n",
      "Epoch 160/1000, Loss: 4.294311872034362\n",
      "Epoch 161/1000, Loss: 4.211613400415941\n",
      "Epoch 162/1000, Loss: 4.30493724526781\n",
      "Epoch 163/1000, Loss: 4.226376323988943\n",
      "Epoch 164/1000, Loss: 4.29823112487793\n",
      "Epoch 165/1000, Loss: 4.274018607356331\n",
      "Epoch 166/1000, Loss: 4.18497026869745\n",
      "Epoch 167/1000, Loss: 4.256137853318995\n",
      "Epoch 168/1000, Loss: 4.221205680659323\n",
      "Epoch 169/1000, Loss: 4.206766525904338\n",
      "Epoch 170/1000, Loss: 4.199130426753651\n",
      "Epoch 171/1000, Loss: 4.259042694713131\n",
      "Epoch 172/1000, Loss: 4.240744009162441\n",
      "Epoch 173/1000, Loss: 4.16941632827123\n",
      "Epoch 174/1000, Loss: 4.305982970830166\n",
      "Epoch 175/1000, Loss: 4.215077053416859\n",
      "Epoch 176/1000, Loss: 4.206017911434174\n",
      "Epoch 177/1000, Loss: 4.206756337122484\n",
      "Epoch 178/1000, Loss: 4.1358541051546736\n",
      "Epoch 179/1000, Loss: 4.213327044790441\n",
      "Epoch 180/1000, Loss: 4.17938613169121\n",
      "Epoch 181/1000, Loss: 4.205251610640324\n",
      "Epoch 182/1000, Loss: 4.076706909772121\n",
      "Epoch 183/1000, Loss: 4.206660989559058\n",
      "Epoch 184/1000, Loss: 4.212020698821906\n",
      "Epoch 185/1000, Loss: 4.125003791216648\n",
      "Epoch 186/1000, Loss: 4.303120651028373\n",
      "Epoch 187/1000, Loss: 4.114061657226447\n",
      "Epoch 188/1000, Loss: 4.133383521527955\n",
      "Epoch 189/1000, Loss: 4.192145560726975\n",
      "Epoch 190/1000, Loss: 4.087406951369661\n",
      "Epoch 191/1000, Loss: 4.103570219242211\n",
      "Epoch 192/1000, Loss: 4.12410440047582\n",
      "Epoch 193/1000, Loss: 4.1623212911865926\n",
      "Epoch 194/1000, Loss: 4.1784757989825625\n",
      "Epoch 195/1000, Loss: 4.090899861220158\n",
      "Epoch 196/1000, Loss: 4.105145774104378\n",
      "Epoch 197/1000, Loss: 4.058235708511237\n",
      "Epoch 198/1000, Loss: 4.0441456545482986\n",
      "Epoch 199/1000, Loss: 4.168544070287184\n",
      "Epoch 200/1000, Loss: 4.196739310568029\n",
      "Epoch 201/1000, Loss: 4.094403257875731\n",
      "Epoch 202/1000, Loss: 4.056705406217864\n",
      "Epoch 203/1000, Loss: 4.051723922743942\n",
      "Epoch 204/1000, Loss: 4.083112778085651\n",
      "Epoch 205/1000, Loss: 4.104838707230308\n",
      "Epoch 206/1000, Loss: 4.0824384779641125\n",
      "Epoch 207/1000, Loss: 3.9202596906459695\n",
      "Epoch 208/1000, Loss: 3.9854640906507317\n",
      "Epoch 209/1000, Loss: 4.039847377574805\n",
      "Epoch 210/1000, Loss: 4.0900361989483685\n",
      "Epoch 211/1000, Loss: 4.015311721599463\n",
      "Epoch 212/1000, Loss: 4.1711907386779785\n",
      "Epoch 213/1000, Loss: 4.03148669907541\n",
      "Epoch 214/1000, Loss: 4.042855221213716\n",
      "Epoch 215/1000, Loss: 3.9634920196099714\n",
      "Epoch 216/1000, Loss: 3.9613272728342\n",
      "Epoch 217/1000, Loss: 3.973930447390585\n",
      "Epoch 218/1000, Loss: 3.931987206141154\n",
      "Epoch 219/1000, Loss: 4.032222224004341\n",
      "Epoch 220/1000, Loss: 3.9249177502863333\n",
      "Epoch 221/1000, Loss: 3.948462946848436\n",
      "Epoch 222/1000, Loss: 3.9468272816051138\n",
      "Epoch 223/1000, Loss: 4.036420236934315\n",
      "Epoch 224/1000, Loss: 4.002410578005241\n",
      "Epoch 225/1000, Loss: 4.033528519399239\n",
      "Epoch 226/1000, Loss: 3.982022569035039\n",
      "Epoch 227/1000, Loss: 4.034971320267879\n",
      "Epoch 228/1000, Loss: 4.04598827976169\n",
      "Epoch 229/1000, Loss: 3.904949032899105\n",
      "Epoch 230/1000, Loss: 4.0006473443724895\n",
      "Epoch 231/1000, Loss: 3.904623382019274\n",
      "Epoch 232/1000, Loss: 3.90229087526148\n",
      "Epoch 233/1000, Loss: 3.8662521279219426\n",
      "Epoch 234/1000, Loss: 3.9547367547497605\n",
      "Epoch 235/1000, Loss: 4.023055436033191\n",
      "Epoch 236/1000, Loss: 3.9665490605614404\n",
      "Epoch 237/1000, Loss: 3.927329202493032\n",
      "Epoch 238/1000, Loss: 3.822557037526911\n",
      "Epoch 239/1000, Loss: 3.934830022580696\n",
      "Epoch 240/1000, Loss: 4.0032129323843755\n",
      "Epoch 241/1000, Loss: 3.9132733074101536\n",
      "Epoch 242/1000, Loss: 3.9066101637753574\n",
      "Epoch 243/1000, Loss: 3.9205948692379575\n",
      "Epoch 244/1000, Loss: 3.8812027555523496\n",
      "Epoch 245/1000, Loss: 3.84919413653287\n",
      "Epoch 246/1000, Loss: 3.9658105319196526\n",
      "Epoch 247/1000, Loss: 3.9667570374228736\n",
      "Epoch 248/1000, Loss: 3.8699733705231636\n",
      "Epoch 249/1000, Loss: 3.9397177768476084\n",
      "Epoch 250/1000, Loss: 3.7933462226029597\n",
      "Epoch 251/1000, Loss: 3.90792884609916\n",
      "Epoch 252/1000, Loss: 3.8614190527887056\n",
      "Epoch 253/1000, Loss: 3.824783392024763\n",
      "Epoch 254/1000, Loss: 3.8760421095472393\n",
      "Epoch 255/1000, Loss: 3.8622369748173337\n",
      "Epoch 256/1000, Loss: 3.8819807533061867\n",
      "Epoch 257/1000, Loss: 3.895829639651559\n",
      "Epoch 258/1000, Loss: 3.8752071640708228\n",
      "Epoch 259/1000, Loss: 3.8568112488948936\n",
      "Epoch 260/1000, Loss: 3.806187488816001\n",
      "Epoch 261/1000, Loss: 3.7797576333537246\n",
      "Epoch 262/1000, Loss: 3.8266448107632725\n",
      "Epoch 263/1000, Loss: 3.7270364779414553\n",
      "Epoch 264/1000, Loss: 3.80314108097192\n",
      "Epoch 265/1000, Loss: 3.8243193409659644\n",
      "Epoch 266/1000, Loss: 3.917870535995021\n",
      "Epoch 267/1000, Loss: 3.8500384399385164\n",
      "Epoch 268/1000, Loss: 3.8424738212065264\n",
      "Epoch 269/1000, Loss: 3.8581932096770317\n",
      "Epoch 270/1000, Loss: 3.7910298903783164\n",
      "Epoch 271/1000, Loss: 3.866694450378418\n",
      "Epoch 272/1000, Loss: 3.8005993709419714\n",
      "Epoch 273/1000, Loss: 3.7832102414333457\n",
      "Epoch 274/1000, Loss: 3.816736152677825\n",
      "Epoch 275/1000, Loss: 3.7431770292195408\n",
      "Epoch 276/1000, Loss: 3.7233343160513677\n",
      "Epoch 277/1000, Loss: 3.8152651570060034\n",
      "Epoch 278/1000, Loss: 3.7466236822532886\n",
      "Epoch 279/1000, Loss: 3.7373693603457827\n",
      "Epoch 280/1000, Loss: 3.727095062082464\n",
      "Epoch 281/1000, Loss: 3.7641050165349785\n",
      "Epoch 282/1000, Loss: 3.7803007779699382\n",
      "Epoch 283/1000, Loss: 3.768395385958932\n",
      "Epoch 284/1000, Loss: 3.743859132130941\n",
      "Epoch 285/1000, Loss: 3.712480725664081\n",
      "Epoch 286/1000, Loss: 3.7062143918239707\n",
      "Epoch 287/1000, Loss: 3.735842469966773\n",
      "Epoch 288/1000, Loss: 3.7447441455089683\n",
      "Epoch 289/1000, Loss: 3.7936032775676614\n",
      "Epoch 290/1000, Loss: 3.774172600471612\n",
      "Epoch 291/1000, Loss: 3.7691294930197974\n",
      "Epoch 292/1000, Loss: 3.852626719258048\n",
      "Epoch 293/1000, Loss: 3.7469235839265767\n",
      "Epoch 294/1000, Loss: 3.683406163345684\n",
      "Epoch 295/1000, Loss: 3.7306293790990654\n",
      "Epoch 296/1000, Loss: 3.7264393838969143\n",
      "Epoch 297/1000, Loss: 3.658198757605119\n",
      "Epoch 298/1000, Loss: 3.692556989915443\n",
      "Epoch 299/1000, Loss: 3.6573391358057656\n",
      "Epoch 300/1000, Loss: 3.7017895579338074\n",
      "Epoch 301/1000, Loss: 3.6671021165269795\n",
      "Epoch 302/1000, Loss: 3.666265440709663\n",
      "Epoch 303/1000, Loss: 3.7174805475003794\n",
      "Epoch 304/1000, Loss: 3.7372429316694085\n",
      "Epoch 305/1000, Loss: 3.678650989676967\n",
      "Epoch 306/1000, Loss: 3.7716657772208704\n",
      "Epoch 307/1000, Loss: 3.6973793813676545\n",
      "Epoch 308/1000, Loss: 3.7173467856464963\n",
      "Epoch 309/1000, Loss: 3.700218281962655\n",
      "Epoch 310/1000, Loss: 3.6607037096312554\n",
      "Epoch 311/1000, Loss: 3.6734397140416233\n",
      "Epoch 312/1000, Loss: 3.632728778954708\n",
      "Epoch 313/1000, Loss: 3.6086864164381316\n",
      "Epoch 314/1000, Loss: 3.6983389764121086\n",
      "Epoch 315/1000, Loss: 3.737237417336666\n",
      "Epoch 316/1000, Loss: 3.6936248089327957\n",
      "Epoch 317/1000, Loss: 3.655930425181533\n",
      "Epoch 318/1000, Loss: 3.687545556010622\n",
      "Epoch 319/1000, Loss: 3.55460218407891\n",
      "Epoch 320/1000, Loss: 3.645830764915004\n",
      "Epoch 321/1000, Loss: 3.7040526740478747\n",
      "Epoch 322/1000, Loss: 3.5819864291133303\n",
      "Epoch 323/1000, Loss: 3.5891052267768164\n",
      "Epoch 324/1000, Loss: 3.6174452521584253\n",
      "Epoch 325/1000, Loss: 3.6215383609135947\n",
      "Epoch 326/1000, Loss: 3.5687650442123413\n",
      "Epoch 327/1000, Loss: 3.637643929683801\n",
      "Epoch 328/1000, Loss: 3.6596107103607873\n",
      "Epoch 329/1000, Loss: 3.609202580018477\n",
      "Epoch 330/1000, Loss: 3.616228755676385\n",
      "Epoch 331/1000, Loss: 3.6360553412726433\n",
      "Epoch 332/1000, Loss: 3.6978855963909263\n",
      "Epoch 333/1000, Loss: 3.615599299922134\n",
      "Epoch 334/1000, Loss: 3.6001122828685874\n",
      "Epoch 335/1000, Loss: 3.630591636354273\n",
      "Epoch 336/1000, Loss: 3.608813307502053\n",
      "Epoch 337/1000, Loss: 3.59867000579834\n",
      "Epoch 338/1000, Loss: 3.5687137047449746\n",
      "Epoch 339/1000, Loss: 3.619559743187644\n",
      "Epoch 340/1000, Loss: 3.63240100217588\n",
      "Epoch 341/1000, Loss: 3.5201355507879546\n",
      "Epoch 342/1000, Loss: 3.5608601425633286\n",
      "Epoch 343/1000, Loss: 3.6166462374456003\n",
      "Epoch 344/1000, Loss: 3.6409307808587044\n",
      "Epoch 345/1000, Loss: 3.6106625253503974\n",
      "Epoch 346/1000, Loss: 3.5497589255824233\n",
      "Epoch 347/1000, Loss: 3.5833631013378953\n",
      "Epoch 348/1000, Loss: 3.5855528874830767\n",
      "Epoch 349/1000, Loss: 3.4949744253447563\n",
      "Epoch 350/1000, Loss: 3.572291520508853\n",
      "Epoch 351/1000, Loss: 3.5701275847174903\n",
      "Epoch 352/1000, Loss: 3.5109349290529885\n",
      "Epoch 353/1000, Loss: 3.5000585281487666\n",
      "Epoch 354/1000, Loss: 3.506069970853401\n",
      "Epoch 355/1000, Loss: 3.581105815641808\n",
      "Epoch 356/1000, Loss: 3.6350627469293997\n",
      "Epoch 357/1000, Loss: 3.5222612095601633\n",
      "Epoch 358/1000, Loss: 3.6275124567927737\n",
      "Epoch 359/1000, Loss: 3.582433581352234\n",
      "Epoch 360/1000, Loss: 3.6064651807149253\n",
      "Epoch 361/1000, Loss: 3.533296086571433\n",
      "Epoch 362/1000, Loss: 3.565337202765725\n",
      "Epoch 363/1000, Loss: 3.4814439397869688\n",
      "Epoch 364/1000, Loss: 3.536381510171023\n",
      "Epoch 365/1000, Loss: 3.533056506604859\n",
      "Epoch 366/1000, Loss: 3.537750179117376\n",
      "Epoch 367/1000, Loss: 3.563857503009565\n",
      "Epoch 368/1000, Loss: 3.5717735633705603\n",
      "Epoch 369/1000, Loss: 3.559040873339682\n",
      "Epoch 370/1000, Loss: 3.504055754704909\n",
      "Epoch 371/1000, Loss: 3.520971863558798\n",
      "Epoch 372/1000, Loss: 3.4715017676353455\n",
      "Epoch 373/1000, Loss: 3.5477981170018515\n",
      "Epoch 374/1000, Loss: 3.4168542424837747\n",
      "Epoch 375/1000, Loss: 3.487979081544009\n",
      "Epoch 376/1000, Loss: 3.494755986965064\n",
      "Epoch 377/1000, Loss: 3.50326216582096\n",
      "Epoch 378/1000, Loss: 3.5339472564783962\n",
      "Epoch 379/1000, Loss: 3.58618965474042\n",
      "Epoch 380/1000, Loss: 3.5475266088138926\n",
      "Epoch 381/1000, Loss: 3.472736445340243\n",
      "Epoch 382/1000, Loss: 3.4917436133731496\n",
      "Epoch 383/1000, Loss: 3.488817025314678\n",
      "Epoch 384/1000, Loss: 3.508026746186343\n",
      "Epoch 385/1000, Loss: 3.5203968250390254\n",
      "Epoch 386/1000, Loss: 3.451269339431416\n",
      "Epoch 387/1000, Loss: 3.451743219837998\n",
      "Epoch 388/1000, Loss: 3.49059101487651\n",
      "Epoch 389/1000, Loss: 3.4760382735367976\n",
      "Epoch 390/1000, Loss: 3.4821032159256213\n",
      "Epoch 391/1000, Loss: 3.57213600657203\n",
      "Epoch 392/1000, Loss: 3.4844659458507192\n",
      "Epoch 393/1000, Loss: 3.442505717277527\n",
      "Epoch 394/1000, Loss: 3.5108463619694565\n",
      "Epoch 395/1000, Loss: 3.487802111741268\n",
      "Epoch 396/1000, Loss: 3.4821196415207605\n",
      "Epoch 397/1000, Loss: 3.4444876367395576\n",
      "Epoch 398/1000, Loss: 3.5174054377006763\n",
      "Epoch 399/1000, Loss: 3.4100355686563435\n",
      "Epoch 400/1000, Loss: 3.484950719457684\n",
      "Epoch 401/1000, Loss: 3.5029479265213013\n",
      "Epoch 402/1000, Loss: 3.4464026129607\n",
      "Epoch 403/1000, Loss: 3.4654332128438083\n",
      "Epoch 404/1000, Loss: 3.4771936535835266\n",
      "Epoch 405/1000, Loss: 3.5087280327623542\n",
      "Epoch 406/1000, Loss: 3.341601519873648\n",
      "Epoch 407/1000, Loss: 3.3838567535082498\n",
      "Epoch 408/1000, Loss: 3.398456683664611\n",
      "Epoch 409/1000, Loss: 3.4390380761840125\n",
      "Epoch 410/1000, Loss: 3.402506761478655\n",
      "Epoch 411/1000, Loss: 3.4439853032430015\n",
      "Epoch 412/1000, Loss: 3.437783246690577\n",
      "Epoch 413/1000, Loss: 3.3728017698634756\n",
      "Epoch 414/1000, Loss: 3.461912653662942\n",
      "Epoch 415/1000, Loss: 3.369244123950149\n",
      "Epoch 416/1000, Loss: 3.4573567563837226\n",
      "Epoch 417/1000, Loss: 3.4514743703784365\n",
      "Epoch 418/1000, Loss: 3.41030370647257\n",
      "Epoch 419/1000, Loss: 3.368941751393405\n",
      "Epoch 420/1000, Loss: 3.4122633175416426\n",
      "Epoch 421/1000, Loss: 3.4493775349674802\n",
      "Epoch 422/1000, Loss: 3.371771503578533\n",
      "Epoch 423/1000, Loss: 3.446617173426079\n",
      "Epoch 424/1000, Loss: 3.4423811905311816\n",
      "Epoch 425/1000, Loss: 3.387991908824805\n",
      "Epoch 426/1000, Loss: 3.39291157505729\n",
      "Epoch 427/1000, Loss: 3.4155504206816354\n",
      "Epoch 428/1000, Loss: 3.4246913664268726\n",
      "Epoch 429/1000, Loss: 3.398291923783042\n",
      "Epoch 430/1000, Loss: 3.3210191997614773\n",
      "Epoch 431/1000, Loss: 3.3822973948536497\n",
      "Epoch 432/1000, Loss: 3.3813334974375637\n",
      "Epoch 433/1000, Loss: 3.413319493785049\n",
      "Epoch 434/1000, Loss: 3.403683299368078\n",
      "Epoch 435/1000, Loss: 3.35237210447138\n",
      "Epoch 436/1000, Loss: 3.3407124082247415\n",
      "Epoch 437/1000, Loss: 3.3793332251635464\n",
      "Epoch 438/1000, Loss: 3.4464476216923106\n",
      "Epoch 439/1000, Loss: 3.4459020206422517\n",
      "Epoch 440/1000, Loss: 3.3804991136897695\n",
      "Epoch 441/1000, Loss: 3.3479735616481667\n",
      "Epoch 442/1000, Loss: 3.340279232371937\n",
      "Epoch 443/1000, Loss: 3.378923905618263\n",
      "Epoch 444/1000, Loss: 3.381729983922207\n",
      "Epoch 445/1000, Loss: 3.369524275714701\n",
      "Epoch 446/1000, Loss: 3.3598156625574287\n",
      "Epoch 447/1000, Loss: 3.3025689179247077\n",
      "Epoch 448/1000, Loss: 3.3485379417737327\n",
      "Epoch 449/1000, Loss: 3.3184494972229004\n",
      "Epoch 450/1000, Loss: 3.3567873167269155\n",
      "Epoch 451/1000, Loss: 3.339599679816853\n",
      "Epoch 452/1000, Loss: 3.4275980393091836\n",
      "Epoch 453/1000, Loss: 3.3109612627462908\n",
      "Epoch 454/1000, Loss: 3.4278352531519802\n",
      "Epoch 455/1000, Loss: 3.3779590635588677\n",
      "Epoch 456/1000, Loss: 3.4049387881250093\n",
      "Epoch 457/1000, Loss: 3.367577760508566\n",
      "Epoch 458/1000, Loss: 3.33196658076662\n",
      "Epoch 459/1000, Loss: 3.3585336208343506\n",
      "Epoch 460/1000, Loss: 3.349056753245267\n",
      "Epoch 461/1000, Loss: 3.345378937143268\n",
      "Epoch 462/1000, Loss: 3.2942067456967905\n",
      "Epoch 463/1000, Loss: 3.3274188276493186\n",
      "Epoch 464/1000, Loss: 3.352295873743115\n",
      "Epoch 465/1000, Loss: 3.2562778050249275\n",
      "Epoch 466/1000, Loss: 3.378993522037159\n",
      "Epoch 467/1000, Loss: 3.3317707126790825\n",
      "Epoch 468/1000, Loss: 3.410615527268612\n",
      "Epoch 469/1000, Loss: 3.326186741843368\n",
      "Epoch 470/1000, Loss: 3.3718766714587356\n",
      "Epoch 471/1000, Loss: 3.250901019934452\n",
      "Epoch 472/1000, Loss: 3.3100679661288406\n",
      "Epoch 473/1000, Loss: 3.3270149050336895\n",
      "Epoch 474/1000, Loss: 3.3244927561644353\n",
      "Epoch 475/1000, Loss: 3.2883562698508753\n",
      "Epoch 476/1000, Loss: 3.294601751096321\n",
      "Epoch 477/1000, Loss: 3.2745447023348375\n",
      "Epoch 478/1000, Loss: 3.3157754350792277\n",
      "Epoch 479/1000, Loss: 3.3681628008683524\n",
      "Epoch 480/1000, Loss: 3.33144107912526\n",
      "Epoch 481/1000, Loss: 3.3059104735201057\n",
      "Epoch 482/1000, Loss: 3.3106836739814645\n",
      "Epoch 483/1000, Loss: 3.3102000138976355\n",
      "Epoch 484/1000, Loss: 3.288520536639474\n",
      "Epoch 485/1000, Loss: 3.2977138938325825\n",
      "Epoch 486/1000, Loss: 3.302390040773334\n",
      "Epoch 487/1000, Loss: 3.2939085590116903\n",
      "Epoch 488/1000, Loss: 3.344311105482506\n",
      "Epoch 489/1000, Loss: 3.346750203407172\n",
      "Epoch 490/1000, Loss: 3.2591226480223914\n",
      "Epoch 491/1000, Loss: 3.300628398403977\n",
      "Epoch 492/1000, Loss: 3.3532775821107808\n",
      "Epoch 493/1000, Loss: 3.242102377342455\n",
      "Epoch 494/1000, Loss: 3.2859973203052175\n",
      "Epoch 495/1000, Loss: 3.27671123454065\n",
      "Epoch 496/1000, Loss: 3.2100512674360564\n",
      "Epoch 497/1000, Loss: 3.2697783934347555\n",
      "Epoch 498/1000, Loss: 3.2663332657380537\n",
      "Epoch 499/1000, Loss: 3.307367523511251\n",
      "Epoch 500/1000, Loss: 3.3037737517645867\n",
      "Epoch 501/1000, Loss: 3.251610781207229\n",
      "Epoch 502/1000, Loss: 3.2741913542603003\n",
      "Epoch 503/1000, Loss: 3.3087382840387747\n",
      "Epoch 504/1000, Loss: 3.2992415355913565\n",
      "Epoch 505/1000, Loss: 3.2991599621194783\n",
      "Epoch 506/1000, Loss: 3.2595955512740393\n",
      "Epoch 507/1000, Loss: 3.33443302638603\n",
      "Epoch 508/1000, Loss: 3.258567531903585\n",
      "Epoch 509/1000, Loss: 3.198744670911269\n",
      "Epoch 510/1000, Loss: 3.2623618534117034\n",
      "Epoch 511/1000, Loss: 3.2710360180247915\n",
      "Epoch 512/1000, Loss: 3.233688335527073\n",
      "Epoch 513/1000, Loss: 3.2885631015806487\n",
      "Epoch 514/1000, Loss: 3.3086425987156955\n",
      "Epoch 515/1000, Loss: 3.2309185790293142\n",
      "Epoch 516/1000, Loss: 3.2649409626469468\n",
      "Epoch 517/1000, Loss: 3.3084313445019\n",
      "Epoch 518/1000, Loss: 3.219187592015122\n",
      "Epoch 519/1000, Loss: 3.303014352466121\n",
      "Epoch 520/1000, Loss: 3.251113553841909\n",
      "Epoch 521/1000, Loss: 3.3112986214233167\n",
      "Epoch 522/1000, Loss: 3.2713514638669565\n",
      "Epoch 523/1000, Loss: 3.2677171284502204\n",
      "Epoch 524/1000, Loss: 3.209652478044683\n",
      "Epoch 525/1000, Loss: 3.2770523934653313\n",
      "Epoch 526/1000, Loss: 3.286728754188075\n",
      "Epoch 527/1000, Loss: 3.2427979545159773\n",
      "Epoch 528/1000, Loss: 3.1821746482993616\n",
      "Epoch 529/1000, Loss: 3.272291309905775\n",
      "Epoch 530/1000, Loss: 3.1521272361278534\n",
      "Epoch 531/1000, Loss: 3.247436481894869\n",
      "Epoch 532/1000, Loss: 3.23712794527863\n",
      "Epoch 533/1000, Loss: 3.2576597269737357\n",
      "Epoch 534/1000, Loss: 3.2694939573605857\n",
      "Epoch 535/1000, Loss: 3.240825728033528\n",
      "Epoch 536/1000, Loss: 3.2341007843162073\n",
      "Epoch 537/1000, Loss: 3.2935206908168215\n",
      "Epoch 538/1000, Loss: 3.229495391701207\n",
      "Epoch 539/1000, Loss: 3.2468980568828005\n",
      "Epoch 540/1000, Loss: 3.2718108928564824\n",
      "Epoch 541/1000, Loss: 3.1781490788315283\n",
      "Epoch 542/1000, Loss: 3.186767000140566\n",
      "Epoch 543/1000, Loss: 3.188881595929464\n",
      "Epoch 544/1000, Loss: 3.2509391181396716\n",
      "Epoch 545/1000, Loss: 3.2286803325017295\n",
      "Epoch 546/1000, Loss: 3.226937461983074\n",
      "Epoch 547/1000, Loss: 3.210751002485102\n",
      "Epoch 548/1000, Loss: 3.27196171428218\n",
      "Epoch 549/1000, Loss: 3.228594852216316\n",
      "Epoch 550/1000, Loss: 3.2692353562875227\n",
      "Epoch 551/1000, Loss: 3.2049902840094133\n",
      "Epoch 552/1000, Loss: 3.2083193840402546\n",
      "Epoch 553/1000, Loss: 3.198366490277377\n",
      "Epoch 554/1000, Loss: 3.2375786277380856\n",
      "Epoch 555/1000, Loss: 3.267271795056083\n",
      "Epoch 556/1000, Loss: 3.286854675321868\n",
      "Epoch 557/1000, Loss: 3.177795375838424\n",
      "Epoch 558/1000, Loss: 3.1972991769964043\n",
      "Epoch 559/1000, Loss: 3.253219510569717\n",
      "Epoch 560/1000, Loss: 3.168292143128135\n",
      "Epoch 561/1000, Loss: 3.2487447875918765\n",
      "Epoch 562/1000, Loss: 3.1653626561164856\n",
      "Epoch 563/1000, Loss: 3.2407521226189355\n",
      "Epoch 564/1000, Loss: 3.301401613336621\n",
      "Epoch 565/1000, Loss: 3.218248396208792\n",
      "Epoch 566/1000, Loss: 3.233963081330964\n",
      "Epoch 567/1000, Loss: 3.225681579474247\n",
      "Epoch 568/1000, Loss: 3.181894133488337\n",
      "Epoch 569/1000, Loss: 3.12677473191059\n",
      "Epoch 570/1000, Loss: 3.2491549768231134\n",
      "Epoch 571/1000, Loss: 3.1352319807717293\n",
      "Epoch 572/1000, Loss: 3.228265767747706\n",
      "Epoch 573/1000, Loss: 3.182503100597497\n",
      "Epoch 574/1000, Loss: 3.1844675450614006\n",
      "Epoch 575/1000, Loss: 3.216034606550679\n",
      "Epoch 576/1000, Loss: 3.245836764574051\n",
      "Epoch 577/1000, Loss: 3.2246965513084875\n",
      "Epoch 578/1000, Loss: 3.145356373353438\n",
      "Epoch 579/1000, Loss: 3.13142373345115\n",
      "Epoch 580/1000, Loss: 3.2487233461755696\n",
      "Epoch 581/1000, Loss: 3.135227671175292\n",
      "Epoch 582/1000, Loss: 3.2384402137814146\n",
      "Epoch 583/1000, Loss: 3.1392921299645393\n",
      "Epoch 584/1000, Loss: 3.0756994278141945\n",
      "Epoch 585/1000, Loss: 3.1951034213557388\n",
      "Epoch 586/1000, Loss: 3.2196351154284044\n",
      "Epoch 587/1000, Loss: 3.1212625232609836\n",
      "Epoch 588/1000, Loss: 3.1816087979258914\n",
      "Epoch 589/1000, Loss: 3.214894594568195\n",
      "Epoch 590/1000, Loss: 3.1278843825513665\n",
      "Epoch 591/1000, Loss: 3.175991707678997\n",
      "Epoch 592/1000, Loss: 3.29085913029584\n",
      "Epoch 593/1000, Loss: 3.2215371502168253\n",
      "Epoch 594/1000, Loss: 3.1435559346820368\n",
      "Epoch 595/1000, Loss: 3.153948532812523\n",
      "Epoch 596/1000, Loss: 3.1836130059126653\n",
      "Epoch 597/1000, Loss: 3.175493141015371\n",
      "Epoch 598/1000, Loss: 3.1850696711829216\n",
      "Epoch 599/1000, Loss: 3.274345829631343\n",
      "Epoch 600/1000, Loss: 3.211431900660197\n",
      "Epoch 601/1000, Loss: 3.229161560535431\n",
      "Epoch 602/1000, Loss: 3.13729465007782\n",
      "Epoch 603/1000, Loss: 3.1204849987319023\n",
      "Epoch 604/1000, Loss: 3.1252759342843834\n",
      "Epoch 605/1000, Loss: 3.08606669216445\n",
      "Epoch 606/1000, Loss: 3.196509832685644\n",
      "Epoch 607/1000, Loss: 3.124739838368965\n",
      "Epoch 608/1000, Loss: 3.2053467920332244\n",
      "Epoch 609/1000, Loss: 3.1569930947188176\n",
      "Epoch 610/1000, Loss: 3.1367622631968874\n",
      "Epoch 611/1000, Loss: 3.0841991567250453\n",
      "Epoch 612/1000, Loss: 3.1087574290506765\n",
      "Epoch 613/1000, Loss: 3.138384035139373\n",
      "Epoch 614/1000, Loss: 3.1864357789357505\n",
      "Epoch 615/1000, Loss: 3.135960983507561\n",
      "Epoch 616/1000, Loss: 3.132852278875582\n",
      "Epoch 617/1000, Loss: 3.1682749896338493\n",
      "Epoch 618/1000, Loss: 3.1531721732833167\n",
      "Epoch 619/1000, Loss: 3.1609337040872285\n",
      "Epoch 620/1000, Loss: 3.1709540377963674\n",
      "Epoch 621/1000, Loss: 3.1085256590987695\n",
      "Epoch 622/1000, Loss: 3.0877335640517147\n",
      "Epoch 623/1000, Loss: 3.0746838143377593\n",
      "Epoch 624/1000, Loss: 3.2425232592857247\n",
      "Epoch 625/1000, Loss: 3.1576598133101608\n",
      "Epoch 626/1000, Loss: 3.1961520061348425\n",
      "Epoch 627/1000, Loss: 3.131947878635291\n",
      "Epoch 628/1000, Loss: 3.2019829045642507\n",
      "Epoch 629/1000, Loss: 3.070717122518655\n",
      "Epoch 630/1000, Loss: 3.1593127449353537\n",
      "Epoch 631/1000, Loss: 3.108834241375779\n",
      "Epoch 632/1000, Loss: 3.1185585323608285\n",
      "Epoch 633/1000, Loss: 3.1310960885250205\n",
      "Epoch 634/1000, Loss: 3.0790084239208335\n",
      "Epoch 635/1000, Loss: 3.120926188700127\n",
      "Epoch 636/1000, Loss: 3.062440041339759\n",
      "Epoch 637/1000, Loss: 3.139951178521821\n",
      "Epoch 638/1000, Loss: 3.047955746000463\n",
      "Epoch 639/1000, Loss: 3.0893005150737185\n",
      "Epoch 640/1000, Loss: 3.102918059536905\n",
      "Epoch 641/1000, Loss: 3.0683824022610984\n",
      "Epoch 642/1000, Loss: 3.193284674124284\n",
      "Epoch 643/1000, Loss: 3.0779079487829497\n",
      "Epoch 644/1000, Loss: 3.179630104339484\n",
      "Epoch 645/1000, Loss: 3.0816388003753894\n",
      "Epoch 646/1000, Loss: 3.106870730717977\n",
      "Epoch 647/1000, Loss: 3.120538927388914\n",
      "Epoch 648/1000, Loss: 3.1762376003193133\n",
      "Epoch 649/1000, Loss: 3.1351284538254593\n",
      "Epoch 650/1000, Loss: 3.0805252573706885\n",
      "Epoch 651/1000, Loss: 3.0703868667284646\n",
      "Epoch 652/1000, Loss: 3.124258518218994\n",
      "Epoch 653/1000, Loss: 3.08339037046288\n",
      "Epoch 654/1000, Loss: 3.091551824049516\n",
      "Epoch 655/1000, Loss: 3.09925539746429\n",
      "Epoch 656/1000, Loss: 3.1694267175414343\n",
      "Epoch 657/1000, Loss: 3.079890198779829\n",
      "Epoch 658/1000, Loss: 3.1118571198347844\n",
      "Epoch 659/1000, Loss: 3.108344970327435\n",
      "Epoch 660/1000, Loss: 3.0874981862125974\n",
      "Epoch 661/1000, Loss: 3.1448628649567114\n",
      "Epoch 662/1000, Loss: 3.1129130468224036\n",
      "Epoch 663/1000, Loss: 3.1057487961017722\n",
      "Epoch 664/1000, Loss: 3.0834360122680664\n",
      "Epoch 665/1000, Loss: 3.0835614655957078\n",
      "Epoch 666/1000, Loss: 3.162018159122178\n",
      "Epoch 667/1000, Loss: 3.0804183844364053\n",
      "Epoch 668/1000, Loss: 3.1289348277178677\n",
      "Epoch 669/1000, Loss: 3.0730262154882606\n",
      "Epoch 670/1000, Loss: 3.1201827237100312\n",
      "Epoch 671/1000, Loss: 3.0747474504239634\n",
      "Epoch 672/1000, Loss: 3.0280485731182676\n",
      "Epoch 673/1000, Loss: 3.0948857999209203\n",
      "Epoch 674/1000, Loss: 3.1075638261708347\n",
      "Epoch 675/1000, Loss: 3.0893178419633345\n",
      "Epoch 676/1000, Loss: 3.0808383867596136\n",
      "Epoch 677/1000, Loss: 3.114644304369435\n",
      "Epoch 678/1000, Loss: 3.1090488217093726\n",
      "Epoch 679/1000, Loss: 3.1082448598110313\n",
      "Epoch 680/1000, Loss: 3.114309159192172\n",
      "Epoch 681/1000, Loss: 3.124418130426696\n",
      "Epoch 682/1000, Loss: 3.1064433040040913\n",
      "Epoch 683/1000, Loss: 3.075780417883035\n",
      "Epoch 684/1000, Loss: 3.0366128285725913\n",
      "Epoch 685/1000, Loss: 3.1206476778695076\n",
      "Epoch 686/1000, Loss: 3.101596450263804\n",
      "Epoch 687/1000, Loss: 3.088349922136827\n",
      "Epoch 688/1000, Loss: 3.119497347058672\n",
      "Epoch 689/1000, Loss: 3.0995929421800557\n",
      "Epoch 690/1000, Loss: 3.0768757083199243\n",
      "Epoch 691/1000, Loss: 3.130118657242168\n",
      "Epoch 692/1000, Loss: 3.1285047440817864\n",
      "Epoch 693/1000, Loss: 3.001949176643834\n",
      "Epoch 694/1000, Loss: 3.092611167467002\n",
      "Epoch 695/1000, Loss: 3.076361801588174\n",
      "Epoch 696/1000, Loss: 2.986811496994712\n",
      "Epoch 697/1000, Loss: 3.157563070456187\n",
      "Epoch 698/1000, Loss: 3.1410452633193047\n",
      "Epoch 699/1000, Loss: 3.107377135392391\n",
      "Epoch 700/1000, Loss: 3.128749719171813\n",
      "Epoch 701/1000, Loss: 3.0699340863661333\n",
      "Epoch 702/1000, Loss: 3.109388731645815\n",
      "Epoch 703/1000, Loss: 3.12091628891049\n",
      "Epoch 704/1000, Loss: 3.039272064512426\n",
      "Epoch 705/1000, Loss: 3.0339694104411383\n",
      "Epoch 706/1000, Loss: 3.111759496457649\n",
      "Epoch 707/1000, Loss: 3.0346770376870125\n",
      "Epoch 708/1000, Loss: 3.063383933269616\n",
      "Epoch 709/1000, Loss: 3.125787427931121\n",
      "Epoch 710/1000, Loss: 3.115967119281942\n",
      "Epoch 711/1000, Loss: 3.09278215693705\n",
      "Epoch 712/1000, Loss: 3.062936170534654\n",
      "Epoch 713/1000, Loss: 3.123426416606614\n",
      "Epoch 714/1000, Loss: 3.056123632373232\n",
      "Epoch 715/1000, Loss: 2.973825533281673\n",
      "Epoch 716/1000, Loss: 3.07177464167277\n",
      "Epoch 717/1000, Loss: 3.115638713041941\n",
      "Epoch 718/1000, Loss: 3.1486448618498715\n",
      "Epoch 719/1000, Loss: 3.108521331440319\n",
      "Epoch 720/1000, Loss: 3.0267964529268667\n",
      "Epoch 721/1000, Loss: 3.0549308693770207\n",
      "Epoch 722/1000, Loss: 3.044628330252387\n",
      "Epoch 723/1000, Loss: 3.0447592608856433\n",
      "Epoch 724/1000, Loss: 3.0355660662506567\n",
      "Epoch 725/1000, Loss: 3.12376667694612\n",
      "Epoch 726/1000, Loss: 3.033746367151087\n",
      "Epoch 727/1000, Loss: 3.000812797835379\n",
      "Epoch 728/1000, Loss: 3.116046262509895\n",
      "Epoch 729/1000, Loss: 3.021096139243155\n",
      "Epoch 730/1000, Loss: 3.0001179268865874\n",
      "Epoch 731/1000, Loss: 3.1009655206492455\n",
      "Epoch 732/1000, Loss: 3.1079631185892858\n",
      "Epoch 733/1000, Loss: 3.0567818428530837\n",
      "Epoch 734/1000, Loss: 3.07273602846897\n",
      "Epoch 735/1000, Loss: 3.0474870683568898\n",
      "Epoch 736/1000, Loss: 3.0082656379902\n",
      "Epoch 737/1000, Loss: 3.123345658634648\n",
      "Epoch 738/1000, Loss: 3.034818463253252\n",
      "Epoch 739/1000, Loss: 3.0121389681642707\n",
      "Epoch 740/1000, Loss: 3.0252335143811777\n",
      "Epoch 741/1000, Loss: 2.985073142882549\n",
      "Epoch 742/1000, Loss: 3.000785798737497\n",
      "Epoch 743/1000, Loss: 3.0441568981517446\n",
      "Epoch 744/1000, Loss: 3.0923218979980005\n",
      "Epoch 745/1000, Loss: 3.077311226815888\n",
      "Epoch 746/1000, Loss: 3.0703631494984482\n",
      "Epoch 747/1000, Loss: 3.04753854599866\n",
      "Epoch 748/1000, Loss: 3.06473247571425\n",
      "Epoch 749/1000, Loss: 3.1141350377689707\n",
      "Epoch 750/1000, Loss: 3.0403935746713118\n",
      "Epoch 751/1000, Loss: 3.09203814015244\n",
      "Epoch 752/1000, Loss: 3.0430444623484756\n",
      "Epoch 753/1000, Loss: 3.035263021787008\n",
      "Epoch 754/1000, Loss: 3.072807743693843\n",
      "Epoch 755/1000, Loss: 3.0453878487601425\n",
      "Epoch 756/1000, Loss: 3.0321229750459846\n",
      "Epoch 757/1000, Loss: 3.022358611677632\n",
      "Epoch 758/1000, Loss: 3.074588398138682\n",
      "Epoch 759/1000, Loss: 3.025455435117086\n",
      "Epoch 760/1000, Loss: 2.9991887789784055\n",
      "Epoch 761/1000, Loss: 2.980920440319813\n",
      "Epoch 762/1000, Loss: 3.000119756568562\n",
      "Epoch 763/1000, Loss: 3.023140778144201\n",
      "Epoch 764/1000, Loss: 3.0817177846576227\n",
      "Epoch 765/1000, Loss: 3.053718159596125\n",
      "Epoch 766/1000, Loss: 3.066196053317099\n",
      "Epoch 767/1000, Loss: 3.0063186974236458\n",
      "Epoch 768/1000, Loss: 3.0583220611919057\n",
      "Epoch 769/1000, Loss: 3.057577419461626\n",
      "Epoch 770/1000, Loss: 3.023006498813629\n",
      "Epoch 771/1000, Loss: 3.0383661353226863\n",
      "Epoch 772/1000, Loss: 3.06151336070263\n",
      "Epoch 773/1000, Loss: 3.046051783995195\n",
      "Epoch 774/1000, Loss: 3.0596590863935873\n",
      "Epoch 775/1000, Loss: 3.0184500470305933\n",
      "Epoch 776/1000, Loss: 3.1075901795517313\n",
      "Epoch 777/1000, Loss: 3.026456847335353\n",
      "Epoch 778/1000, Loss: 3.0470127056945455\n",
      "Epoch 779/1000, Loss: 3.045986595478925\n",
      "Epoch 780/1000, Loss: 3.0702602484009485\n",
      "Epoch 781/1000, Loss: 3.031136455861005\n",
      "Epoch 782/1000, Loss: 3.0280669178023483\n",
      "Epoch 783/1000, Loss: 3.0383476199525776\n",
      "Epoch 784/1000, Loss: 2.988070730007056\n",
      "Epoch 785/1000, Loss: 3.0043931260253443\n",
      "Epoch 786/1000, Loss: 3.0825324401710974\n",
      "Epoch 787/1000, Loss: 3.0032826779466686\n",
      "Epoch 788/1000, Loss: 3.018510867248882\n",
      "Epoch 789/1000, Loss: 3.054102984341708\n",
      "Epoch 790/1000, Loss: 3.000458406679558\n",
      "Epoch 791/1000, Loss: 3.0818117595080174\n",
      "Epoch 792/1000, Loss: 3.0209562399170617\n",
      "Epoch 793/1000, Loss: 3.0476078978090575\n",
      "Epoch 794/1000, Loss: 2.9888748318860023\n",
      "Epoch 795/1000, Loss: 3.0459479653474055\n",
      "Epoch 796/1000, Loss: 2.9880604527213355\n",
      "Epoch 797/1000, Loss: 3.020537976062659\n",
      "Epoch 798/1000, Loss: 3.0386904767065337\n",
      "Epoch 799/1000, Loss: 2.9452892090335037\n",
      "Epoch 800/1000, Loss: 3.007588063225602\n",
      "Epoch 801/1000, Loss: 2.9673229040521565\n",
      "Epoch 802/1000, Loss: 3.0504734163934533\n",
      "Epoch 803/1000, Loss: 3.0358147350224582\n",
      "Epoch 804/1000, Loss: 3.0992485465425434\n",
      "Epoch 805/1000, Loss: 3.0176531067400267\n",
      "Epoch 806/1000, Loss: 3.074035088221232\n",
      "Epoch 807/1000, Loss: 3.0386686370228277\n",
      "Epoch 808/1000, Loss: 3.0130848550435267\n",
      "Epoch 809/1000, Loss: 3.036479890346527\n",
      "Epoch 810/1000, Loss: 3.0399419394406406\n",
      "Epoch 811/1000, Loss: 3.0392631111723003\n",
      "Epoch 812/1000, Loss: 3.0558195330879907\n",
      "Epoch 813/1000, Loss: 3.044117181590109\n",
      "Epoch 814/1000, Loss: 3.011851734284199\n",
      "Epoch 815/1000, Loss: 2.997849354238221\n",
      "Epoch 816/1000, Loss: 2.961321944540197\n",
      "Epoch 817/1000, Loss: 3.0397267160993633\n",
      "Epoch 818/1000, Loss: 2.990603174224044\n",
      "Epoch 819/1000, Loss: 2.996982749664422\n",
      "Epoch 820/1000, Loss: 3.0323301273764987\n",
      "Epoch 821/1000, Loss: 3.1576313412550725\n",
      "Epoch 822/1000, Loss: 2.968991377136924\n",
      "Epoch 823/1000, Loss: 3.054260382146546\n",
      "Epoch 824/1000, Loss: 3.0228595128565123\n",
      "Epoch 825/1000, Loss: 3.0315392143798596\n",
      "Epoch 826/1000, Loss: 3.0423679442116707\n",
      "Epoch 827/1000, Loss: 2.9951526910969704\n",
      "Epoch 828/1000, Loss: 3.0219144216089537\n",
      "Epoch 829/1000, Loss: 3.067594436081973\n",
      "Epoch 830/1000, Loss: 3.006558832797137\n",
      "Epoch 831/1000, Loss: 2.930641320618716\n",
      "Epoch 832/1000, Loss: 3.0438679077408533\n",
      "Epoch 833/1000, Loss: 3.032160188212539\n",
      "Epoch 834/1000, Loss: 3.0315085279219076\n",
      "Epoch 835/1000, Loss: 3.0904039298043107\n",
      "Epoch 836/1000, Loss: 2.9957939603111963\n",
      "Epoch 837/1000, Loss: 2.999191254377365\n",
      "Epoch 838/1000, Loss: 2.9700819831905942\n",
      "Epoch 839/1000, Loss: 2.9694861742583187\n",
      "Epoch 840/1000, Loss: 3.00286363381328\n",
      "Epoch 841/1000, Loss: 2.99268221132683\n",
      "Epoch 842/1000, Loss: 3.0128877108747307\n",
      "Epoch 843/1000, Loss: 2.9794668826189907\n",
      "Epoch 844/1000, Loss: 2.979422435616002\n",
      "Epoch 845/1000, Loss: 2.9707168087814795\n",
      "Epoch 846/1000, Loss: 3.0251993816910367\n",
      "Epoch 847/1000, Loss: 2.956853830453121\n",
      "Epoch 848/1000, Loss: 3.0736128010533075\n",
      "Epoch 849/1000, Loss: 2.9918588607600243\n",
      "Epoch 850/1000, Loss: 3.039818359143806\n",
      "Epoch 851/1000, Loss: 3.007752717444391\n",
      "Epoch 852/1000, Loss: 2.8850230755227986\n",
      "Epoch 853/1000, Loss: 3.0733085048921183\n",
      "Epoch 854/1000, Loss: 3.0046854705521553\n",
      "Epoch 855/1000, Loss: 2.929732005704533\n",
      "Epoch 856/1000, Loss: 3.016488088802858\n",
      "Epoch 857/1000, Loss: 2.998385115103288\n",
      "Epoch 858/1000, Loss: 2.956019954247908\n",
      "Epoch 859/1000, Loss: 2.975995157704209\n",
      "Epoch 860/1000, Loss: 3.043798411434347\n",
      "Epoch 861/1000, Loss: 3.0061545444257334\n",
      "Epoch 862/1000, Loss: 2.9183800238551516\n",
      "Epoch 863/1000, Loss: 2.945612016952399\n",
      "Epoch 864/1000, Loss: 3.0299841355193746\n",
      "Epoch 865/1000, Loss: 2.947033711455085\n",
      "Epoch 866/1000, Loss: 3.00562271566102\n",
      "Epoch 867/1000, Loss: 3.0075480314818295\n",
      "Epoch 868/1000, Loss: 2.9736025152784404\n",
      "Epoch 869/1000, Loss: 3.0035319228967032\n",
      "Epoch 870/1000, Loss: 2.9673031333721047\n",
      "Epoch 871/1000, Loss: 3.0242728184569967\n",
      "Epoch 872/1000, Loss: 3.0446402538906443\n",
      "Epoch 873/1000, Loss: 3.0107345797798852\n",
      "Epoch 874/1000, Loss: 2.9391774751923303\n",
      "Epoch 875/1000, Loss: 2.976404735536286\n",
      "Epoch 876/1000, Loss: 3.0519852864019796\n",
      "Epoch 877/1000, Loss: 3.0240022829084685\n",
      "Epoch 878/1000, Loss: 3.0517201965505425\n",
      "Epoch 879/1000, Loss: 2.9832442899545035\n",
      "Epoch 880/1000, Loss: 2.971400847940734\n",
      "Epoch 881/1000, Loss: 3.0049417398192664\n",
      "Epoch 882/1000, Loss: 2.921375569069024\n",
      "Epoch 883/1000, Loss: 2.9653998044404117\n",
      "Epoch 884/1000, Loss: 2.9785225210767803\n",
      "Epoch 885/1000, Loss: 2.9840474553180463\n",
      "Epoch 886/1000, Loss: 2.955077405228759\n",
      "Epoch 887/1000, Loss: 2.995673735936483\n",
      "Epoch 888/1000, Loss: 3.00051525144866\n",
      "Epoch 889/1000, Loss: 3.0289740390849835\n",
      "Epoch 890/1000, Loss: 3.054931194493265\n",
      "Epoch 891/1000, Loss: 2.9838437347701103\n",
      "Epoch 892/1000, Loss: 2.9764390190442405\n",
      "Epoch 893/1000, Loss: 2.9896233479181924\n",
      "Epoch 894/1000, Loss: 2.973096486293908\n",
      "Epoch 895/1000, Loss: 3.000603223388845\n",
      "Epoch 896/1000, Loss: 2.9405132098631426\n",
      "Epoch 897/1000, Loss: 3.0189899567401772\n",
      "Epoch 898/1000, Loss: 2.9154106270183218\n",
      "Epoch 899/1000, Loss: 2.9781465232372284\n",
      "Epoch 900/1000, Loss: 3.0169992862325725\n",
      "Epoch 901/1000, Loss: 2.9825083035411257\n",
      "Epoch 902/1000, Loss: 2.9577035651062475\n",
      "Epoch 903/1000, Loss: 2.9704700031063775\n",
      "Epoch 904/1000, Loss: 2.9896066152688228\n",
      "Epoch 905/1000, Loss: 2.9790018426649496\n",
      "Epoch 906/1000, Loss: 2.956691167571328\n",
      "Epoch 907/1000, Loss: 2.979420577034806\n",
      "Epoch 908/1000, Loss: 2.9764656261964277\n",
      "Epoch 909/1000, Loss: 2.957532937779571\n",
      "Epoch 910/1000, Loss: 2.976425913247195\n",
      "Epoch 911/1000, Loss: 2.937509595444708\n",
      "Epoch 912/1000, Loss: 2.955170397505616\n",
      "Epoch 913/1000, Loss: 2.9231287213889035\n",
      "Epoch 914/1000, Loss: 3.00102412610343\n",
      "Epoch 915/1000, Loss: 2.934911518385916\n",
      "Epoch 916/1000, Loss: 3.0326853719624607\n",
      "Epoch 917/1000, Loss: 2.9874727075750176\n",
      "Epoch 918/1000, Loss: 2.959748126340635\n",
      "Epoch 919/1000, Loss: 2.9828088039701637\n",
      "Epoch 920/1000, Loss: 2.9522289100921517\n",
      "Epoch 921/1000, Loss: 2.9676261896436866\n",
      "Epoch 922/1000, Loss: 2.9691063635277026\n",
      "Epoch 923/1000, Loss: 2.9784271653854484\n",
      "Epoch 924/1000, Loss: 2.942979664513559\n",
      "Epoch 925/1000, Loss: 2.9714203151789578\n",
      "Epoch 926/1000, Loss: 2.9001120952042667\n",
      "Epoch 927/1000, Loss: 2.958994535785733\n",
      "Epoch 928/1000, Loss: 3.004996550805641\n",
      "Epoch 929/1000, Loss: 2.9452045397324995\n",
      "Epoch 930/1000, Loss: 2.99470869790424\n",
      "Epoch 931/1000, Loss: 2.9185179526155647\n",
      "Epoch 932/1000, Loss: 2.926855956966227\n",
      "Epoch 933/1000, Loss: 2.932552990588275\n",
      "Epoch 934/1000, Loss: 2.9425750898592398\n",
      "Epoch 935/1000, Loss: 2.9631535600532186\n",
      "Epoch 936/1000, Loss: 2.916787488894029\n",
      "Epoch 937/1000, Loss: 2.9784460428989297\n",
      "Epoch 938/1000, Loss: 2.9378738845839645\n",
      "Epoch 939/1000, Loss: 2.9275479569579614\n",
      "Epoch 940/1000, Loss: 2.9101740501143714\n",
      "Epoch 941/1000, Loss: 2.975360264380773\n",
      "Epoch 942/1000, Loss: 2.9527167605631277\n",
      "Epoch 943/1000, Loss: 2.993760547854684\n",
      "Epoch 944/1000, Loss: 2.999190390110016\n",
      "Epoch 945/1000, Loss: 3.009571070020849\n",
      "Epoch 946/1000, Loss: 2.9881477139212866\n",
      "Epoch 947/1000, Loss: 2.9052161447929614\n",
      "Epoch 948/1000, Loss: 2.986729728453087\n",
      "Epoch 949/1000, Loss: 2.9820058950872133\n",
      "Epoch 950/1000, Loss: 2.906398027232199\n",
      "Epoch 951/1000, Loss: 2.9582282837593192\n",
      "Epoch 952/1000, Loss: 2.9312957895524576\n",
      "Epoch 953/1000, Loss: 3.0219598042242453\n",
      "Epoch 954/1000, Loss: 2.9536397872549114\n",
      "Epoch 955/1000, Loss: 2.9504465036319965\n",
      "Epoch 956/1000, Loss: 3.010943616881515\n",
      "Epoch 957/1000, Loss: 2.9111050313169304\n",
      "Epoch 958/1000, Loss: 2.955796771880352\n",
      "Epoch 959/1000, Loss: 2.8841235475106672\n",
      "Epoch 960/1000, Loss: 2.9573407570521035\n",
      "Epoch 961/1000, Loss: 2.9844067159927254\n",
      "Epoch 962/1000, Loss: 3.0272511126417103\n",
      "Epoch 963/1000, Loss: 2.926972744139758\n",
      "Epoch 964/1000, Loss: 3.000221365328991\n",
      "Epoch 965/1000, Loss: 2.967236098014947\n",
      "Epoch 966/1000, Loss: 2.948828063227914\n",
      "Epoch 967/1000, Loss: 2.9425751277894685\n",
      "Epoch 968/1000, Loss: 2.9472624641476255\n",
      "Epoch 969/1000, Loss: 3.0129842433062466\n",
      "Epoch 970/1000, Loss: 2.9848627150058746\n",
      "Epoch 971/1000, Loss: 2.9808364889838477\n",
      "Epoch 972/1000, Loss: 2.898856288555897\n",
      "Epoch 973/1000, Loss: 2.8981535254102764\n",
      "Epoch 974/1000, Loss: 2.8901087217258685\n",
      "Epoch 975/1000, Loss: 2.9334310564127835\n",
      "Epoch 976/1000, Loss: 2.9380729577758093\n",
      "Epoch 977/1000, Loss: 2.893218662702676\n",
      "Epoch 978/1000, Loss: 2.9296867432016316\n",
      "Epoch 979/1000, Loss: 2.9349431160724526\n",
      "Epoch 980/1000, Loss: 2.934451028253093\n",
      "Epoch 981/1000, Loss: 2.9312699166211216\n",
      "Epoch 982/1000, Loss: 2.9126801653341814\n",
      "Epoch 983/1000, Loss: 2.9135649637742476\n",
      "Epoch 984/1000, Loss: 2.9682452687711427\n",
      "Epoch 985/1000, Loss: 3.0046517234860044\n",
      "Epoch 986/1000, Loss: 2.9756254680228955\n",
      "Epoch 987/1000, Loss: 2.940467171596758\n",
      "Epoch 988/1000, Loss: 2.8974789471337288\n",
      "Epoch 989/1000, Loss: 2.9865825257518073\n",
      "Epoch 990/1000, Loss: 2.8543987147735828\n",
      "Epoch 991/1000, Loss: 2.9138793177676923\n",
      "Epoch 992/1000, Loss: 2.893576803532514\n",
      "Epoch 993/1000, Loss: 2.9534736943967417\n",
      "Epoch 994/1000, Loss: 2.9479939865343496\n",
      "Epoch 995/1000, Loss: 2.9734700295058163\n",
      "Epoch 996/1000, Loss: 2.9578087492422624\n",
      "Epoch 997/1000, Loss: 3.037477683840376\n",
      "Epoch 998/1000, Loss: 2.8905069412607136\n",
      "Epoch 999/1000, Loss: 2.9411201269337623\n",
      "Epoch 1000/1000, Loss: 2.9151987252813396\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets_G_act, targets_G_r in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        outputs_G_act, outputs_G_r = outputs  # Separate the outputs into two tensors\n",
    "        loss_G_act = criterion(outputs_G_act, targets_G_act.unsqueeze(1))\n",
    "        loss_G_r = criterion(outputs_G_r, targets_G_r.unsqueeze(1))\n",
    "        loss = loss_G_act + loss_G_r\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Loss G_act: 3.3577062072175923\n",
      "Average Test Loss G_r: 4.0672737468372695\n",
      "Mean Squared Error (MSE): 64.6612\n",
      "Mean Absolute Error (MAE): 8.3677\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the testing data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss_G_act = 0.0\n",
    "    test_loss_G_r = 0.0\n",
    "    total_mse = 0.0\n",
    "    total_mae = 0.0\n",
    "\n",
    "    for inputs, targets_G_act, targets_G_r in test_dataloader:\n",
    "        outputs = model(inputs)\n",
    "        outputs_G_act, outputs_G_r = outputs  # Separate the outputs into two tensors\n",
    "\n",
    "        loss_G_act = criterion(outputs_G_act, targets_G_act.unsqueeze(1))\n",
    "        loss_G_r = criterion(outputs_G_r, targets_G_r.unsqueeze(1))\n",
    "\n",
    "        test_loss_G_act += loss_G_act.item()\n",
    "        test_loss_G_r += loss_G_r.item()\n",
    "\n",
    "        mse_G_act = mean_squared_error(targets_G_act.cpu(), outputs_G_act.cpu().detach())\n",
    "        mae_G_act = mean_absolute_error(targets_G_act.cpu(), outputs_G_act.cpu().detach())\n",
    "        mse_G_r = mean_squared_error(targets_G_r.cpu(), outputs_G_r.cpu().detach())\n",
    "        mae_G_r = mean_absolute_error(targets_G_r.cpu(), outputs_G_r.cpu().detach())\n",
    "\n",
    "        total_mse += mse_G_act.item() + mse_G_r.item()\n",
    "        total_mae += mae_G_act.item() + mae_G_r.item()\n",
    "\n",
    "    average_test_loss_G_act = test_loss_G_act / len(test_dataloader)\n",
    "    average_test_loss_G_r = test_loss_G_r / len(test_dataloader)\n",
    "\n",
    "    avg_mse = total_mse / len(test_dataloader)\n",
    "    avg_mae = total_mae / len(test_dataloader)\n",
    "\n",
    "    print(f\"Average Test Loss G_act: {average_test_loss_G_act}\")\n",
    "    print(f\"Average Test Loss G_r: {average_test_loss_G_r}\")\n",
    "    \n",
    "    print(f\"Mean Squared Error (MSE): {avg_mse:.4f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {avg_mae:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For showing how it works, we will use the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G_act 0     15.875896\n",
      "1     15.155477\n",
      "2     18.013824\n",
      "3     23.687079\n",
      "4     23.438094\n",
      "5     20.969801\n",
      "6     11.722625\n",
      "7      7.268044\n",
      "8      6.289673\n",
      "9     16.044475\n",
      "10    13.604005\n",
      "Name: G_act, dtype: float64 G_r 0    -51.881526\n",
      "1    -51.398681\n",
      "2    -66.822349\n",
      "3    -62.481289\n",
      "4    -84.836459\n",
      "5    -57.785046\n",
      "6    -63.721331\n",
      "7    -86.996102\n",
      "8    -86.799671\n",
      "9    -50.409197\n",
      "10   -51.008126\n",
      "Name: G_r, dtype: float64  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read the dataset\n",
    "df = pd.read_csv('debuging_dataset.csv')\n",
    "\n",
    "# Extract the SMILES strings\n",
    "smiles = df['rxn_smiles']\n",
    "\n",
    "print(f\"G_act {df['G_act']} G_r {df['G_r']}  \")\n",
    "\n",
    "# Encode SMILES strings to fingerprints\n",
    "X_fingerprints = []\n",
    "\n",
    "for idx, smiles in enumerate(smiles):\n",
    "    reactants, products = smiles.split('>>')\n",
    "    reactant_fingerprints = [encode_smiles(reactant) for reactant in reactants.split('.')]\n",
    "    product_fingerprints = [encode_smiles(product) for product in products.split('.')]\n",
    "    X_fingerprints.append(reactant_fingerprints + product_fingerprints)\n",
    "\n",
    "# Convert the list of fingerprints and target values to tensors\n",
    "X_test = torch.stack([torch.cat(fingerprints) for fingerprints in X_fingerprints])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for G_act:\n",
      "15.393006\n",
      "15.140726\n",
      "18.152287\n",
      "23.367092\n",
      "23.329334\n",
      "20.518341\n",
      "12.146641\n",
      "6.154443\n",
      "6.6333065\n",
      "16.150503\n",
      "13.550837\n",
      "Predictions for G_r:\n",
      "-51.71132\n",
      "-50.58262\n",
      "-65.53673\n",
      "-62.80134\n",
      "-83.415535\n",
      "-56.07112\n",
      "-63.67861\n",
      "-87.32187\n",
      "-78.072044\n",
      "-49.272835\n",
      "-50.920586\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Make predictions on test inputs\n",
    "with torch.no_grad():\n",
    "    test_inputs = X_test  \n",
    "    predictions_G_act, predictions_G_r = model(test_inputs)  # Separate the predictions into two tensors\n",
    "\n",
    "# Convert predictions to numpy arrays\n",
    "predictions_G_act = predictions_G_act.numpy()\n",
    "predictions_G_r = predictions_G_r.numpy()\n",
    "\n",
    "\n",
    "print(\"Predictions for G_act:\")\n",
    "for pred in predictions_G_act:\n",
    "    print(pred[0])\n",
    "\n",
    "print(\"Predictions for G_r:\")\n",
    "for pred in predictions_G_r:\n",
    "    print(pred[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
