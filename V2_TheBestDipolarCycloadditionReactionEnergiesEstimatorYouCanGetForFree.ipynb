{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\python39\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\andri\\appdata\\roaming\\python\\python39\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\python39\\lib\\site-packages (from pandas) (1.24.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python39\\lib\\site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\andri\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\python39\\lib\\site-packages (1.24.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\python39\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python39\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\python39\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\python39\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\python39\\lib\\site-packages (from scikit-learn) (1.24.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\python39\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\python39\\lib\\site-packages (from torch) (4.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rdkit in c:\\python39\\lib\\site-packages (2023.3.1)\n",
      "Requirement already satisfied: Pillow in c:\\python39\\lib\\site-packages (from rdkit) (9.4.0)\n",
      "Requirement already satisfied: numpy in c:\\python39\\lib\\site-packages (from rdkit) (1.24.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deepchem"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Python39\\\\Lib\\\\site-packages\\\\~cipy.libs\\\\libopenblas-802f9ed1179cb9c9b03d67ff79f48187.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "WARNING: You are using pip version 21.1.3; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading deepchem-2.7.1-py3-none-any.whl (693 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\python39\\lib\\site-packages (from deepchem) (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.21 in c:\\python39\\lib\\site-packages (from deepchem) (1.24.2)\n",
      "Requirement already satisfied: joblib in c:\\python39\\lib\\site-packages (from deepchem) (1.2.0)\n",
      "Requirement already satisfied: rdkit in c:\\python39\\lib\\site-packages (from deepchem) (2023.3.1)\n",
      "Requirement already satisfied: pandas in c:\\python39\\lib\\site-packages (from deepchem) (1.5.3)\n",
      "Collecting scipy<1.9\n",
      "  Downloading scipy-1.8.1-cp39-cp39-win_amd64.whl (36.9 MB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python39\\lib\\site-packages (from pandas->deepchem) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\andri\\appdata\\roaming\\python\\python39\\site-packages (from pandas->deepchem) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\andri\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->deepchem) (1.16.0)\n",
      "Requirement already satisfied: Pillow in c:\\python39\\lib\\site-packages (from rdkit->deepchem) (9.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python39\\lib\\site-packages (from scikit-learn->deepchem) (3.1.0)\n",
      "Installing collected packages: scipy, deepchem\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.10.1\n",
      "    Uninstalling scipy-1.10.1:\n",
      "      Successfully uninstalled scipy-1.10.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install scikit-learn\n",
    "!{sys.executable} -m pip install torch\n",
    "!{sys.executable} -m pip install rdkit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Reaction dataset class\n",
    "class ReactionDataset(Dataset):\n",
    "    def __init__(self, X, y_G_act, y_G_r):\n",
    "        self.X = X\n",
    "        self.y_G_act = y_G_act\n",
    "        self.y_G_r = y_G_r\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y_G_act[idx], self.y_G_r[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "df = pd.read_csv('full_dataset.csv') \n",
    "\n",
    "# Prepare the feature matrix X and target variables y\n",
    "X_smiles = df['rxn_smiles']\n",
    "y_G_act = df['G_act']\n",
    "y_G_r = df['G_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([5269, 3072])\n"
     ]
    }
   ],
   "source": [
    "# Define the encoding function\n",
    "def encode_smiles(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=1024)\n",
    "    bitstring = fp.ToBitString()\n",
    "    features = np.array([int(bit) for bit in bitstring], dtype=np.float32)\n",
    "    features_tensor = torch.tensor(features)\n",
    "    return features_tensor\n",
    "\n",
    "# Encode SMILES strings to fingerprints\n",
    "X_fingerprints = []\n",
    "\n",
    "for idx, smiles in enumerate(X_smiles):\n",
    "    reactants, products = smiles.split('>>')\n",
    "    reactant_fingerprints = [encode_smiles(reactant) for reactant in reactants.split('.')]\n",
    "    product_fingerprints = [encode_smiles(product) for product in products.split('.')]\n",
    "    X_fingerprints.append(reactant_fingerprints + product_fingerprints)\n",
    "\n",
    "# Convert the list of fingerprints and target values to tensors\n",
    "X_tensor = torch.stack([torch.cat(fingerprints) for fingerprints in X_fingerprints])\n",
    "\n",
    "# Print the fingerprint tensor\n",
    "print(X_tensor)\n",
    "print(X_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data into PyTorch tensors\n",
    "y_G_act_tensor = torch.tensor(y_G_act.values, dtype=torch.float32)\n",
    "y_G_r_tensor = torch.tensor(y_G_r.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_G_act_train, y_G_act_test, y_G_r_train, y_G_r_test = train_test_split(X_tensor, y_G_act_tensor, y_G_r_tensor, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.fc4_act = nn.Linear(64, 1)  # Output layer for G_act\n",
    "        self.fc4_r = nn.Linear(64, 1)  # Output layer for G_r\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout3(x)\n",
    "        output_act = self.fc4_act(x)  # Output for G_act\n",
    "        output_r = self.fc4_r(x)  # Output for G_r\n",
    "        return output_act, output_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = NeuralNetwork(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.SmoothL1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data loaders\n",
    "train_dataset = ReactionDataset(X_train, y_G_act_train, y_G_r_train)\n",
    "test_dataset = ReactionDataset(X_test, y_G_act_test, y_G_r_test)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 21.89886762156631\n",
      "Epoch 2/1000, Loss: 13.191494486548685\n",
      "Epoch 3/1000, Loss: 10.78838198112719\n",
      "Epoch 4/1000, Loss: 9.930351156176943\n",
      "Epoch 5/1000, Loss: 9.428149804924473\n",
      "Epoch 6/1000, Loss: 8.972054615165248\n",
      "Epoch 7/1000, Loss: 8.739297231038412\n",
      "Epoch 8/1000, Loss: 8.557795336752227\n",
      "Epoch 9/1000, Loss: 8.184347008213852\n",
      "Epoch 10/1000, Loss: 8.142174930283517\n",
      "Epoch 11/1000, Loss: 7.974739851373615\n",
      "Epoch 12/1000, Loss: 7.828078277183302\n",
      "Epoch 13/1000, Loss: 7.63043111743349\n",
      "Epoch 14/1000, Loss: 7.7647493644194165\n",
      "Epoch 15/1000, Loss: 7.532644712563717\n",
      "Epoch 16/1000, Loss: 7.481084960879701\n",
      "Epoch 17/1000, Loss: 7.369630354823488\n",
      "Epoch 18/1000, Loss: 7.262411554654439\n",
      "Epoch 19/1000, Loss: 7.217942906148506\n",
      "Epoch 20/1000, Loss: 7.214967398932486\n",
      "Epoch 21/1000, Loss: 7.1029068159334585\n",
      "Epoch 22/1000, Loss: 7.050482005784006\n",
      "Epoch 23/1000, Loss: 6.865694060470119\n",
      "Epoch 24/1000, Loss: 6.910609678788618\n",
      "Epoch 25/1000, Loss: 6.806640531077529\n",
      "Epoch 26/1000, Loss: 6.894545193874475\n",
      "Epoch 27/1000, Loss: 6.705290158589681\n",
      "Epoch 28/1000, Loss: 6.770007971561316\n",
      "Epoch 29/1000, Loss: 6.58220737991911\n",
      "Epoch 30/1000, Loss: 6.5912184968139185\n",
      "Epoch 31/1000, Loss: 6.667784983461553\n",
      "Epoch 32/1000, Loss: 6.571837905681495\n",
      "Epoch 33/1000, Loss: 6.514128107013124\n",
      "Epoch 34/1000, Loss: 6.359530239394217\n",
      "Epoch 35/1000, Loss: 6.416171211184877\n",
      "Epoch 36/1000, Loss: 6.3046161731084185\n",
      "Epoch 37/1000, Loss: 6.191437854911342\n",
      "Epoch 38/1000, Loss: 6.120521675456654\n",
      "Epoch 39/1000, Loss: 6.129394188071743\n",
      "Epoch 40/1000, Loss: 6.157491514177034\n",
      "Epoch 41/1000, Loss: 6.156971353473085\n",
      "Epoch 42/1000, Loss: 6.156964468233513\n",
      "Epoch 43/1000, Loss: 6.006495233738061\n",
      "Epoch 44/1000, Loss: 5.935024832234238\n",
      "Epoch 45/1000, Loss: 5.934369647141659\n",
      "Epoch 46/1000, Loss: 5.931284293983921\n",
      "Epoch 47/1000, Loss: 5.938270238312808\n",
      "Epoch 48/1000, Loss: 5.896844831379977\n",
      "Epoch 49/1000, Loss: 5.913530830181006\n",
      "Epoch 50/1000, Loss: 5.780051776857087\n",
      "Epoch 51/1000, Loss: 5.8430061918316465\n",
      "Epoch 52/1000, Loss: 5.890977776411808\n",
      "Epoch 53/1000, Loss: 5.719164171002128\n",
      "Epoch 54/1000, Loss: 5.7564792091196235\n",
      "Epoch 55/1000, Loss: 5.5508610010147095\n",
      "Epoch 56/1000, Loss: 5.607495528278929\n",
      "Epoch 57/1000, Loss: 5.623412399580984\n",
      "Epoch 58/1000, Loss: 5.563660979270935\n",
      "Epoch 59/1000, Loss: 5.516446245439125\n",
      "Epoch 60/1000, Loss: 5.596598081516497\n",
      "Epoch 61/1000, Loss: 5.517814240672371\n",
      "Epoch 62/1000, Loss: 5.392644741318443\n",
      "Epoch 63/1000, Loss: 5.504428025447961\n",
      "Epoch 64/1000, Loss: 5.52822204430898\n",
      "Epoch 65/1000, Loss: 5.435426041935429\n",
      "Epoch 66/1000, Loss: 5.4748462185715185\n",
      "Epoch 67/1000, Loss: 5.373459927963488\n",
      "Epoch 68/1000, Loss: 5.3740100047805095\n",
      "Epoch 69/1000, Loss: 5.46084519588586\n",
      "Epoch 70/1000, Loss: 5.310808851863399\n",
      "Epoch 71/1000, Loss: 5.30654183662299\n",
      "Epoch 72/1000, Loss: 5.37040014519836\n",
      "Epoch 73/1000, Loss: 5.31586755586393\n",
      "Epoch 74/1000, Loss: 5.283998946348826\n",
      "Epoch 75/1000, Loss: 5.297242247697079\n",
      "Epoch 76/1000, Loss: 5.260975180250226\n",
      "Epoch 77/1000, Loss: 5.1985506884979475\n",
      "Epoch 78/1000, Loss: 5.181778038993026\n",
      "Epoch 79/1000, Loss: 5.238997674349583\n",
      "Epoch 80/1000, Loss: 5.14086432167978\n",
      "Epoch 81/1000, Loss: 5.060353604230014\n",
      "Epoch 82/1000, Loss: 5.0583520387158245\n",
      "Epoch 83/1000, Loss: 5.06796619386384\n",
      "Epoch 84/1000, Loss: 5.090725486928767\n",
      "Epoch 85/1000, Loss: 4.979491241050489\n",
      "Epoch 86/1000, Loss: 5.027168633359851\n",
      "Epoch 87/1000, Loss: 5.056126823931029\n",
      "Epoch 88/1000, Loss: 4.927643425536878\n",
      "Epoch 89/1000, Loss: 4.939249544432669\n",
      "Epoch 90/1000, Loss: 4.990621052005074\n",
      "Epoch 91/1000, Loss: 4.9618672782724556\n",
      "Epoch 92/1000, Loss: 4.968818787372474\n",
      "Epoch 93/1000, Loss: 4.966080559022499\n",
      "Epoch 94/1000, Loss: 5.02049318226901\n",
      "Epoch 95/1000, Loss: 4.884953394080654\n",
      "Epoch 96/1000, Loss: 4.953427594719511\n",
      "Epoch 97/1000, Loss: 4.868457463654605\n",
      "Epoch 98/1000, Loss: 4.887236215851524\n",
      "Epoch 99/1000, Loss: 4.809518145792412\n",
      "Epoch 100/1000, Loss: 4.9004480188543145\n",
      "Epoch 101/1000, Loss: 4.863604928507949\n",
      "Epoch 102/1000, Loss: 4.723070428226933\n",
      "Epoch 103/1000, Loss: 4.787172788923437\n",
      "Epoch 104/1000, Loss: 4.790596678401485\n",
      "Epoch 105/1000, Loss: 4.770648230205882\n",
      "Epoch 106/1000, Loss: 4.743080825516672\n",
      "Epoch 107/1000, Loss: 4.733455685052005\n",
      "Epoch 108/1000, Loss: 4.759148781949824\n",
      "Epoch 109/1000, Loss: 4.692180725661191\n",
      "Epoch 110/1000, Loss: 4.7262219714395926\n",
      "Epoch 111/1000, Loss: 4.725362179857312\n",
      "Epoch 112/1000, Loss: 4.716004637154666\n",
      "Epoch 113/1000, Loss: 4.824977833213228\n",
      "Epoch 114/1000, Loss: 4.666832864284515\n",
      "Epoch 115/1000, Loss: 4.646138944409111\n",
      "Epoch 116/1000, Loss: 4.7049868901570635\n",
      "Epoch 117/1000, Loss: 4.639426251252492\n",
      "Epoch 118/1000, Loss: 4.600329140822093\n",
      "Epoch 119/1000, Loss: 4.670145204572966\n",
      "Epoch 120/1000, Loss: 4.603259173306552\n",
      "Epoch 121/1000, Loss: 4.774387310851704\n",
      "Epoch 122/1000, Loss: 4.625714798768361\n",
      "Epoch 123/1000, Loss: 4.639360868569576\n",
      "Epoch 124/1000, Loss: 4.579640211481037\n",
      "Epoch 125/1000, Loss: 4.504386596607439\n",
      "Epoch 126/1000, Loss: 4.566247578823205\n",
      "Epoch 127/1000, Loss: 4.595490150379412\n",
      "Epoch 128/1000, Loss: 4.632003244125482\n",
      "Epoch 129/1000, Loss: 4.645681103070577\n",
      "Epoch 130/1000, Loss: 4.502599956411304\n",
      "Epoch 131/1000, Loss: 4.447978222008907\n",
      "Epoch 132/1000, Loss: 4.54622350136439\n",
      "Epoch 133/1000, Loss: 4.574775305661288\n",
      "Epoch 134/1000, Loss: 4.401699284712474\n",
      "Epoch 135/1000, Loss: 4.585351317217856\n",
      "Epoch 136/1000, Loss: 4.582650433887135\n",
      "Epoch 137/1000, Loss: 4.427855993762161\n",
      "Epoch 138/1000, Loss: 4.447305451739918\n",
      "Epoch 139/1000, Loss: 4.51296049898321\n",
      "Epoch 140/1000, Loss: 4.457545479138692\n",
      "Epoch 141/1000, Loss: 4.419024480111672\n",
      "Epoch 142/1000, Loss: 4.503533670396516\n",
      "Epoch 143/1000, Loss: 4.481556590759393\n",
      "Epoch 144/1000, Loss: 4.480930126074589\n",
      "Epoch 145/1000, Loss: 4.483820786981871\n",
      "Epoch 146/1000, Loss: 4.48602083235076\n",
      "Epoch 147/1000, Loss: 4.439455521829201\n",
      "Epoch 148/1000, Loss: 4.4584318977413755\n",
      "Epoch 149/1000, Loss: 4.403684688336922\n",
      "Epoch 150/1000, Loss: 4.430026836467512\n",
      "Epoch 151/1000, Loss: 4.443764215165919\n",
      "Epoch 152/1000, Loss: 4.367482365983905\n",
      "Epoch 153/1000, Loss: 4.3661702022407995\n",
      "Epoch 154/1000, Loss: 4.348640476212357\n",
      "Epoch 155/1000, Loss: 4.364642835024632\n",
      "Epoch 156/1000, Loss: 4.371353393251246\n",
      "Epoch 157/1000, Loss: 4.453391356901689\n",
      "Epoch 158/1000, Loss: 4.3419090130112385\n",
      "Epoch 159/1000, Loss: 4.34327429894245\n",
      "Epoch 160/1000, Loss: 4.305206819014116\n",
      "Epoch 161/1000, Loss: 4.3942149588556\n",
      "Epoch 162/1000, Loss: 4.359525823231899\n",
      "Epoch 163/1000, Loss: 4.3593059146043025\n",
      "Epoch 164/1000, Loss: 4.329749911120444\n",
      "Epoch 165/1000, Loss: 4.2966714519442935\n",
      "Epoch 166/1000, Loss: 4.361077189445496\n",
      "Epoch 167/1000, Loss: 4.334486961364746\n",
      "Epoch 168/1000, Loss: 4.362841169039409\n",
      "Epoch 169/1000, Loss: 4.299406678387613\n",
      "Epoch 170/1000, Loss: 4.332485072540514\n",
      "Epoch 171/1000, Loss: 4.340960197376482\n",
      "Epoch 172/1000, Loss: 4.242932772997654\n",
      "Epoch 173/1000, Loss: 4.29742480949922\n",
      "Epoch 174/1000, Loss: 4.278120366009799\n",
      "Epoch 175/1000, Loss: 4.197499333005963\n",
      "Epoch 176/1000, Loss: 4.309987904447498\n",
      "Epoch 177/1000, Loss: 4.264380666342649\n",
      "Epoch 178/1000, Loss: 4.1775795221328735\n",
      "Epoch 179/1000, Loss: 4.224229102784937\n",
      "Epoch 180/1000, Loss: 4.306715786457062\n",
      "Epoch 181/1000, Loss: 4.328447435841416\n",
      "Epoch 182/1000, Loss: 4.315617406007015\n",
      "Epoch 183/1000, Loss: 4.310779569727002\n",
      "Epoch 184/1000, Loss: 4.272373838858171\n",
      "Epoch 185/1000, Loss: 4.288116399085883\n",
      "Epoch 186/1000, Loss: 4.235220314878406\n",
      "Epoch 187/1000, Loss: 4.112302241903363\n",
      "Epoch 188/1000, Loss: 4.154494879823742\n",
      "Epoch 189/1000, Loss: 4.268262261694128\n",
      "Epoch 190/1000, Loss: 4.142856966365468\n",
      "Epoch 191/1000, Loss: 4.2094772277456345\n",
      "Epoch 192/1000, Loss: 4.145793185089573\n",
      "Epoch 193/1000, Loss: 4.13816118962837\n",
      "Epoch 194/1000, Loss: 4.156791242686185\n",
      "Epoch 195/1000, Loss: 4.219074549097003\n",
      "Epoch 196/1000, Loss: 4.2164109963359255\n",
      "Epoch 197/1000, Loss: 4.189804382396467\n",
      "Epoch 198/1000, Loss: 4.175277097658678\n",
      "Epoch 199/1000, Loss: 4.226651386781172\n",
      "Epoch 200/1000, Loss: 4.126611362804066\n",
      "Epoch 201/1000, Loss: 4.095188225760604\n",
      "Epoch 202/1000, Loss: 4.105910404161974\n",
      "Epoch 203/1000, Loss: 4.171602644703605\n",
      "Epoch 204/1000, Loss: 4.0694779049266465\n",
      "Epoch 205/1000, Loss: 4.126305381457011\n",
      "Epoch 206/1000, Loss: 4.076458175977071\n",
      "Epoch 207/1000, Loss: 4.08230080387809\n",
      "Epoch 208/1000, Loss: 4.055264736666824\n",
      "Epoch 209/1000, Loss: 4.116158514311819\n",
      "Epoch 210/1000, Loss: 4.129353794184598\n",
      "Epoch 211/1000, Loss: 4.104511270017335\n",
      "Epoch 212/1000, Loss: 4.135336081186931\n",
      "Epoch 213/1000, Loss: 4.04310747348901\n",
      "Epoch 214/1000, Loss: 4.164818919066227\n",
      "Epoch 215/1000, Loss: 4.171329958872362\n",
      "Epoch 216/1000, Loss: 4.067114530187665\n",
      "Epoch 217/1000, Loss: 4.1114384152672505\n",
      "Epoch 218/1000, Loss: 4.0208386399529195\n",
      "Epoch 219/1000, Loss: 4.080790467334516\n",
      "Epoch 220/1000, Loss: 4.104579244599198\n",
      "Epoch 221/1000, Loss: 4.068617519104119\n",
      "Epoch 222/1000, Loss: 4.09366554924936\n",
      "Epoch 223/1000, Loss: 4.092317127820217\n",
      "Epoch 224/1000, Loss: 4.151393129970089\n",
      "Epoch 225/1000, Loss: 4.073308151779753\n",
      "Epoch 226/1000, Loss: 4.0979523369760225\n",
      "Epoch 227/1000, Loss: 4.069631278514862\n",
      "Epoch 228/1000, Loss: 4.025156960342869\n",
      "Epoch 229/1000, Loss: 4.0666849649313725\n",
      "Epoch 230/1000, Loss: 4.057095742586887\n",
      "Epoch 231/1000, Loss: 4.1010363950873865\n",
      "Epoch 232/1000, Loss: 3.949532438408245\n",
      "Epoch 233/1000, Loss: 4.131045294530464\n",
      "Epoch 234/1000, Loss: 4.069332657438336\n",
      "Epoch 235/1000, Loss: 3.9635316440553376\n",
      "Epoch 236/1000, Loss: 3.968708793322245\n",
      "Epoch 237/1000, Loss: 4.011512779828274\n",
      "Epoch 238/1000, Loss: 3.9870636463165283\n",
      "Epoch 239/1000, Loss: 4.076536920937625\n",
      "Epoch 240/1000, Loss: 4.045812810912277\n",
      "Epoch 241/1000, Loss: 4.076156375986157\n",
      "Epoch 242/1000, Loss: 4.080476972189817\n",
      "Epoch 243/1000, Loss: 4.00968966700814\n",
      "Epoch 244/1000, Loss: 4.055253211295966\n",
      "Epoch 245/1000, Loss: 3.9978063106536865\n",
      "Epoch 246/1000, Loss: 3.952419860796495\n",
      "Epoch 247/1000, Loss: 4.021497148455995\n",
      "Epoch 248/1000, Loss: 3.9902344526666584\n",
      "Epoch 249/1000, Loss: 3.957540158069495\n",
      "Epoch 250/1000, Loss: 4.022958161252918\n",
      "Epoch 251/1000, Loss: 3.9789934989177818\n",
      "Epoch 252/1000, Loss: 3.924920490293792\n",
      "Epoch 253/1000, Loss: 3.9523998535040654\n",
      "Epoch 254/1000, Loss: 3.9408774213357405\n",
      "Epoch 255/1000, Loss: 4.0062864458922185\n",
      "Epoch 256/1000, Loss: 3.9105060335361594\n",
      "Epoch 257/1000, Loss: 3.9962982697920366\n",
      "Epoch 258/1000, Loss: 3.895714091532158\n",
      "Epoch 259/1000, Loss: 3.986765185991923\n",
      "Epoch 260/1000, Loss: 3.908866022572373\n",
      "Epoch 261/1000, Loss: 3.981635951634609\n",
      "Epoch 262/1000, Loss: 3.881913627638961\n",
      "Epoch 263/1000, Loss: 3.848216726924434\n",
      "Epoch 264/1000, Loss: 3.915997474482565\n",
      "Epoch 265/1000, Loss: 3.9442352237123433\n",
      "Epoch 266/1000, Loss: 3.9087555318167717\n",
      "Epoch 267/1000, Loss: 3.8509002418229072\n",
      "Epoch 268/1000, Loss: 3.9279375274976096\n",
      "Epoch 269/1000, Loss: 3.8993789065967905\n",
      "Epoch 270/1000, Loss: 3.9268005815419285\n",
      "Epoch 271/1000, Loss: 3.850502332051595\n",
      "Epoch 272/1000, Loss: 3.935694407333027\n",
      "Epoch 273/1000, Loss: 3.8961649013288095\n",
      "Epoch 274/1000, Loss: 3.945830970099478\n",
      "Epoch 275/1000, Loss: 3.9325995102073206\n",
      "Epoch 276/1000, Loss: 3.97516657186277\n",
      "Epoch 277/1000, Loss: 3.9143116149035366\n",
      "Epoch 278/1000, Loss: 3.848597647565784\n",
      "Epoch 279/1000, Loss: 3.881006031325369\n",
      "Epoch 280/1000, Loss: 3.8245055079460144\n",
      "Epoch 281/1000, Loss: 3.9192172848817073\n",
      "Epoch 282/1000, Loss: 3.860043103044683\n",
      "Epoch 283/1000, Loss: 3.8974978616743376\n",
      "Epoch 284/1000, Loss: 3.890415467999198\n",
      "Epoch 285/1000, Loss: 3.9427132173018022\n",
      "Epoch 286/1000, Loss: 3.8336445653077327\n",
      "Epoch 287/1000, Loss: 3.853864949761015\n",
      "Epoch 288/1000, Loss: 3.836028451269323\n",
      "Epoch 289/1000, Loss: 3.856163790731719\n",
      "Epoch 290/1000, Loss: 3.8501756317687756\n",
      "Epoch 291/1000, Loss: 3.782059450944265\n",
      "Epoch 292/1000, Loss: 3.8014779488245645\n",
      "Epoch 293/1000, Loss: 3.8811201200340735\n",
      "Epoch 294/1000, Loss: 3.832408545595227\n",
      "Epoch 295/1000, Loss: 3.876238859061039\n",
      "Epoch 296/1000, Loss: 3.8587443738272698\n",
      "Epoch 297/1000, Loss: 3.9153767083630417\n",
      "Epoch 298/1000, Loss: 3.8288916927395444\n",
      "Epoch 299/1000, Loss: 3.8040666544076167\n",
      "Epoch 300/1000, Loss: 3.8328990918217283\n",
      "Epoch 301/1000, Loss: 3.7646750645204023\n",
      "Epoch 302/1000, Loss: 3.808767609524004\n",
      "Epoch 303/1000, Loss: 3.8374173532832754\n",
      "Epoch 304/1000, Loss: 3.8127668210954377\n",
      "Epoch 305/1000, Loss: 3.781124902494026\n",
      "Epoch 306/1000, Loss: 3.7634720332694775\n",
      "Epoch 307/1000, Loss: 3.806440268502091\n",
      "Epoch 308/1000, Loss: 3.798816102923769\n",
      "Epoch 309/1000, Loss: 3.8395789139198535\n",
      "Epoch 310/1000, Loss: 3.815662194382061\n",
      "Epoch 311/1000, Loss: 3.7425393588615186\n",
      "Epoch 312/1000, Loss: 3.7505280429666694\n",
      "Epoch 313/1000, Loss: 3.807178101756356\n",
      "Epoch 314/1000, Loss: 3.7690378228823342\n",
      "Epoch 315/1000, Loss: 3.7071964216954783\n",
      "Epoch 316/1000, Loss: 3.709988433303255\n",
      "Epoch 317/1000, Loss: 3.7735699957067315\n",
      "Epoch 318/1000, Loss: 3.7906900662364382\n",
      "Epoch 319/1000, Loss: 3.690295833529848\n",
      "Epoch 320/1000, Loss: 3.759463097109939\n",
      "Epoch 321/1000, Loss: 3.7591298908898323\n",
      "Epoch 322/1000, Loss: 3.7664640600031074\n",
      "Epoch 323/1000, Loss: 3.6512383475448145\n",
      "Epoch 324/1000, Loss: 3.874556935194767\n",
      "Epoch 325/1000, Loss: 3.756664003386642\n",
      "Epoch 326/1000, Loss: 3.81625220450488\n",
      "Epoch 327/1000, Loss: 3.6694887551394375\n",
      "Epoch 328/1000, Loss: 3.8679093610156667\n",
      "Epoch 329/1000, Loss: 3.7692853772278987\n",
      "Epoch 330/1000, Loss: 3.769988948648626\n",
      "Epoch 331/1000, Loss: 3.7815915635137847\n",
      "Epoch 332/1000, Loss: 3.716934794729406\n",
      "Epoch 333/1000, Loss: 3.7889212984027285\n",
      "Epoch 334/1000, Loss: 3.7521029819141734\n",
      "Epoch 335/1000, Loss: 3.755704984520421\n",
      "Epoch 336/1000, Loss: 3.729347669717037\n",
      "Epoch 337/1000, Loss: 3.761895907647682\n",
      "Epoch 338/1000, Loss: 3.7119530656120996\n",
      "Epoch 339/1000, Loss: 3.7340159542632825\n",
      "Epoch 340/1000, Loss: 3.664613543134747\n",
      "Epoch 341/1000, Loss: 3.7876738689162512\n",
      "Epoch 342/1000, Loss: 3.8079106988328877\n",
      "Epoch 343/1000, Loss: 3.73831751129844\n",
      "Epoch 344/1000, Loss: 3.760671933492025\n",
      "Epoch 345/1000, Loss: 3.6997835780635024\n",
      "Epoch 346/1000, Loss: 3.7059874245614717\n",
      "Epoch 347/1000, Loss: 3.6486771883386555\n",
      "Epoch 348/1000, Loss: 3.5916673757813196\n",
      "Epoch 349/1000, Loss: 3.739692902926243\n",
      "Epoch 350/1000, Loss: 3.7884119153022766\n",
      "Epoch 351/1000, Loss: 3.755646627960783\n",
      "Epoch 352/1000, Loss: 3.682674644571362\n",
      "Epoch 353/1000, Loss: 3.7010268225814356\n",
      "Epoch 354/1000, Loss: 3.6637397787787696\n",
      "Epoch 355/1000, Loss: 3.7180638042363254\n",
      "Epoch 356/1000, Loss: 3.638061574011138\n",
      "Epoch 357/1000, Loss: 3.7202290043686377\n",
      "Epoch 358/1000, Loss: 3.7227043617855418\n",
      "Epoch 359/1000, Loss: 3.7504481438434487\n",
      "Epoch 360/1000, Loss: 3.681369832067779\n",
      "Epoch 361/1000, Loss: 3.666773785244335\n",
      "Epoch 362/1000, Loss: 3.6493551568551497\n",
      "Epoch 363/1000, Loss: 3.602320154507955\n",
      "Epoch 364/1000, Loss: 3.716910730708729\n",
      "Epoch 365/1000, Loss: 3.642439807906295\n",
      "Epoch 366/1000, Loss: 3.689923122073665\n",
      "Epoch 367/1000, Loss: 3.640428268548214\n",
      "Epoch 368/1000, Loss: 3.716338408715797\n",
      "Epoch 369/1000, Loss: 3.683120015895728\n",
      "Epoch 370/1000, Loss: 3.6909927906412068\n",
      "Epoch 371/1000, Loss: 3.6888273773771343\n",
      "Epoch 372/1000, Loss: 3.6561724651943552\n",
      "Epoch 373/1000, Loss: 3.550395463452195\n",
      "Epoch 374/1000, Loss: 3.5948483673009006\n",
      "Epoch 375/1000, Loss: 3.629669877615842\n",
      "Epoch 376/1000, Loss: 3.5952882026181077\n",
      "Epoch 377/1000, Loss: 3.6612426006432734\n",
      "Epoch 378/1000, Loss: 3.5853776426026314\n",
      "Epoch 379/1000, Loss: 3.5891301415183325\n",
      "Epoch 380/1000, Loss: 3.6007544524741895\n",
      "Epoch 381/1000, Loss: 3.575453519821167\n",
      "Epoch 382/1000, Loss: 3.6551059412233755\n",
      "Epoch 383/1000, Loss: 3.5540613333384194\n",
      "Epoch 384/1000, Loss: 3.669303955453815\n",
      "Epoch 385/1000, Loss: 3.660633097995411\n",
      "Epoch 386/1000, Loss: 3.7083421367587466\n",
      "Epoch 387/1000, Loss: 3.638261404904452\n",
      "Epoch 388/1000, Loss: 3.709840136947054\n",
      "Epoch 389/1000, Loss: 3.6484339797135554\n",
      "Epoch 390/1000, Loss: 3.698462173794255\n",
      "Epoch 391/1000, Loss: 3.6826997345144097\n",
      "Epoch 392/1000, Loss: 3.5886826406825674\n",
      "Epoch 393/1000, Loss: 3.581012472961888\n",
      "Epoch 394/1000, Loss: 3.560971854311047\n",
      "Epoch 395/1000, Loss: 3.6364878181255227\n",
      "Epoch 396/1000, Loss: 3.5092481013500327\n",
      "Epoch 397/1000, Loss: 3.577712797757351\n",
      "Epoch 398/1000, Loss: 3.571808484467593\n",
      "Epoch 399/1000, Loss: 3.600991801782088\n",
      "Epoch 400/1000, Loss: 3.549946409283262\n",
      "Epoch 401/1000, Loss: 3.596842231172504\n",
      "Epoch 402/1000, Loss: 3.6005631215644605\n",
      "Epoch 403/1000, Loss: 3.5689814578403127\n",
      "Epoch 404/1000, Loss: 3.5692428801998948\n",
      "Epoch 405/1000, Loss: 3.5474378285986004\n",
      "Epoch 406/1000, Loss: 3.626559434515057\n",
      "Epoch 407/1000, Loss: 3.583393277543964\n",
      "Epoch 408/1000, Loss: 3.54677155342969\n",
      "Epoch 409/1000, Loss: 3.6205312591610532\n",
      "Epoch 410/1000, Loss: 3.556269786574624\n",
      "Epoch 411/1000, Loss: 3.522714278914712\n",
      "Epoch 412/1000, Loss: 3.5833871021415247\n",
      "Epoch 413/1000, Loss: 3.5844032240636423\n",
      "Epoch 414/1000, Loss: 3.641378048694495\n",
      "Epoch 415/1000, Loss: 3.525706666888613\n",
      "Epoch 416/1000, Loss: 3.517943606232152\n",
      "Epoch 417/1000, Loss: 3.5737243764328235\n",
      "Epoch 418/1000, Loss: 3.5660449519301904\n",
      "Epoch 419/1000, Loss: 3.4845760211800085\n",
      "Epoch 420/1000, Loss: 3.5786593448032034\n",
      "Epoch 421/1000, Loss: 3.527433357455514\n",
      "Epoch 422/1000, Loss: 3.5361874690561583\n",
      "Epoch 423/1000, Loss: 3.5806027379902927\n",
      "Epoch 424/1000, Loss: 3.556728010827845\n",
      "Epoch 425/1000, Loss: 3.514712839415579\n",
      "Epoch 426/1000, Loss: 3.560318305636897\n",
      "Epoch 427/1000, Loss: 3.50257051894159\n",
      "Epoch 428/1000, Loss: 3.5843152819257793\n",
      "Epoch 429/1000, Loss: 3.45718576149507\n",
      "Epoch 430/1000, Loss: 3.5378619396325313\n",
      "Epoch 431/1000, Loss: 3.5612312483065054\n",
      "Epoch 432/1000, Loss: 3.557586846929608\n",
      "Epoch 433/1000, Loss: 3.611762751232494\n",
      "Epoch 434/1000, Loss: 3.5239458391160676\n",
      "Epoch 435/1000, Loss: 3.591809287215724\n",
      "Epoch 436/1000, Loss: 3.5410690741105513\n",
      "Epoch 437/1000, Loss: 3.4778014927199394\n",
      "Epoch 438/1000, Loss: 3.5681004000432566\n",
      "Epoch 439/1000, Loss: 3.4856077053330163\n",
      "Epoch 440/1000, Loss: 3.5181612065344146\n",
      "Epoch 441/1000, Loss: 3.491933151628032\n",
      "Epoch 442/1000, Loss: 3.56806064193899\n",
      "Epoch 443/1000, Loss: 3.5444420287103364\n",
      "Epoch 444/1000, Loss: 3.5460041309847976\n",
      "Epoch 445/1000, Loss: 3.5980659398165615\n",
      "Epoch 446/1000, Loss: 3.53575850797422\n",
      "Epoch 447/1000, Loss: 3.539964264089411\n",
      "Epoch 448/1000, Loss: 3.4607381044012127\n",
      "Epoch 449/1000, Loss: 3.5064096866231975\n",
      "Epoch 450/1000, Loss: 3.511540811170231\n",
      "Epoch 451/1000, Loss: 3.4304450208490547\n",
      "Epoch 452/1000, Loss: 3.477386519764409\n",
      "Epoch 453/1000, Loss: 3.530493620670203\n",
      "Epoch 454/1000, Loss: 3.5121609315727698\n",
      "Epoch 455/1000, Loss: 3.4677627944585048\n",
      "Epoch 456/1000, Loss: 3.5057848393917084\n",
      "Epoch 457/1000, Loss: 3.396213952339057\n",
      "Epoch 458/1000, Loss: 3.5768776644359934\n",
      "Epoch 459/1000, Loss: 3.491016837683591\n",
      "Epoch 460/1000, Loss: 3.5656758203650964\n",
      "Epoch 461/1000, Loss: 3.4955525994300842\n",
      "Epoch 462/1000, Loss: 3.442485262047161\n",
      "Epoch 463/1000, Loss: 3.4732055510535385\n",
      "Epoch 464/1000, Loss: 3.3872852939547915\n",
      "Epoch 465/1000, Loss: 3.60105494296912\n",
      "Epoch 466/1000, Loss: 3.417718228065606\n",
      "Epoch 467/1000, Loss: 3.4865462906432874\n",
      "Epoch 468/1000, Loss: 3.4359441833062605\n",
      "Epoch 469/1000, Loss: 3.457075979673501\n",
      "Epoch 470/1000, Loss: 3.5052721717140893\n",
      "Epoch 471/1000, Loss: 3.5252824606317463\n",
      "Epoch 472/1000, Loss: 3.442693625435685\n",
      "Epoch 473/1000, Loss: 3.4821876016530124\n",
      "Epoch 474/1000, Loss: 3.4691073786128652\n",
      "Epoch 475/1000, Loss: 3.4884864832415725\n",
      "Epoch 476/1000, Loss: 3.5080496488195476\n",
      "Epoch 477/1000, Loss: 3.4168611710721795\n",
      "Epoch 478/1000, Loss: 3.4934620784990713\n",
      "Epoch 479/1000, Loss: 3.4010410453334\n",
      "Epoch 480/1000, Loss: 3.411681256510995\n",
      "Epoch 481/1000, Loss: 3.5402356964169126\n",
      "Epoch 482/1000, Loss: 3.4736576188694346\n",
      "Epoch 483/1000, Loss: 3.401965730118029\n",
      "Epoch 484/1000, Loss: 3.4679704001455596\n",
      "Epoch 485/1000, Loss: 3.4908348737340984\n",
      "Epoch 486/1000, Loss: 3.4416863719622293\n",
      "Epoch 487/1000, Loss: 3.52374648325371\n",
      "Epoch 488/1000, Loss: 3.48992718891664\n",
      "Epoch 489/1000, Loss: 3.468104199929671\n",
      "Epoch 490/1000, Loss: 3.458595436630827\n",
      "Epoch 491/1000, Loss: 3.570610187270425\n",
      "Epoch 492/1000, Loss: 3.4688747741959314\n",
      "Epoch 493/1000, Loss: 3.5038024855382517\n",
      "Epoch 494/1000, Loss: 3.4336615403493247\n",
      "Epoch 495/1000, Loss: 3.420837371638327\n",
      "Epoch 496/1000, Loss: 3.468244312387524\n",
      "Epoch 497/1000, Loss: 3.4603014732852126\n",
      "Epoch 498/1000, Loss: 3.4686646353114736\n",
      "Epoch 499/1000, Loss: 3.3669541442033015\n",
      "Epoch 500/1000, Loss: 3.3906637195384866\n",
      "Epoch 501/1000, Loss: 3.4579746741237063\n",
      "Epoch 502/1000, Loss: 3.354227485078754\n",
      "Epoch 503/1000, Loss: 3.40481928803704\n",
      "Epoch 504/1000, Loss: 3.416560768178015\n",
      "Epoch 505/1000, Loss: 3.4355306896296414\n",
      "Epoch 506/1000, Loss: 3.446303741498427\n",
      "Epoch 507/1000, Loss: 3.424065618804007\n",
      "Epoch 508/1000, Loss: 3.4288223855423205\n",
      "Epoch 509/1000, Loss: 3.4266653566649468\n",
      "Epoch 510/1000, Loss: 3.391999983426296\n",
      "Epoch 511/1000, Loss: 3.4191515626329365\n",
      "Epoch 512/1000, Loss: 3.4089919816363943\n",
      "Epoch 513/1000, Loss: 3.4494330178607595\n",
      "Epoch 514/1000, Loss: 3.4452951460173638\n",
      "Epoch 515/1000, Loss: 3.4510243679537917\n",
      "Epoch 516/1000, Loss: 3.4429035891186106\n",
      "Epoch 517/1000, Loss: 3.46885135679534\n",
      "Epoch 518/1000, Loss: 3.3925071774107036\n",
      "Epoch 519/1000, Loss: 3.4413209593657292\n",
      "Epoch 520/1000, Loss: 3.4656429453329607\n",
      "Epoch 521/1000, Loss: 3.4091988693584097\n",
      "Epoch 522/1000, Loss: 3.429245858481436\n",
      "Epoch 523/1000, Loss: 3.3948745853973157\n",
      "Epoch 524/1000, Loss: 3.3453632656371957\n",
      "Epoch 525/1000, Loss: 3.3842676932161506\n",
      "Epoch 526/1000, Loss: 3.467952743624196\n",
      "Epoch 527/1000, Loss: 3.397260257692048\n",
      "Epoch 528/1000, Loss: 3.3886790058829566\n",
      "Epoch 529/1000, Loss: 3.477828677856561\n",
      "Epoch 530/1000, Loss: 3.375211612744765\n",
      "Epoch 531/1000, Loss: 3.4336611863338584\n",
      "Epoch 532/1000, Loss: 3.3475974960760637\n",
      "Epoch 533/1000, Loss: 3.4619114417018313\n",
      "Epoch 534/1000, Loss: 3.364882196440841\n",
      "Epoch 535/1000, Loss: 3.4445809216210335\n",
      "Epoch 536/1000, Loss: 3.313036761500619\n",
      "Epoch 537/1000, Loss: 3.3840074755928735\n",
      "Epoch 538/1000, Loss: 3.400033114534436\n",
      "Epoch 539/1000, Loss: 3.4264678142287512\n",
      "Epoch 540/1000, Loss: 3.365915112423174\n",
      "Epoch 541/1000, Loss: 3.475065740672025\n",
      "Epoch 542/1000, Loss: 3.4159816322904644\n",
      "Epoch 543/1000, Loss: 3.4887478152910867\n",
      "Epoch 544/1000, Loss: 3.342114127043522\n",
      "Epoch 545/1000, Loss: 3.3996623754501343\n",
      "Epoch 546/1000, Loss: 3.3743030844312725\n",
      "Epoch 547/1000, Loss: 3.4376568776188474\n",
      "Epoch 548/1000, Loss: 3.340636611887903\n",
      "Epoch 549/1000, Loss: 3.394687836820429\n",
      "Epoch 550/1000, Loss: 3.4308258403431284\n",
      "Epoch 551/1000, Loss: 3.356005283919248\n",
      "Epoch 552/1000, Loss: 3.4055523330515083\n",
      "Epoch 553/1000, Loss: 3.3831706643104553\n",
      "Epoch 554/1000, Loss: 3.3570202751593157\n",
      "Epoch 555/1000, Loss: 3.3546550201647207\n",
      "Epoch 556/1000, Loss: 3.247438121925701\n",
      "Epoch 557/1000, Loss: 3.3476225634415946\n",
      "Epoch 558/1000, Loss: 3.287581929654786\n",
      "Epoch 559/1000, Loss: 3.3755604317694\n",
      "Epoch 560/1000, Loss: 3.401074499794931\n",
      "Epoch 561/1000, Loss: 3.3303007754412564\n",
      "Epoch 562/1000, Loss: 3.3496312986720693\n",
      "Epoch 563/1000, Loss: 3.330267091592153\n",
      "Epoch 564/1000, Loss: 3.2814267458337727\n",
      "Epoch 565/1000, Loss: 3.3940564141129004\n",
      "Epoch 566/1000, Loss: 3.378422691966548\n",
      "Epoch 567/1000, Loss: 3.36410949085698\n",
      "Epoch 568/1000, Loss: 3.3239215522101433\n",
      "Epoch 569/1000, Loss: 3.380783929969325\n",
      "Epoch 570/1000, Loss: 3.339014152685801\n",
      "Epoch 571/1000, Loss: 3.428710825515516\n",
      "Epoch 572/1000, Loss: 3.3986002017151224\n",
      "Epoch 573/1000, Loss: 3.3978744076960012\n",
      "Epoch 574/1000, Loss: 3.2452858397454927\n",
      "Epoch 575/1000, Loss: 3.3882622447880832\n",
      "Epoch 576/1000, Loss: 3.410601072239153\n",
      "Epoch 577/1000, Loss: 3.2438528465502188\n",
      "Epoch 578/1000, Loss: 3.311361616308039\n",
      "Epoch 579/1000, Loss: 3.3422712503057537\n",
      "Epoch 580/1000, Loss: 3.2861644318609526\n",
      "Epoch 581/1000, Loss: 3.273212609869061\n",
      "Epoch 582/1000, Loss: 3.304367300235864\n",
      "Epoch 583/1000, Loss: 3.31308427362731\n",
      "Epoch 584/1000, Loss: 3.359529356161753\n",
      "Epoch 585/1000, Loss: 3.3264484495827644\n",
      "Epoch 586/1000, Loss: 3.388638164057876\n",
      "Epoch 587/1000, Loss: 3.3645385883071204\n",
      "Epoch 588/1000, Loss: 3.3353279594219094\n",
      "Epoch 589/1000, Loss: 3.318830991333181\n",
      "Epoch 590/1000, Loss: 3.3179495154005108\n",
      "Epoch 591/1000, Loss: 3.2562455173694724\n",
      "Epoch 592/1000, Loss: 3.273991823196411\n",
      "Epoch 593/1000, Loss: 3.376670924100009\n",
      "Epoch 594/1000, Loss: 3.285892929091598\n",
      "Epoch 595/1000, Loss: 3.3650161307869535\n",
      "Epoch 596/1000, Loss: 3.35688975724307\n",
      "Epoch 597/1000, Loss: 3.3513160492434646\n",
      "Epoch 598/1000, Loss: 3.343892950000185\n",
      "Epoch 599/1000, Loss: 3.245268323204734\n",
      "Epoch 600/1000, Loss: 3.322410606976711\n",
      "Epoch 601/1000, Loss: 3.330656085953568\n",
      "Epoch 602/1000, Loss: 3.3041374484697976\n",
      "Epoch 603/1000, Loss: 3.2966448068618774\n",
      "Epoch 604/1000, Loss: 3.269209025484143\n",
      "Epoch 605/1000, Loss: 3.283908587513548\n",
      "Epoch 606/1000, Loss: 3.338952474521868\n",
      "Epoch 607/1000, Loss: 3.317257863102537\n",
      "Epoch 608/1000, Loss: 3.3263155781861506\n",
      "Epoch 609/1000, Loss: 3.307349326032581\n",
      "Epoch 610/1000, Loss: 3.241477491277637\n",
      "Epoch 611/1000, Loss: 3.2866084196350793\n",
      "Epoch 612/1000, Loss: 3.3610895962426155\n",
      "Epoch 613/1000, Loss: 3.294981320699056\n",
      "Epoch 614/1000, Loss: 3.3020195681037325\n",
      "Epoch 615/1000, Loss: 3.3604751229286194\n",
      "Epoch 616/1000, Loss: 3.3414009953990127\n",
      "Epoch 617/1000, Loss: 3.252676683844942\n",
      "Epoch 618/1000, Loss: 3.288913641915177\n",
      "Epoch 619/1000, Loss: 3.2749606497359998\n",
      "Epoch 620/1000, Loss: 3.3600959199847598\n",
      "Epoch 621/1000, Loss: 3.2773195884444495\n",
      "Epoch 622/1000, Loss: 3.3275683666720535\n",
      "Epoch 623/1000, Loss: 3.2240250661517633\n",
      "Epoch 624/1000, Loss: 3.237754494854898\n",
      "Epoch 625/1000, Loss: 3.2698896111864033\n",
      "Epoch 626/1000, Loss: 3.3406353610934634\n",
      "Epoch 627/1000, Loss: 3.32251158898527\n",
      "Epoch 628/1000, Loss: 3.3258706963423528\n",
      "Epoch 629/1000, Loss: 3.330886454293222\n",
      "Epoch 630/1000, Loss: 3.2253627768068602\n",
      "Epoch 631/1000, Loss: 3.273231995828224\n",
      "Epoch 632/1000, Loss: 3.2445362878568247\n",
      "Epoch 633/1000, Loss: 3.29444096847014\n",
      "Epoch 634/1000, Loss: 3.2870913364670495\n",
      "Epoch 635/1000, Loss: 3.3178206248716875\n",
      "Epoch 636/1000, Loss: 3.2615574273196133\n",
      "Epoch 637/1000, Loss: 3.273690332065929\n",
      "Epoch 638/1000, Loss: 3.3350873065717295\n",
      "Epoch 639/1000, Loss: 3.18921498818831\n",
      "Epoch 640/1000, Loss: 3.217404426950397\n",
      "Epoch 641/1000, Loss: 3.276306183049173\n",
      "Epoch 642/1000, Loss: 3.2665662584882793\n",
      "Epoch 643/1000, Loss: 3.2796828566175518\n",
      "Epoch 644/1000, Loss: 3.259637778455561\n",
      "Epoch 645/1000, Loss: 3.257918950283166\n",
      "Epoch 646/1000, Loss: 3.3031527815443096\n",
      "Epoch 647/1000, Loss: 3.2778685688972473\n",
      "Epoch 648/1000, Loss: 3.3512774886506977\n",
      "Epoch 649/1000, Loss: 3.332788368066152\n",
      "Epoch 650/1000, Loss: 3.27467367143342\n",
      "Epoch 651/1000, Loss: 3.2782734018383604\n",
      "Epoch 652/1000, Loss: 3.266921563581987\n",
      "Epoch 653/1000, Loss: 3.2611338351712083\n",
      "Epoch 654/1000, Loss: 3.2401902016365165\n",
      "Epoch 655/1000, Loss: 3.2839962578181066\n",
      "Epoch 656/1000, Loss: 3.2695069945219792\n",
      "Epoch 657/1000, Loss: 3.285391906897227\n",
      "Epoch 658/1000, Loss: 3.211719513842554\n",
      "Epoch 659/1000, Loss: 3.3329805533091226\n",
      "Epoch 660/1000, Loss: 3.2575365666187173\n",
      "Epoch 661/1000, Loss: 3.2708217212648103\n",
      "Epoch 662/1000, Loss: 3.1900604159543007\n",
      "Epoch 663/1000, Loss: 3.1954001901727733\n",
      "Epoch 664/1000, Loss: 3.25155525857752\n",
      "Epoch 665/1000, Loss: 3.293222113089128\n",
      "Epoch 666/1000, Loss: 3.3559033383022654\n",
      "Epoch 667/1000, Loss: 3.2575164726286223\n",
      "Epoch 668/1000, Loss: 3.281343853834904\n",
      "Epoch 669/1000, Loss: 3.1982422705852622\n",
      "Epoch 670/1000, Loss: 3.1899001038435735\n",
      "Epoch 671/1000, Loss: 3.2315954945304175\n",
      "Epoch 672/1000, Loss: 3.2575285344412834\n",
      "Epoch 673/1000, Loss: 3.2126352832172858\n",
      "Epoch 674/1000, Loss: 3.2660774796298058\n",
      "Epoch 675/1000, Loss: 3.271357957160834\n",
      "Epoch 676/1000, Loss: 3.238580300952449\n",
      "Epoch 677/1000, Loss: 3.238926947116852\n",
      "Epoch 678/1000, Loss: 3.190405704758384\n",
      "Epoch 679/1000, Loss: 3.2472079838767196\n",
      "Epoch 680/1000, Loss: 3.2991724267150415\n",
      "Epoch 681/1000, Loss: 3.23860503597693\n",
      "Epoch 682/1000, Loss: 3.2372604265357507\n",
      "Epoch 683/1000, Loss: 3.2102227933479077\n",
      "Epoch 684/1000, Loss: 3.288609078436187\n",
      "Epoch 685/1000, Loss: 3.2367551272565667\n",
      "Epoch 686/1000, Loss: 3.232686813130523\n",
      "Epoch 687/1000, Loss: 3.260295898625345\n",
      "Epoch 688/1000, Loss: 3.253789818648136\n",
      "Epoch 689/1000, Loss: 3.282257464799014\n",
      "Epoch 690/1000, Loss: 3.2398141965721594\n",
      "Epoch 691/1000, Loss: 3.2565617516185297\n",
      "Epoch 692/1000, Loss: 3.179389471357519\n",
      "Epoch 693/1000, Loss: 3.2733243306477866\n",
      "Epoch 694/1000, Loss: 3.246016735380346\n",
      "Epoch 695/1000, Loss: 3.2452764456922356\n",
      "Epoch 696/1000, Loss: 3.214936700734225\n",
      "Epoch 697/1000, Loss: 3.2509015437328452\n",
      "Epoch 698/1000, Loss: 3.2035854549118965\n",
      "Epoch 699/1000, Loss: 3.2227451440059776\n",
      "Epoch 700/1000, Loss: 3.2374520365035897\n",
      "Epoch 701/1000, Loss: 3.1860710364399534\n",
      "Epoch 702/1000, Loss: 3.21434435338685\n",
      "Epoch 703/1000, Loss: 3.1726018000732767\n",
      "Epoch 704/1000, Loss: 3.1667308951869155\n",
      "Epoch 705/1000, Loss: 3.250388835415696\n",
      "Epoch 706/1000, Loss: 3.2542584719079914\n",
      "Epoch 707/1000, Loss: 3.240399622555935\n",
      "Epoch 708/1000, Loss: 3.2421226700146994\n",
      "Epoch 709/1000, Loss: 3.274912787206245\n",
      "Epoch 710/1000, Loss: 3.246905115517703\n",
      "Epoch 711/1000, Loss: 3.294267517147642\n",
      "Epoch 712/1000, Loss: 3.2170997821923457\n",
      "Epoch 713/1000, Loss: 3.225885342467915\n",
      "Epoch 714/1000, Loss: 3.1693812771276995\n",
      "Epoch 715/1000, Loss: 3.23363612185825\n",
      "Epoch 716/1000, Loss: 3.286938297026085\n",
      "Epoch 717/1000, Loss: 3.1688895351958997\n",
      "Epoch 718/1000, Loss: 3.2373635895324475\n",
      "Epoch 719/1000, Loss: 3.268821868029508\n",
      "Epoch 720/1000, Loss: 3.1348709482135195\n",
      "Epoch 721/1000, Loss: 3.2252754298123447\n",
      "Epoch 722/1000, Loss: 3.2682726148403054\n",
      "Epoch 723/1000, Loss: 3.260045192458413\n",
      "Epoch 724/1000, Loss: 3.2188753467617612\n",
      "Epoch 725/1000, Loss: 3.231918013457096\n",
      "Epoch 726/1000, Loss: 3.170085813059951\n",
      "Epoch 727/1000, Loss: 3.1807586270751376\n",
      "Epoch 728/1000, Loss: 3.119011609843283\n",
      "Epoch 729/1000, Loss: 3.1606518445592937\n",
      "Epoch 730/1000, Loss: 3.158867404316411\n",
      "Epoch 731/1000, Loss: 3.1890680862195566\n",
      "Epoch 732/1000, Loss: 3.2310371778228064\n",
      "Epoch 733/1000, Loss: 3.1289149884021645\n",
      "Epoch 734/1000, Loss: 3.181718841646657\n",
      "Epoch 735/1000, Loss: 3.1873577056509075\n",
      "Epoch 736/1000, Loss: 3.186546421412266\n",
      "Epoch 737/1000, Loss: 3.1990787757165506\n",
      "Epoch 738/1000, Loss: 3.204062324581724\n",
      "Epoch 739/1000, Loss: 3.217877214605158\n",
      "Epoch 740/1000, Loss: 3.1803557637966042\n",
      "Epoch 741/1000, Loss: 3.1535677702137916\n",
      "Epoch 742/1000, Loss: 3.1891832749048867\n",
      "Epoch 743/1000, Loss: 3.224471870696906\n",
      "Epoch 744/1000, Loss: 3.245510599829934\n",
      "Epoch 745/1000, Loss: 3.1677598483634717\n",
      "Epoch 746/1000, Loss: 3.1684896259596855\n",
      "Epoch 747/1000, Loss: 3.197364725849845\n",
      "Epoch 748/1000, Loss: 3.132189645911708\n",
      "Epoch 749/1000, Loss: 3.1675528143391465\n",
      "Epoch 750/1000, Loss: 3.2079283513806085\n",
      "Epoch 751/1000, Loss: 3.165826730655901\n",
      "Epoch 752/1000, Loss: 3.1644362969831987\n",
      "Epoch 753/1000, Loss: 3.2137289878093833\n",
      "Epoch 754/1000, Loss: 3.194652131109527\n",
      "Epoch 755/1000, Loss: 3.197103503075513\n",
      "Epoch 756/1000, Loss: 3.1854984236486033\n",
      "Epoch 757/1000, Loss: 3.164026155616298\n",
      "Epoch 758/1000, Loss: 3.2283301922408016\n",
      "Epoch 759/1000, Loss: 3.21201707016338\n",
      "Epoch 760/1000, Loss: 3.181543256297256\n",
      "Epoch 761/1000, Loss: 3.1332304405443594\n",
      "Epoch 762/1000, Loss: 3.091296461495486\n",
      "Epoch 763/1000, Loss: 3.1348064252824495\n",
      "Epoch 764/1000, Loss: 3.1505467385956734\n",
      "Epoch 765/1000, Loss: 3.107462839646773\n",
      "Epoch 766/1000, Loss: 3.227592757253936\n",
      "Epoch 767/1000, Loss: 3.190767271049095\n",
      "Epoch 768/1000, Loss: 3.1488228801525002\n",
      "Epoch 769/1000, Loss: 3.1686831495978613\n",
      "Epoch 770/1000, Loss: 3.184713844097022\n",
      "Epoch 771/1000, Loss: 3.2263923142895554\n",
      "Epoch 772/1000, Loss: 3.13601084911462\n",
      "Epoch 773/1000, Loss: 3.1503983474139012\n",
      "Epoch 774/1000, Loss: 3.184513220281312\n",
      "Epoch 775/1000, Loss: 3.204482726978533\n",
      "Epoch 776/1000, Loss: 3.217023744727626\n",
      "Epoch 777/1000, Loss: 3.184436910080187\n",
      "Epoch 778/1000, Loss: 3.2213529510931536\n",
      "Epoch 779/1000, Loss: 3.201662540435791\n",
      "Epoch 780/1000, Loss: 3.1645292287523095\n",
      "Epoch 781/1000, Loss: 3.195135266491861\n",
      "Epoch 782/1000, Loss: 3.1481837512868824\n",
      "Epoch 783/1000, Loss: 3.1448002060254416\n",
      "Epoch 784/1000, Loss: 3.133117475292899\n",
      "Epoch 785/1000, Loss: 3.2255766581405294\n",
      "Epoch 786/1000, Loss: 3.198741733124762\n",
      "Epoch 787/1000, Loss: 3.1932004184433906\n",
      "Epoch 788/1000, Loss: 3.1895755673899795\n",
      "Epoch 789/1000, Loss: 3.1144067166429577\n",
      "Epoch 790/1000, Loss: 3.1236443465406243\n",
      "Epoch 791/1000, Loss: 3.1441952044313606\n",
      "Epoch 792/1000, Loss: 3.160549929647735\n",
      "Epoch 793/1000, Loss: 3.185410664840178\n",
      "Epoch 794/1000, Loss: 3.1555535901676524\n",
      "Epoch 795/1000, Loss: 3.1062667965888977\n",
      "Epoch 796/1000, Loss: 3.083506976113175\n",
      "Epoch 797/1000, Loss: 3.090950228951194\n",
      "Epoch 798/1000, Loss: 3.1243962546189628\n",
      "Epoch 799/1000, Loss: 3.1394503188855722\n",
      "Epoch 800/1000, Loss: 3.1978209677970773\n",
      "Epoch 801/1000, Loss: 3.179743230342865\n",
      "Epoch 802/1000, Loss: 3.1621885137124495\n",
      "Epoch 803/1000, Loss: 3.20135227839152\n",
      "Epoch 804/1000, Loss: 3.163761819853927\n",
      "Epoch 805/1000, Loss: 3.155430230227384\n",
      "Epoch 806/1000, Loss: 3.1288281895897607\n",
      "Epoch 807/1000, Loss: 3.1718818500186456\n",
      "Epoch 808/1000, Loss: 3.132066350994688\n",
      "Epoch 809/1000, Loss: 3.15617782238758\n",
      "Epoch 810/1000, Loss: 3.070889955217188\n",
      "Epoch 811/1000, Loss: 3.0892701013521715\n",
      "Epoch 812/1000, Loss: 3.090209236650756\n",
      "Epoch 813/1000, Loss: 3.1828362851431877\n",
      "Epoch 814/1000, Loss: 3.181041491754127\n",
      "Epoch 815/1000, Loss: 3.157132777300748\n",
      "Epoch 816/1000, Loss: 3.2324242384144752\n",
      "Epoch 817/1000, Loss: 3.1363073479045522\n",
      "Epoch 818/1000, Loss: 3.1377452322931\n",
      "Epoch 819/1000, Loss: 3.072426721905217\n",
      "Epoch 820/1000, Loss: 3.1481944701888342\n",
      "Epoch 821/1000, Loss: 3.162290808829394\n",
      "Epoch 822/1000, Loss: 3.110094386519808\n",
      "Epoch 823/1000, Loss: 3.1468992269400395\n",
      "Epoch 824/1000, Loss: 3.104085959268339\n",
      "Epoch 825/1000, Loss: 3.1271647810935974\n",
      "Epoch 826/1000, Loss: 3.1961868053132836\n",
      "Epoch 827/1000, Loss: 3.139220337073008\n",
      "Epoch 828/1000, Loss: 3.1147918592799795\n",
      "Epoch 829/1000, Loss: 3.173350162578352\n",
      "Epoch 830/1000, Loss: 3.1769928336143494\n",
      "Epoch 831/1000, Loss: 3.1553619838122167\n",
      "Epoch 832/1000, Loss: 3.2139310042063394\n",
      "Epoch 833/1000, Loss: 3.211937239675811\n",
      "Epoch 834/1000, Loss: 3.130241944031282\n",
      "Epoch 835/1000, Loss: 3.0993072020285055\n",
      "Epoch 836/1000, Loss: 3.2006112066182224\n",
      "Epoch 837/1000, Loss: 3.179847636006095\n",
      "Epoch 838/1000, Loss: 3.1078652143478394\n",
      "Epoch 839/1000, Loss: 3.1550173705274407\n",
      "Epoch 840/1000, Loss: 3.140838512868592\n",
      "Epoch 841/1000, Loss: 3.1152418075185833\n",
      "Epoch 842/1000, Loss: 3.0867196086681252\n",
      "Epoch 843/1000, Loss: 3.187800864378611\n",
      "Epoch 844/1000, Loss: 3.118914757714127\n",
      "Epoch 845/1000, Loss: 3.1065255746696936\n",
      "Epoch 846/1000, Loss: 3.1726911718195137\n",
      "Epoch 847/1000, Loss: 3.150208948236523\n",
      "Epoch 848/1000, Loss: 3.1528505953875454\n",
      "Epoch 849/1000, Loss: 3.1527535012274077\n",
      "Epoch 850/1000, Loss: 3.071149177623518\n",
      "Epoch 851/1000, Loss: 3.099176414085157\n",
      "Epoch 852/1000, Loss: 3.065299588622469\n",
      "Epoch 853/1000, Loss: 3.1447125655232053\n",
      "Epoch 854/1000, Loss: 3.1773034966353215\n",
      "Epoch 855/1000, Loss: 3.0839554486852703\n",
      "Epoch 856/1000, Loss: 3.1562482100544553\n",
      "Epoch 857/1000, Loss: 3.058348977204525\n",
      "Epoch 858/1000, Loss: 3.129383446592273\n",
      "Epoch 859/1000, Loss: 3.0787269545323923\n",
      "Epoch 860/1000, Loss: 3.1629205192580367\n",
      "Epoch 861/1000, Loss: 3.146072425625541\n",
      "Epoch 862/1000, Loss: 3.096756451057665\n",
      "Epoch 863/1000, Loss: 3.0733497206008797\n",
      "Epoch 864/1000, Loss: 3.0708745320638022\n",
      "Epoch 865/1000, Loss: 3.1359228148604883\n",
      "Epoch 866/1000, Loss: 3.0841299566355618\n",
      "Epoch 867/1000, Loss: 3.074270618684364\n",
      "Epoch 868/1000, Loss: 3.08115543199308\n",
      "Epoch 869/1000, Loss: 3.2472725582845285\n",
      "Epoch 870/1000, Loss: 3.1111326389240497\n",
      "Epoch 871/1000, Loss: 3.17959636630434\n",
      "Epoch 872/1000, Loss: 3.0851635923891356\n",
      "Epoch 873/1000, Loss: 3.0978257619973384\n",
      "Epoch 874/1000, Loss: 3.09175896283352\n",
      "Epoch 875/1000, Loss: 3.1351147853966914\n",
      "Epoch 876/1000, Loss: 3.1210732694828147\n",
      "Epoch 877/1000, Loss: 3.076996492617058\n",
      "Epoch 878/1000, Loss: 3.0978465911113853\n",
      "Epoch 879/1000, Loss: 3.1189837058385215\n",
      "Epoch 880/1000, Loss: 3.126953459147251\n",
      "Epoch 881/1000, Loss: 3.040424206040122\n",
      "Epoch 882/1000, Loss: 3.1061302278981064\n",
      "Epoch 883/1000, Loss: 3.0785346410491248\n",
      "Epoch 884/1000, Loss: 3.0555191870891685\n",
      "Epoch 885/1000, Loss: 3.0512454013029733\n",
      "Epoch 886/1000, Loss: 3.1873613108288157\n",
      "Epoch 887/1000, Loss: 3.093811515605811\n",
      "Epoch 888/1000, Loss: 3.0508452039776426\n",
      "Epoch 889/1000, Loss: 3.1110804071932128\n",
      "Epoch 890/1000, Loss: 3.0905754268169403\n",
      "Epoch 891/1000, Loss: 3.1543073455492654\n",
      "Epoch 892/1000, Loss: 3.0536889217116614\n",
      "Epoch 893/1000, Loss: 3.1792697780060046\n",
      "Epoch 894/1000, Loss: 3.101788714979634\n",
      "Epoch 895/1000, Loss: 3.111699705774134\n",
      "Epoch 896/1000, Loss: 3.053746832139564\n",
      "Epoch 897/1000, Loss: 3.03088683793039\n",
      "Epoch 898/1000, Loss: 3.1495437016992858\n",
      "Epoch 899/1000, Loss: 3.113061214938308\n",
      "Epoch 900/1000, Loss: 3.1790073947473005\n",
      "Epoch 901/1000, Loss: 3.0845856928464137\n",
      "Epoch 902/1000, Loss: 3.1409883589455574\n",
      "Epoch 903/1000, Loss: 3.0681758080468033\n",
      "Epoch 904/1000, Loss: 3.0687135141907316\n",
      "Epoch 905/1000, Loss: 3.030133015278614\n",
      "Epoch 906/1000, Loss: 3.068928342877012\n",
      "Epoch 907/1000, Loss: 3.0461735580906724\n",
      "Epoch 908/1000, Loss: 3.1122595396908848\n",
      "Epoch 909/1000, Loss: 3.0608590723890248\n",
      "Epoch 910/1000, Loss: 3.103650824590163\n",
      "Epoch 911/1000, Loss: 3.118364435253721\n",
      "Epoch 912/1000, Loss: 3.1035345142537896\n",
      "Epoch 913/1000, Loss: 3.0511163364757192\n",
      "Epoch 914/1000, Loss: 3.0601410712256576\n",
      "Epoch 915/1000, Loss: 3.0994879910440156\n",
      "Epoch 916/1000, Loss: 3.0963862718957844\n",
      "Epoch 917/1000, Loss: 3.0953926290526534\n",
      "Epoch 918/1000, Loss: 3.1106222958275764\n",
      "Epoch 919/1000, Loss: 3.0343159446210572\n",
      "Epoch 920/1000, Loss: 3.1240573380932664\n",
      "Epoch 921/1000, Loss: 3.064168851483952\n",
      "Epoch 922/1000, Loss: 3.100147598620617\n",
      "Epoch 923/1000, Loss: 3.1024423837661743\n",
      "Epoch 924/1000, Loss: 3.126812969193314\n",
      "Epoch 925/1000, Loss: 3.0976462310010735\n",
      "Epoch 926/1000, Loss: 3.0948934202844445\n",
      "Epoch 927/1000, Loss: 3.085026381593762\n",
      "Epoch 928/1000, Loss: 3.1121857130166255\n",
      "Epoch 929/1000, Loss: 3.081648161917022\n",
      "Epoch 930/1000, Loss: 3.086099498199694\n",
      "Epoch 931/1000, Loss: 3.0909604520508736\n",
      "Epoch 932/1000, Loss: 3.051927449125232\n",
      "Epoch 933/1000, Loss: 3.051555736498399\n",
      "Epoch 934/1000, Loss: 3.056906754320318\n",
      "Epoch 935/1000, Loss: 3.0934600179845635\n",
      "Epoch 936/1000, Loss: 3.092504960117918\n",
      "Epoch 937/1000, Loss: 3.019371430079142\n",
      "Epoch 938/1000, Loss: 3.1000057151823333\n",
      "Epoch 939/1000, Loss: 3.1150474927642127\n",
      "Epoch 940/1000, Loss: 3.0726460832538027\n",
      "Epoch 941/1000, Loss: 3.0581881512295115\n",
      "Epoch 942/1000, Loss: 3.025158683458964\n",
      "Epoch 943/1000, Loss: 3.0706403544454863\n",
      "Epoch 944/1000, Loss: 3.070328217564207\n",
      "Epoch 945/1000, Loss: 3.127453883488973\n",
      "Epoch 946/1000, Loss: 3.0623776172146653\n",
      "Epoch 947/1000, Loss: 3.031818923625079\n",
      "Epoch 948/1000, Loss: 3.0899091507449294\n",
      "Epoch 949/1000, Loss: 3.0917824949278976\n",
      "Epoch 950/1000, Loss: 3.0996639186685737\n",
      "Epoch 951/1000, Loss: 3.082905930100065\n",
      "Epoch 952/1000, Loss: 3.0702775463913428\n",
      "Epoch 953/1000, Loss: 3.0632812091798494\n",
      "Epoch 954/1000, Loss: 3.112161858515306\n",
      "Epoch 955/1000, Loss: 3.133942598646337\n",
      "Epoch 956/1000, Loss: 3.1096228288881704\n",
      "Epoch 957/1000, Loss: 3.0584458257212783\n",
      "Epoch 958/1000, Loss: 3.0463099750605496\n",
      "Epoch 959/1000, Loss: 3.071384605133172\n",
      "Epoch 960/1000, Loss: 3.0653724155642768\n",
      "Epoch 961/1000, Loss: 3.128071575453787\n",
      "Epoch 962/1000, Loss: 3.0462718560840143\n",
      "Epoch 963/1000, Loss: 3.004717236215418\n",
      "Epoch 964/1000, Loss: 3.0374839585838895\n",
      "Epoch 965/1000, Loss: 3.0288333892822266\n",
      "Epoch 966/1000, Loss: 3.0645608215621025\n",
      "Epoch 967/1000, Loss: 3.076122145761143\n",
      "Epoch 968/1000, Loss: 3.036252759622805\n",
      "Epoch 969/1000, Loss: 2.9567094165267367\n",
      "Epoch 970/1000, Loss: 3.0327990876905844\n",
      "Epoch 971/1000, Loss: 3.0432681513555124\n",
      "Epoch 972/1000, Loss: 3.0659428690419053\n",
      "Epoch 973/1000, Loss: 3.048694670200348\n",
      "Epoch 974/1000, Loss: 3.100974305109544\n",
      "Epoch 975/1000, Loss: 3.0790509419007734\n",
      "Epoch 976/1000, Loss: 3.0570783217748008\n",
      "Epoch 977/1000, Loss: 3.0292558471361795\n",
      "Epoch 978/1000, Loss: 3.0477200800722297\n",
      "Epoch 979/1000, Loss: 3.021867750268994\n",
      "Epoch 980/1000, Loss: 3.1178141859444706\n",
      "Epoch 981/1000, Loss: 3.0473086084380294\n",
      "Epoch 982/1000, Loss: 3.105270858966943\n",
      "Epoch 983/1000, Loss: 3.087141089367144\n",
      "Epoch 984/1000, Loss: 3.0671425118590845\n",
      "Epoch 985/1000, Loss: 3.0124282267960636\n",
      "Epoch 986/1000, Loss: 3.0398504986907495\n",
      "Epoch 987/1000, Loss: 3.0686647964246347\n",
      "Epoch 988/1000, Loss: 3.1004701843767455\n",
      "Epoch 989/1000, Loss: 3.001956494468631\n",
      "Epoch 990/1000, Loss: 3.0757615764935813\n",
      "Epoch 991/1000, Loss: 3.055202951937011\n",
      "Epoch 992/1000, Loss: 3.1103385759122446\n",
      "Epoch 993/1000, Loss: 3.0777517033345774\n",
      "Epoch 994/1000, Loss: 3.025793146906477\n",
      "Epoch 995/1000, Loss: 3.0844943938833294\n",
      "Epoch 996/1000, Loss: 3.0374273251403463\n",
      "Epoch 997/1000, Loss: 3.0145435179724838\n",
      "Epoch 998/1000, Loss: 3.062498743786956\n",
      "Epoch 999/1000, Loss: 3.0004964073499045\n",
      "Epoch 1000/1000, Loss: 3.0608806492704335\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets_G_act, targets_G_r in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        outputs_G_act, outputs_G_r = outputs  # Separate the outputs into two tensors\n",
    "        loss_G_act = criterion(outputs_G_act, targets_G_act.unsqueeze(1))\n",
    "        loss_G_r = criterion(outputs_G_r, targets_G_r.unsqueeze(1))\n",
    "        loss = loss_G_act + loss_G_r\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs G_act: tensor([[18.1612],\n",
      "        [ 7.4811],\n",
      "        [ 5.3078],\n",
      "        [14.2548],\n",
      "        [23.5206],\n",
      "        [25.5965],\n",
      "        [11.8800],\n",
      "        [25.8802],\n",
      "        [30.9657],\n",
      "        [30.2225],\n",
      "        [16.0195],\n",
      "        [10.9047],\n",
      "        [18.0491],\n",
      "        [20.3994],\n",
      "        [18.1849],\n",
      "        [30.7277],\n",
      "        [19.3494],\n",
      "        [27.4446],\n",
      "        [26.6009],\n",
      "        [24.1394],\n",
      "        [34.4497],\n",
      "        [13.0333],\n",
      "        [16.5097],\n",
      "        [27.5808],\n",
      "        [26.4234],\n",
      "        [20.3017],\n",
      "        [31.2845],\n",
      "        [32.1672],\n",
      "        [28.1824],\n",
      "        [18.6490],\n",
      "        [16.6567],\n",
      "        [12.9035]])\n",
      "outputs G_r: tensor([[ -0.7273],\n",
      "        [-78.1272],\n",
      "        [-46.4466],\n",
      "        [-44.0128],\n",
      "        [-26.5577],\n",
      "        [-16.2092],\n",
      "        [-39.3486],\n",
      "        [ -6.0317],\n",
      "        [  3.4190],\n",
      "        [  3.0248],\n",
      "        [-40.0658],\n",
      "        [-73.6674],\n",
      "        [-29.5666],\n",
      "        [-16.4575],\n",
      "        [-38.7464],\n",
      "        [ -3.2144],\n",
      "        [-22.6353],\n",
      "        [ -2.7243],\n",
      "        [ -1.9958],\n",
      "        [ -4.7744],\n",
      "        [ 12.2775],\n",
      "        [-52.4226],\n",
      "        [-33.3360],\n",
      "        [ -3.4047],\n",
      "        [-21.0847],\n",
      "        [-29.6021],\n",
      "        [ -4.3021],\n",
      "        [  3.1579],\n",
      "        [-21.4952],\n",
      "        [-28.9838],\n",
      "        [-33.1327],\n",
      "        [-25.2944]])\n",
      "outputs G_act: tensor([[15.6632],\n",
      "        [31.7209],\n",
      "        [28.3741],\n",
      "        [15.8380],\n",
      "        [15.9156],\n",
      "        [25.3522],\n",
      "        [18.3729],\n",
      "        [30.1802],\n",
      "        [15.4465],\n",
      "        [12.0816],\n",
      "        [12.7925],\n",
      "        [19.2624],\n",
      "        [35.1662],\n",
      "        [39.9304],\n",
      "        [21.4245],\n",
      "        [14.6149],\n",
      "        [11.3044],\n",
      "        [24.1467],\n",
      "        [16.9470],\n",
      "        [12.5342],\n",
      "        [14.2904],\n",
      "        [22.7813],\n",
      "        [18.2864],\n",
      "        [26.1660],\n",
      "        [16.8809],\n",
      "        [17.1840],\n",
      "        [10.1711],\n",
      "        [14.2837],\n",
      "        [18.6259],\n",
      "        [38.9349],\n",
      "        [15.8430],\n",
      "        [18.6470]])\n",
      "outputs G_r: tensor([[-29.0069],\n",
      "        [  0.4861],\n",
      "        [ -0.7339],\n",
      "        [-61.6108],\n",
      "        [-17.8713],\n",
      "        [-11.3351],\n",
      "        [-47.0335],\n",
      "        [ -1.1142],\n",
      "        [-31.1062],\n",
      "        [-49.1667],\n",
      "        [-47.8701],\n",
      "        [-31.0685],\n",
      "        [-22.3290],\n",
      "        [  7.1224],\n",
      "        [ -8.1205],\n",
      "        [-29.4502],\n",
      "        [-22.7497],\n",
      "        [-36.0830],\n",
      "        [-32.2079],\n",
      "        [-35.3555],\n",
      "        [-39.4079],\n",
      "        [-21.2309],\n",
      "        [-29.3540],\n",
      "        [-12.1229],\n",
      "        [-35.9948],\n",
      "        [-32.6002],\n",
      "        [-62.0678],\n",
      "        [-28.7422],\n",
      "        [-13.5189],\n",
      "        [-13.5655],\n",
      "        [-58.8836],\n",
      "        [-13.1279]])\n",
      "outputs G_act: tensor([[41.1164],\n",
      "        [23.9652],\n",
      "        [20.3935],\n",
      "        [55.2096],\n",
      "        [21.9747],\n",
      "        [16.1716],\n",
      "        [14.2461],\n",
      "        [16.6879],\n",
      "        [ 6.0781],\n",
      "        [18.5066],\n",
      "        [12.9492],\n",
      "        [19.3025],\n",
      "        [29.1781],\n",
      "        [13.8919],\n",
      "        [10.6433],\n",
      "        [12.4478],\n",
      "        [33.7350],\n",
      "        [14.0409],\n",
      "        [26.6104],\n",
      "        [16.7707],\n",
      "        [17.8707],\n",
      "        [12.3576],\n",
      "        [21.1068],\n",
      "        [15.8853],\n",
      "        [14.9282],\n",
      "        [23.8939],\n",
      "        [13.5515],\n",
      "        [39.6729],\n",
      "        [26.7732],\n",
      "        [26.1964],\n",
      "        [15.4727],\n",
      "        [21.8530]])\n",
      "outputs G_r: tensor([[ 21.6235],\n",
      "        [-27.5245],\n",
      "        [-35.2154],\n",
      "        [ 11.8930],\n",
      "        [-30.0166],\n",
      "        [-21.5681],\n",
      "        [-36.4712],\n",
      "        [-46.5605],\n",
      "        [-38.9937],\n",
      "        [-43.4044],\n",
      "        [-54.0761],\n",
      "        [-47.8039],\n",
      "        [-12.3191],\n",
      "        [-39.7360],\n",
      "        [-44.0159],\n",
      "        [-29.2085],\n",
      "        [ -5.4185],\n",
      "        [-37.2966],\n",
      "        [-14.1483],\n",
      "        [-28.7950],\n",
      "        [-38.8680],\n",
      "        [-24.1597],\n",
      "        [-24.1473],\n",
      "        [-31.8807],\n",
      "        [-38.8499],\n",
      "        [-27.9564],\n",
      "        [-18.7490],\n",
      "        [  5.8854],\n",
      "        [-56.9061],\n",
      "        [  1.2938],\n",
      "        [-52.6651],\n",
      "        [-15.7414]])\n",
      "outputs G_act: tensor([[21.0332],\n",
      "        [ 9.7340],\n",
      "        [20.2443],\n",
      "        [ 6.7600],\n",
      "        [16.9259],\n",
      "        [17.2738],\n",
      "        [12.6076],\n",
      "        [22.6297],\n",
      "        [50.0901],\n",
      "        [31.5445],\n",
      "        [18.5402],\n",
      "        [14.9137],\n",
      "        [12.0313],\n",
      "        [24.1753],\n",
      "        [30.5521],\n",
      "        [12.2987],\n",
      "        [15.7606],\n",
      "        [34.5603],\n",
      "        [33.8543],\n",
      "        [24.6789],\n",
      "        [17.9407],\n",
      "        [11.0526],\n",
      "        [17.0148],\n",
      "        [20.2450],\n",
      "        [19.2870],\n",
      "        [23.2941],\n",
      "        [ 9.9403],\n",
      "        [12.6005],\n",
      "        [15.8726],\n",
      "        [12.7341],\n",
      "        [14.8857],\n",
      "        [32.9122]])\n",
      "outputs G_r: tensor([[-12.6624],\n",
      "        [-42.1816],\n",
      "        [-25.4189],\n",
      "        [-60.3649],\n",
      "        [-37.5526],\n",
      "        [-38.9663],\n",
      "        [-52.4484],\n",
      "        [-26.8678],\n",
      "        [  0.9736],\n",
      "        [ -2.1014],\n",
      "        [-59.9852],\n",
      "        [-46.3426],\n",
      "        [-67.1059],\n",
      "        [-15.8185],\n",
      "        [  3.4567],\n",
      "        [-31.3965],\n",
      "        [-19.1275],\n",
      "        [ 12.6566],\n",
      "        [  6.0211],\n",
      "        [-23.8519],\n",
      "        [-29.8760],\n",
      "        [-60.9638],\n",
      "        [-21.1771],\n",
      "        [-15.6752],\n",
      "        [-52.6226],\n",
      "        [-37.7267],\n",
      "        [-74.8802],\n",
      "        [-74.4047],\n",
      "        [-62.5142],\n",
      "        [-39.3496],\n",
      "        [-32.2287],\n",
      "        [-13.8215]])\n",
      "outputs G_act: tensor([[29.6862],\n",
      "        [13.3088],\n",
      "        [19.4042],\n",
      "        [13.0445],\n",
      "        [12.8380],\n",
      "        [22.1366],\n",
      "        [16.9956],\n",
      "        [27.4509],\n",
      "        [ 8.6387],\n",
      "        [28.7838],\n",
      "        [32.3089],\n",
      "        [16.2410],\n",
      "        [29.4887],\n",
      "        [14.5826],\n",
      "        [17.2255],\n",
      "        [17.0635],\n",
      "        [12.6520],\n",
      "        [40.1455],\n",
      "        [23.9599],\n",
      "        [23.6060],\n",
      "        [21.7093],\n",
      "        [17.4214],\n",
      "        [17.5554],\n",
      "        [16.5110],\n",
      "        [32.5359],\n",
      "        [16.7432],\n",
      "        [18.1727],\n",
      "        [16.2376],\n",
      "        [19.8701],\n",
      "        [19.9034],\n",
      "        [16.9179],\n",
      "        [11.3547]])\n",
      "outputs G_r: tensor([[  1.5207],\n",
      "        [-36.1115],\n",
      "        [-40.4919],\n",
      "        [-40.9895],\n",
      "        [-36.0471],\n",
      "        [-21.0010],\n",
      "        [-23.4829],\n",
      "        [-11.9964],\n",
      "        [-69.1733],\n",
      "        [ -3.9912],\n",
      "        [-17.2725],\n",
      "        [-16.0635],\n",
      "        [ -5.6887],\n",
      "        [-41.4934],\n",
      "        [-17.5435],\n",
      "        [-32.1341],\n",
      "        [-49.3689],\n",
      "        [  5.5518],\n",
      "        [-12.2802],\n",
      "        [-15.3428],\n",
      "        [-16.7682],\n",
      "        [-47.3386],\n",
      "        [-57.6781],\n",
      "        [-47.2028],\n",
      "        [  7.3396],\n",
      "        [-53.9886],\n",
      "        [-20.3426],\n",
      "        [-17.5240],\n",
      "        [-18.1868],\n",
      "        [-20.5972],\n",
      "        [-21.3991],\n",
      "        [-51.0278]])\n",
      "outputs G_act: tensor([[29.1178],\n",
      "        [28.3707],\n",
      "        [17.6801],\n",
      "        [16.9161],\n",
      "        [25.8978],\n",
      "        [31.3567],\n",
      "        [12.1489],\n",
      "        [22.0771],\n",
      "        [28.5144],\n",
      "        [17.5945],\n",
      "        [27.5521],\n",
      "        [31.8752],\n",
      "        [10.3498],\n",
      "        [16.4460],\n",
      "        [11.5752],\n",
      "        [38.4984],\n",
      "        [15.4917],\n",
      "        [14.1687],\n",
      "        [31.5198],\n",
      "        [23.4795],\n",
      "        [24.0723],\n",
      "        [ 7.2780],\n",
      "        [11.8540],\n",
      "        [ 4.7805],\n",
      "        [12.9978],\n",
      "        [12.4693],\n",
      "        [24.7145],\n",
      "        [10.0140],\n",
      "        [29.3903],\n",
      "        [13.1274],\n",
      "        [16.2551],\n",
      "        [26.2182]])\n",
      "outputs G_r: tensor([[ -4.4493],\n",
      "        [ -1.6064],\n",
      "        [-21.0014],\n",
      "        [-24.5873],\n",
      "        [ -9.2116],\n",
      "        [-38.4297],\n",
      "        [-32.3463],\n",
      "        [-15.3852],\n",
      "        [ -9.5836],\n",
      "        [-33.5114],\n",
      "        [-28.7599],\n",
      "        [-18.1743],\n",
      "        [-49.1063],\n",
      "        [-45.2178],\n",
      "        [-62.1445],\n",
      "        [ -5.7488],\n",
      "        [-53.6668],\n",
      "        [-36.5271],\n",
      "        [ -2.7488],\n",
      "        [-10.6430],\n",
      "        [-11.9648],\n",
      "        [-40.0676],\n",
      "        [-48.2583],\n",
      "        [-36.1560],\n",
      "        [-57.4066],\n",
      "        [-57.4982],\n",
      "        [-19.8084],\n",
      "        [-77.8846],\n",
      "        [ -5.2246],\n",
      "        [-35.3862],\n",
      "        [-11.6144],\n",
      "        [ -1.6343]])\n",
      "outputs G_act: tensor([[23.1936],\n",
      "        [14.2545],\n",
      "        [20.9893],\n",
      "        [10.7341],\n",
      "        [18.4292],\n",
      "        [24.5865],\n",
      "        [30.7730],\n",
      "        [25.9767],\n",
      "        [21.6503],\n",
      "        [21.8010],\n",
      "        [11.8973],\n",
      "        [11.9024],\n",
      "        [14.0811],\n",
      "        [12.6999],\n",
      "        [39.9983],\n",
      "        [11.2536],\n",
      "        [19.7451],\n",
      "        [ 3.4779],\n",
      "        [26.3331],\n",
      "        [33.8996],\n",
      "        [12.9176],\n",
      "        [14.7666],\n",
      "        [10.0941],\n",
      "        [26.9926],\n",
      "        [15.8435],\n",
      "        [10.2495],\n",
      "        [21.3706],\n",
      "        [21.8992],\n",
      "        [19.1733],\n",
      "        [16.8185],\n",
      "        [11.7723],\n",
      "        [ 3.0164]])\n",
      "outputs G_r: tensor([[-36.7763],\n",
      "        [-26.3054],\n",
      "        [-21.6017],\n",
      "        [-17.4242],\n",
      "        [ -4.9131],\n",
      "        [  0.6657],\n",
      "        [  8.8217],\n",
      "        [-15.4083],\n",
      "        [-35.6816],\n",
      "        [-77.3034],\n",
      "        [-52.4878],\n",
      "        [-27.5929],\n",
      "        [-40.4698],\n",
      "        [-31.7675],\n",
      "        [ -9.6362],\n",
      "        [-20.8193],\n",
      "        [ -3.2479],\n",
      "        [-54.6798],\n",
      "        [-16.9242],\n",
      "        [-17.3114],\n",
      "        [-38.6242],\n",
      "        [-34.4879],\n",
      "        [-28.8552],\n",
      "        [-36.2124],\n",
      "        [-40.1707],\n",
      "        [-38.1634],\n",
      "        [-13.5124],\n",
      "        [ -7.7049],\n",
      "        [-23.2820],\n",
      "        [-30.4123],\n",
      "        [-49.3857],\n",
      "        [-42.2621]])\n",
      "outputs G_act: tensor([[36.6513],\n",
      "        [21.8256],\n",
      "        [26.0376],\n",
      "        [24.2497],\n",
      "        [14.1225],\n",
      "        [15.8336],\n",
      "        [18.5736],\n",
      "        [28.3131],\n",
      "        [15.4103],\n",
      "        [26.1478],\n",
      "        [13.3642],\n",
      "        [28.9071],\n",
      "        [25.5735],\n",
      "        [33.4158],\n",
      "        [35.8897],\n",
      "        [19.2116],\n",
      "        [22.3462],\n",
      "        [20.0855],\n",
      "        [22.0683],\n",
      "        [27.5200],\n",
      "        [21.8930],\n",
      "        [14.0555],\n",
      "        [18.0666],\n",
      "        [14.8015],\n",
      "        [13.1926],\n",
      "        [33.8639],\n",
      "        [26.9833],\n",
      "        [16.9060],\n",
      "        [13.6029],\n",
      "        [42.3894],\n",
      "        [24.4107],\n",
      "        [16.8268]])\n",
      "outputs G_r: tensor([[-38.9307],\n",
      "        [ -7.6083],\n",
      "        [-17.5362],\n",
      "        [-35.0138],\n",
      "        [-70.4152],\n",
      "        [-37.8791],\n",
      "        [-16.9802],\n",
      "        [  1.6283],\n",
      "        [-17.1353],\n",
      "        [ -3.0743],\n",
      "        [-43.8946],\n",
      "        [  1.1052],\n",
      "        [ -1.0883],\n",
      "        [ -0.8975],\n",
      "        [ -6.5438],\n",
      "        [-25.1615],\n",
      "        [-32.0046],\n",
      "        [-40.3207],\n",
      "        [-15.0823],\n",
      "        [-42.5474],\n",
      "        [-18.7069],\n",
      "        [-38.8727],\n",
      "        [-25.3406],\n",
      "        [-35.0838],\n",
      "        [-40.0868],\n",
      "        [  7.2834],\n",
      "        [ -7.4568],\n",
      "        [-17.5839],\n",
      "        [-42.0123],\n",
      "        [ 25.8506],\n",
      "        [  0.2253],\n",
      "        [-55.4684]])\n",
      "outputs G_act: tensor([[13.2722],\n",
      "        [12.0697],\n",
      "        [32.5515],\n",
      "        [10.8541],\n",
      "        [25.3246],\n",
      "        [27.0479],\n",
      "        [32.1213],\n",
      "        [15.1227],\n",
      "        [ 7.6982],\n",
      "        [26.8110],\n",
      "        [ 7.4477],\n",
      "        [36.2341],\n",
      "        [14.6612],\n",
      "        [28.4181],\n",
      "        [ 6.8726],\n",
      "        [17.5210],\n",
      "        [27.1565],\n",
      "        [12.8193],\n",
      "        [10.5043],\n",
      "        [14.5267],\n",
      "        [18.9272],\n",
      "        [20.8714],\n",
      "        [21.2629],\n",
      "        [14.8802],\n",
      "        [23.8532],\n",
      "        [25.2318],\n",
      "        [12.9404],\n",
      "        [20.6550],\n",
      "        [17.3377],\n",
      "        [14.2940],\n",
      "        [18.1217],\n",
      "        [28.8871]])\n",
      "outputs G_r: tensor([[-29.4259],\n",
      "        [-30.6561],\n",
      "        [-35.5026],\n",
      "        [-24.2186],\n",
      "        [ -6.9106],\n",
      "        [ -1.2135],\n",
      "        [  2.5256],\n",
      "        [-34.1225],\n",
      "        [-44.1952],\n",
      "        [-31.6043],\n",
      "        [-72.5470],\n",
      "        [  4.7978],\n",
      "        [-39.1580],\n",
      "        [-31.8641],\n",
      "        [-40.8214],\n",
      "        [-46.2188],\n",
      "        [-12.3272],\n",
      "        [-45.5877],\n",
      "        [-34.3020],\n",
      "        [-48.9968],\n",
      "        [-36.6726],\n",
      "        [-16.2582],\n",
      "        [ -9.5276],\n",
      "        [-48.5228],\n",
      "        [-22.5226],\n",
      "        [ -9.8248],\n",
      "        [-41.4653],\n",
      "        [-21.7067],\n",
      "        [-47.5082],\n",
      "        [-52.6755],\n",
      "        [-18.8423],\n",
      "        [-11.1533]])\n",
      "outputs G_act: tensor([[11.9018],\n",
      "        [23.5647],\n",
      "        [33.8782],\n",
      "        [19.0040],\n",
      "        [15.1510],\n",
      "        [14.9684],\n",
      "        [19.3220],\n",
      "        [30.1358],\n",
      "        [14.0697],\n",
      "        [15.0762],\n",
      "        [13.7921],\n",
      "        [16.4145],\n",
      "        [39.0044],\n",
      "        [20.1748],\n",
      "        [17.0125],\n",
      "        [14.3635],\n",
      "        [32.7303],\n",
      "        [27.9115],\n",
      "        [13.0652],\n",
      "        [15.8305],\n",
      "        [36.6404],\n",
      "        [18.0333],\n",
      "        [37.1612],\n",
      "        [19.9943],\n",
      "        [31.4040],\n",
      "        [15.2540],\n",
      "        [14.4087],\n",
      "        [49.2904],\n",
      "        [20.8961],\n",
      "        [17.6123],\n",
      "        [29.4389],\n",
      "        [ 6.2218]])\n",
      "outputs G_r: tensor([[-56.7539],\n",
      "        [ -7.8763],\n",
      "        [  7.0353],\n",
      "        [-35.1083],\n",
      "        [-38.7987],\n",
      "        [-47.1090],\n",
      "        [-30.9221],\n",
      "        [  4.6864],\n",
      "        [-68.3939],\n",
      "        [-29.4510],\n",
      "        [-58.8005],\n",
      "        [-30.9859],\n",
      "        [ -0.6176],\n",
      "        [-33.1443],\n",
      "        [-16.4830],\n",
      "        [-34.4299],\n",
      "        [  7.7410],\n",
      "        [ -0.9437],\n",
      "        [-64.5177],\n",
      "        [-40.1940],\n",
      "        [  1.7237],\n",
      "        [-18.6173],\n",
      "        [-23.9076],\n",
      "        [-10.2090],\n",
      "        [  6.3600],\n",
      "        [-36.8082],\n",
      "        [-42.9359],\n",
      "        [ -4.2299],\n",
      "        [-25.3618],\n",
      "        [-59.5863],\n",
      "        [  1.8918],\n",
      "        [-49.0641]])\n",
      "outputs G_act: tensor([[26.6518],\n",
      "        [38.8257],\n",
      "        [ 5.1399],\n",
      "        [26.4916],\n",
      "        [19.4213],\n",
      "        [19.7697],\n",
      "        [15.3716],\n",
      "        [27.8733],\n",
      "        [21.0875],\n",
      "        [22.2384],\n",
      "        [17.1907],\n",
      "        [13.5622],\n",
      "        [31.4781],\n",
      "        [14.9084],\n",
      "        [17.6925],\n",
      "        [24.7422],\n",
      "        [12.1456],\n",
      "        [21.9611],\n",
      "        [39.6401],\n",
      "        [11.3121],\n",
      "        [17.5163],\n",
      "        [12.8752],\n",
      "        [25.9019],\n",
      "        [20.9783],\n",
      "        [31.6466],\n",
      "        [23.5190],\n",
      "        [18.0876],\n",
      "        [24.0694],\n",
      "        [24.4532],\n",
      "        [13.7803],\n",
      "        [14.3643],\n",
      "        [16.4806]])\n",
      "outputs G_r: tensor([[-11.2871],\n",
      "        [ -9.6260],\n",
      "        [-72.7679],\n",
      "        [-20.2249],\n",
      "        [-18.5855],\n",
      "        [-44.4267],\n",
      "        [-11.2679],\n",
      "        [  1.9395],\n",
      "        [-13.9327],\n",
      "        [-18.1491],\n",
      "        [-37.6370],\n",
      "        [-53.1535],\n",
      "        [  1.8595],\n",
      "        [-45.2983],\n",
      "        [-46.5091],\n",
      "        [-21.3730],\n",
      "        [-32.5392],\n",
      "        [-29.3363],\n",
      "        [ -0.6594],\n",
      "        [-53.4549],\n",
      "        [-24.5298],\n",
      "        [-48.4604],\n",
      "        [-11.7098],\n",
      "        [-14.1728],\n",
      "        [-17.9197],\n",
      "        [ -6.7493],\n",
      "        [-33.2737],\n",
      "        [ -2.1404],\n",
      "        [-45.8996],\n",
      "        [-27.7806],\n",
      "        [-38.8216],\n",
      "        [-32.2321]])\n",
      "outputs G_act: tensor([[28.0207],\n",
      "        [21.0369],\n",
      "        [17.5507],\n",
      "        [18.8995],\n",
      "        [17.1378],\n",
      "        [13.2751],\n",
      "        [29.1344],\n",
      "        [18.2171],\n",
      "        [30.9306],\n",
      "        [14.7627],\n",
      "        [28.4983],\n",
      "        [40.1273],\n",
      "        [13.5417],\n",
      "        [24.0714],\n",
      "        [19.6862],\n",
      "        [ 8.9340],\n",
      "        [12.1563],\n",
      "        [15.3806],\n",
      "        [13.4239],\n",
      "        [13.9580],\n",
      "        [13.8135],\n",
      "        [17.9650],\n",
      "        [31.2433],\n",
      "        [22.1599],\n",
      "        [29.1590],\n",
      "        [11.9738],\n",
      "        [29.5513],\n",
      "        [17.4499],\n",
      "        [36.5809],\n",
      "        [15.3162],\n",
      "        [19.0354],\n",
      "        [20.6051]])\n",
      "outputs G_r: tensor([[ -5.2316],\n",
      "        [-32.7443],\n",
      "        [ -9.1866],\n",
      "        [-26.0705],\n",
      "        [-43.0460],\n",
      "        [-63.1233],\n",
      "        [-12.4866],\n",
      "        [-33.8135],\n",
      "        [ -5.2112],\n",
      "        [-39.1703],\n",
      "        [  2.8936],\n",
      "        [-13.1663],\n",
      "        [-30.9713],\n",
      "        [ -3.1885],\n",
      "        [-24.5168],\n",
      "        [-52.2757],\n",
      "        [-26.8421],\n",
      "        [-49.9383],\n",
      "        [-40.0289],\n",
      "        [-25.3805],\n",
      "        [-39.0345],\n",
      "        [-72.6237],\n",
      "        [-11.4828],\n",
      "        [-38.5686],\n",
      "        [ -3.6845],\n",
      "        [-42.1406],\n",
      "        [  3.6649],\n",
      "        [-21.4727],\n",
      "        [  0.6814],\n",
      "        [-29.4039],\n",
      "        [-35.4109],\n",
      "        [-15.5570]])\n",
      "outputs G_act: tensor([[11.1304],\n",
      "        [16.1792],\n",
      "        [26.2649],\n",
      "        [12.7124],\n",
      "        [27.4420],\n",
      "        [30.5158],\n",
      "        [13.7144],\n",
      "        [10.9405],\n",
      "        [31.1239],\n",
      "        [25.7039],\n",
      "        [25.1342],\n",
      "        [28.8894],\n",
      "        [33.4790],\n",
      "        [22.1253],\n",
      "        [14.6962],\n",
      "        [11.2520],\n",
      "        [12.0357],\n",
      "        [15.7897],\n",
      "        [40.4180],\n",
      "        [16.8970],\n",
      "        [19.8274],\n",
      "        [22.2351],\n",
      "        [22.6456],\n",
      "        [12.9095],\n",
      "        [25.5007],\n",
      "        [22.6428],\n",
      "        [10.7920],\n",
      "        [32.0044],\n",
      "        [20.7503],\n",
      "        [12.1308],\n",
      "        [17.0895],\n",
      "        [ 6.4024]])\n",
      "outputs G_r: tensor([[-40.9596],\n",
      "        [-27.0787],\n",
      "        [-29.2180],\n",
      "        [-32.1996],\n",
      "        [-33.9599],\n",
      "        [-12.4576],\n",
      "        [-35.7820],\n",
      "        [-69.3107],\n",
      "        [ -2.3007],\n",
      "        [-38.4240],\n",
      "        [-12.7577],\n",
      "        [  0.4022],\n",
      "        [ -2.4677],\n",
      "        [-18.7581],\n",
      "        [-39.2560],\n",
      "        [-39.0023],\n",
      "        [-38.0032],\n",
      "        [-80.6328],\n",
      "        [ -0.9323],\n",
      "        [-31.4101],\n",
      "        [-11.6779],\n",
      "        [-20.9051],\n",
      "        [-45.6837],\n",
      "        [-38.0000],\n",
      "        [ -9.0553],\n",
      "        [-34.9813],\n",
      "        [-38.9184],\n",
      "        [-12.1207],\n",
      "        [-34.7505],\n",
      "        [-29.4082],\n",
      "        [-40.9540],\n",
      "        [-44.2827]])\n",
      "outputs G_act: tensor([[17.9408],\n",
      "        [33.1465],\n",
      "        [18.0801],\n",
      "        [10.6957],\n",
      "        [13.5334],\n",
      "        [29.9937],\n",
      "        [14.9583],\n",
      "        [16.1097],\n",
      "        [31.6265],\n",
      "        [ 6.9596],\n",
      "        [22.7678],\n",
      "        [29.0676],\n",
      "        [13.3500],\n",
      "        [19.5098],\n",
      "        [23.4223],\n",
      "        [12.6683],\n",
      "        [27.6741],\n",
      "        [13.1582],\n",
      "        [14.9786],\n",
      "        [33.5090],\n",
      "        [ 7.9660],\n",
      "        [12.4773],\n",
      "        [11.0391],\n",
      "        [20.4902],\n",
      "        [ 8.3994],\n",
      "        [28.3723],\n",
      "        [38.8734],\n",
      "        [26.7637],\n",
      "        [10.5113],\n",
      "        [26.2342],\n",
      "        [23.1894],\n",
      "        [21.5972]])\n",
      "outputs G_r: tensor([[-1.2055e+01],\n",
      "        [ 1.0569e+01],\n",
      "        [-3.7361e+01],\n",
      "        [-3.9661e+01],\n",
      "        [-3.8868e+01],\n",
      "        [-1.4793e+01],\n",
      "        [-3.3814e+01],\n",
      "        [-3.3354e+01],\n",
      "        [ 3.8716e-01],\n",
      "        [-6.3412e+01],\n",
      "        [-9.4085e+00],\n",
      "        [-2.1352e+00],\n",
      "        [-2.4705e+01],\n",
      "        [-3.3502e+01],\n",
      "        [-1.6660e+01],\n",
      "        [-3.4521e+01],\n",
      "        [-2.9319e+01],\n",
      "        [-2.7617e+01],\n",
      "        [-3.5641e+01],\n",
      "        [ 1.3544e+01],\n",
      "        [-3.9866e+01],\n",
      "        [-5.0070e+01],\n",
      "        [-3.7958e+01],\n",
      "        [-4.6004e+01],\n",
      "        [-6.7299e+01],\n",
      "        [-6.5727e-02],\n",
      "        [-1.1483e+01],\n",
      "        [ 6.4865e-01],\n",
      "        [-2.6814e+01],\n",
      "        [ 1.9646e-04],\n",
      "        [-1.0717e+01],\n",
      "        [-2.1715e+01]])\n",
      "outputs G_act: tensor([[21.0115],\n",
      "        [11.1307],\n",
      "        [19.1487],\n",
      "        [30.0807],\n",
      "        [40.4397],\n",
      "        [ 8.9740],\n",
      "        [14.6767],\n",
      "        [ 8.4957],\n",
      "        [26.9442],\n",
      "        [21.1843],\n",
      "        [15.1651],\n",
      "        [ 9.1540],\n",
      "        [30.1979],\n",
      "        [15.9517],\n",
      "        [32.0805],\n",
      "        [32.7825],\n",
      "        [30.9134],\n",
      "        [10.3612],\n",
      "        [24.3593],\n",
      "        [32.8984],\n",
      "        [12.2067],\n",
      "        [17.7196],\n",
      "        [31.2412],\n",
      "        [19.2740],\n",
      "        [21.8472],\n",
      "        [24.8511],\n",
      "        [14.7364],\n",
      "        [11.5844],\n",
      "        [20.8881],\n",
      "        [23.2629],\n",
      "        [14.1419],\n",
      "        [15.4931]])\n",
      "outputs G_r: tensor([[-20.0822],\n",
      "        [-58.9391],\n",
      "        [-32.3084],\n",
      "        [ -1.9850],\n",
      "        [ 14.9889],\n",
      "        [-54.6696],\n",
      "        [-27.3579],\n",
      "        [-55.3386],\n",
      "        [ -0.0900],\n",
      "        [-27.2680],\n",
      "        [-67.1863],\n",
      "        [-41.2078],\n",
      "        [ -6.1726],\n",
      "        [-43.3625],\n",
      "        [-23.4470],\n",
      "        [ -2.2314],\n",
      "        [-11.1028],\n",
      "        [-68.1624],\n",
      "        [-47.0666],\n",
      "        [ -5.2452],\n",
      "        [-54.7488],\n",
      "        [-33.2327],\n",
      "        [  6.5460],\n",
      "        [-30.6979],\n",
      "        [ -0.6896],\n",
      "        [-12.3770],\n",
      "        [-36.7427],\n",
      "        [-21.0744],\n",
      "        [-16.7900],\n",
      "        [-12.6192],\n",
      "        [-30.5121],\n",
      "        [-27.7463]])\n",
      "outputs G_act: tensor([[30.4610],\n",
      "        [11.3595],\n",
      "        [19.1398],\n",
      "        [22.2807],\n",
      "        [27.7205],\n",
      "        [16.8290],\n",
      "        [15.8648],\n",
      "        [20.0189],\n",
      "        [ 9.5869],\n",
      "        [11.2258],\n",
      "        [23.0150],\n",
      "        [32.2107],\n",
      "        [24.6608],\n",
      "        [13.1144],\n",
      "        [25.8923],\n",
      "        [25.5681],\n",
      "        [27.5560],\n",
      "        [13.1649],\n",
      "        [21.6988],\n",
      "        [19.8967],\n",
      "        [20.7740],\n",
      "        [22.0945],\n",
      "        [ 6.7675],\n",
      "        [23.1985],\n",
      "        [20.6536],\n",
      "        [25.1831],\n",
      "        [23.3470],\n",
      "        [27.1740],\n",
      "        [ 5.8450],\n",
      "        [20.5570],\n",
      "        [13.1541],\n",
      "        [13.7378]])\n",
      "outputs G_r: tensor([[ -3.1143],\n",
      "        [-63.4146],\n",
      "        [-48.8821],\n",
      "        [-22.1839],\n",
      "        [ -9.2262],\n",
      "        [-31.1801],\n",
      "        [-32.6932],\n",
      "        [-24.3318],\n",
      "        [-54.3770],\n",
      "        [-48.5944],\n",
      "        [-16.9765],\n",
      "        [ -3.8056],\n",
      "        [-10.5400],\n",
      "        [-38.9190],\n",
      "        [-25.1307],\n",
      "        [  3.4977],\n",
      "        [ -5.5764],\n",
      "        [-51.8191],\n",
      "        [ -5.3847],\n",
      "        [-21.3088],\n",
      "        [-32.9797],\n",
      "        [-25.8169],\n",
      "        [-50.5852],\n",
      "        [-25.7617],\n",
      "        [ -7.9631],\n",
      "        [-15.1929],\n",
      "        [-41.9439],\n",
      "        [ -7.7322],\n",
      "        [-93.0071],\n",
      "        [-25.5540],\n",
      "        [-58.7559],\n",
      "        [-42.6692]])\n",
      "outputs G_act: tensor([[18.2611],\n",
      "        [17.4563],\n",
      "        [25.8146],\n",
      "        [28.7324],\n",
      "        [26.8472],\n",
      "        [16.3860],\n",
      "        [11.7547],\n",
      "        [23.9017],\n",
      "        [12.6030],\n",
      "        [21.5670],\n",
      "        [33.6051],\n",
      "        [16.3028],\n",
      "        [ 5.0978],\n",
      "        [27.5123],\n",
      "        [29.3656],\n",
      "        [13.7515],\n",
      "        [18.7550],\n",
      "        [21.5081],\n",
      "        [33.8789],\n",
      "        [25.4454],\n",
      "        [15.7859],\n",
      "        [21.9446],\n",
      "        [16.5408],\n",
      "        [15.5535],\n",
      "        [11.9570],\n",
      "        [24.9964],\n",
      "        [27.4003],\n",
      "        [28.0992],\n",
      "        [24.6756],\n",
      "        [15.6685],\n",
      "        [20.4292],\n",
      "        [28.3806]])\n",
      "outputs G_r: tensor([[-35.5987],\n",
      "        [-45.7747],\n",
      "        [ -6.2379],\n",
      "        [  4.1231],\n",
      "        [-22.1529],\n",
      "        [-38.8025],\n",
      "        [-28.7400],\n",
      "        [-11.3341],\n",
      "        [-54.4776],\n",
      "        [ -2.2834],\n",
      "        [ -9.4322],\n",
      "        [-27.5990],\n",
      "        [-47.4802],\n",
      "        [ -3.1686],\n",
      "        [ -1.6809],\n",
      "        [-52.0570],\n",
      "        [-38.4176],\n",
      "        [-22.0503],\n",
      "        [  7.1473],\n",
      "        [-17.9067],\n",
      "        [-34.8688],\n",
      "        [-24.5917],\n",
      "        [-39.7690],\n",
      "        [-40.4081],\n",
      "        [-71.1899],\n",
      "        [-16.6155],\n",
      "        [-24.6269],\n",
      "        [-29.3552],\n",
      "        [ -8.0315],\n",
      "        [-27.6458],\n",
      "        [ -9.0126],\n",
      "        [ -7.5149]])\n",
      "outputs G_act: tensor([[31.8062],\n",
      "        [11.9246],\n",
      "        [21.0995],\n",
      "        [35.7619],\n",
      "        [29.1201],\n",
      "        [36.4792],\n",
      "        [11.9751],\n",
      "        [20.4786],\n",
      "        [18.2243],\n",
      "        [28.7822],\n",
      "        [12.8988],\n",
      "        [11.5338],\n",
      "        [31.7741],\n",
      "        [24.5282],\n",
      "        [21.2930],\n",
      "        [19.9293],\n",
      "        [10.7658],\n",
      "        [26.5341],\n",
      "        [19.3746],\n",
      "        [26.9888],\n",
      "        [20.0997],\n",
      "        [13.1108],\n",
      "        [25.0735],\n",
      "        [38.9507],\n",
      "        [31.2258],\n",
      "        [ 5.4941],\n",
      "        [ 8.9802],\n",
      "        [30.5445],\n",
      "        [ 9.4149],\n",
      "        [20.4383],\n",
      "        [ 9.7339],\n",
      "        [16.9263]])\n",
      "outputs G_r: tensor([[ 2.2760e+00],\n",
      "        [-3.9428e+01],\n",
      "        [-3.8480e+01],\n",
      "        [ 9.4221e-01],\n",
      "        [-3.3792e+01],\n",
      "        [ 6.2283e-02],\n",
      "        [-5.5944e+01],\n",
      "        [-2.2160e+00],\n",
      "        [-2.9436e+01],\n",
      "        [ 3.0280e+00],\n",
      "        [-5.4095e+01],\n",
      "        [-3.2492e+01],\n",
      "        [-8.5821e+00],\n",
      "        [-4.5167e+01],\n",
      "        [-2.7429e+01],\n",
      "        [-2.7092e+01],\n",
      "        [-4.1767e+01],\n",
      "        [-4.8175e+01],\n",
      "        [-3.2773e+01],\n",
      "        [-1.0095e+01],\n",
      "        [-1.6747e+01],\n",
      "        [-5.2034e+01],\n",
      "        [-2.7103e+00],\n",
      "        [-1.4550e+01],\n",
      "        [-8.6914e-01],\n",
      "        [-6.4757e+01],\n",
      "        [-2.5549e+01],\n",
      "        [ 3.3500e+00],\n",
      "        [-3.4304e+01],\n",
      "        [-2.7302e+01],\n",
      "        [-6.7936e+01],\n",
      "        [-2.9808e+01]])\n",
      "outputs G_act: tensor([[12.6041],\n",
      "        [18.6617],\n",
      "        [21.5128],\n",
      "        [21.6676],\n",
      "        [24.3241],\n",
      "        [32.8620],\n",
      "        [33.4145],\n",
      "        [19.9617],\n",
      "        [23.2504],\n",
      "        [38.6404],\n",
      "        [12.9563],\n",
      "        [43.5951],\n",
      "        [28.9419],\n",
      "        [20.7597],\n",
      "        [25.6812],\n",
      "        [20.1025],\n",
      "        [32.1612],\n",
      "        [23.5667],\n",
      "        [11.5925],\n",
      "        [34.2007],\n",
      "        [22.2952],\n",
      "        [13.3548],\n",
      "        [20.6674],\n",
      "        [11.7298],\n",
      "        [15.4839],\n",
      "        [31.1782],\n",
      "        [ 8.0078],\n",
      "        [61.2128],\n",
      "        [40.2870],\n",
      "        [25.4710],\n",
      "        [18.2037],\n",
      "        [10.4417]])\n",
      "outputs G_r: tensor([[-44.8822],\n",
      "        [-25.2100],\n",
      "        [ -9.4764],\n",
      "        [-36.8593],\n",
      "        [ -6.3491],\n",
      "        [ 11.0160],\n",
      "        [  4.6499],\n",
      "        [-27.1630],\n",
      "        [-18.0003],\n",
      "        [  4.6763],\n",
      "        [-34.5891],\n",
      "        [ 12.0630],\n",
      "        [-34.3976],\n",
      "        [-24.7398],\n",
      "        [-17.0774],\n",
      "        [-22.1753],\n",
      "        [ -2.1569],\n",
      "        [-20.6476],\n",
      "        [-25.2923],\n",
      "        [  3.2763],\n",
      "        [-22.6771],\n",
      "        [-52.4325],\n",
      "        [-68.7751],\n",
      "        [-58.4214],\n",
      "        [-29.5981],\n",
      "        [  2.7539],\n",
      "        [-34.1839],\n",
      "        [ 40.8617],\n",
      "        [-21.7920],\n",
      "        [ -1.6095],\n",
      "        [-24.5100],\n",
      "        [-60.6808]])\n",
      "outputs G_act: tensor([[11.7315],\n",
      "        [11.1076],\n",
      "        [13.1568],\n",
      "        [17.4242],\n",
      "        [25.4440],\n",
      "        [28.8889],\n",
      "        [14.7443],\n",
      "        [19.2969],\n",
      "        [21.3514],\n",
      "        [22.4492],\n",
      "        [13.5016],\n",
      "        [12.4840],\n",
      "        [ 8.3353],\n",
      "        [12.0443],\n",
      "        [15.2668],\n",
      "        [25.5194],\n",
      "        [31.1677],\n",
      "        [22.3203],\n",
      "        [38.8671],\n",
      "        [39.6847],\n",
      "        [15.4931],\n",
      "        [14.6136],\n",
      "        [14.9018],\n",
      "        [27.8311],\n",
      "        [25.7818],\n",
      "        [30.3825],\n",
      "        [14.2335],\n",
      "        [35.4146],\n",
      "        [23.2701],\n",
      "        [27.6238],\n",
      "        [11.4233],\n",
      "        [18.3756]])\n",
      "outputs G_r: tensor([[-33.3673],\n",
      "        [-30.7545],\n",
      "        [-38.6938],\n",
      "        [-31.3198],\n",
      "        [-28.9323],\n",
      "        [ -0.3219],\n",
      "        [-52.0129],\n",
      "        [-24.7527],\n",
      "        [ -4.4959],\n",
      "        [-32.0746],\n",
      "        [-12.7542],\n",
      "        [-51.9568],\n",
      "        [-48.4735],\n",
      "        [-38.3285],\n",
      "        [-32.4747],\n",
      "        [-23.9252],\n",
      "        [ -7.7591],\n",
      "        [-21.6146],\n",
      "        [ -5.3580],\n",
      "        [-11.6734],\n",
      "        [-45.4901],\n",
      "        [-56.4457],\n",
      "        [-52.4822],\n",
      "        [ -0.4941],\n",
      "        [-25.0481],\n",
      "        [-15.1487],\n",
      "        [-51.1151],\n",
      "        [  1.6956],\n",
      "        [ -7.9347],\n",
      "        [  0.9748],\n",
      "        [-67.6432],\n",
      "        [-32.2837]])\n",
      "outputs G_act: tensor([[34.6906],\n",
      "        [17.5261],\n",
      "        [25.4683],\n",
      "        [ 7.7979],\n",
      "        [20.5070],\n",
      "        [11.8369],\n",
      "        [26.1888],\n",
      "        [ 9.3867],\n",
      "        [18.7056],\n",
      "        [27.6743],\n",
      "        [20.7870],\n",
      "        [21.2851],\n",
      "        [35.2930],\n",
      "        [36.2329],\n",
      "        [14.8454],\n",
      "        [14.1651],\n",
      "        [16.7415],\n",
      "        [15.5735],\n",
      "        [17.5733],\n",
      "        [ 8.9516],\n",
      "        [ 5.2191],\n",
      "        [28.8778],\n",
      "        [28.8918],\n",
      "        [17.2752],\n",
      "        [28.3336],\n",
      "        [ 6.2315],\n",
      "        [30.9378],\n",
      "        [20.7381],\n",
      "        [21.9957],\n",
      "        [10.9743],\n",
      "        [23.5056],\n",
      "        [16.4727]])\n",
      "outputs G_r: tensor([[  2.9952],\n",
      "        [-48.6957],\n",
      "        [ -1.8535],\n",
      "        [-58.0713],\n",
      "        [-31.8323],\n",
      "        [-57.2819],\n",
      "        [ -8.9217],\n",
      "        [-69.4496],\n",
      "        [-26.0791],\n",
      "        [-12.9169],\n",
      "        [-10.1025],\n",
      "        [-25.0057],\n",
      "        [-31.4618],\n",
      "        [ -2.7473],\n",
      "        [-37.3994],\n",
      "        [-37.8496],\n",
      "        [-24.0816],\n",
      "        [-29.5935],\n",
      "        [-22.3704],\n",
      "        [-41.9680],\n",
      "        [-47.0811],\n",
      "        [  7.7874],\n",
      "        [ 10.4204],\n",
      "        [-53.8848],\n",
      "        [-15.0636],\n",
      "        [-50.3736],\n",
      "        [ -4.7000],\n",
      "        [-27.3818],\n",
      "        [ -9.5099],\n",
      "        [-66.2822],\n",
      "        [ -4.0923],\n",
      "        [-36.6672]])\n",
      "outputs G_act: tensor([[22.8611],\n",
      "        [34.3565],\n",
      "        [22.9523],\n",
      "        [14.0152],\n",
      "        [21.7469],\n",
      "        [13.9404],\n",
      "        [31.1904],\n",
      "        [24.9394],\n",
      "        [16.8805],\n",
      "        [11.4676],\n",
      "        [15.0911],\n",
      "        [ 6.5291],\n",
      "        [17.9265],\n",
      "        [17.3955],\n",
      "        [14.6762],\n",
      "        [12.1895],\n",
      "        [30.7834],\n",
      "        [48.9742],\n",
      "        [14.3348],\n",
      "        [12.2390],\n",
      "        [16.7575],\n",
      "        [26.4252],\n",
      "        [29.6338],\n",
      "        [24.6348],\n",
      "        [34.2216],\n",
      "        [29.2133],\n",
      "        [22.2186],\n",
      "        [27.7120],\n",
      "        [27.4072],\n",
      "        [ 9.9094],\n",
      "        [15.6276],\n",
      "        [23.8460]])\n",
      "outputs G_r: tensor([[ -7.3690],\n",
      "        [-12.2244],\n",
      "        [ -3.5175],\n",
      "        [-39.0499],\n",
      "        [-21.6618],\n",
      "        [-38.7055],\n",
      "        [  5.3764],\n",
      "        [-13.1898],\n",
      "        [-33.9157],\n",
      "        [-50.7840],\n",
      "        [-24.3775],\n",
      "        [-50.0709],\n",
      "        [-16.1946],\n",
      "        [-16.5172],\n",
      "        [-50.3579],\n",
      "        [-43.8388],\n",
      "        [-15.9196],\n",
      "        [  8.9099],\n",
      "        [-41.0641],\n",
      "        [-35.2425],\n",
      "        [-38.9775],\n",
      "        [  0.5206],\n",
      "        [-23.6939],\n",
      "        [-38.9747],\n",
      "        [ -8.3165],\n",
      "        [-34.2397],\n",
      "        [-20.5238],\n",
      "        [-21.0120],\n",
      "        [ -2.9884],\n",
      "        [-42.5404],\n",
      "        [-27.1552],\n",
      "        [-10.7142]])\n",
      "outputs G_act: tensor([[28.3162],\n",
      "        [41.1733],\n",
      "        [26.7228],\n",
      "        [29.7185],\n",
      "        [17.9544],\n",
      "        [10.5430],\n",
      "        [ 9.6660],\n",
      "        [16.9452],\n",
      "        [11.7365],\n",
      "        [37.9147],\n",
      "        [23.9008],\n",
      "        [19.8195],\n",
      "        [17.4344],\n",
      "        [ 8.3804],\n",
      "        [13.6315],\n",
      "        [32.4239],\n",
      "        [28.5096],\n",
      "        [16.1624],\n",
      "        [21.4198],\n",
      "        [12.9069],\n",
      "        [15.4187],\n",
      "        [11.5940],\n",
      "        [33.1372],\n",
      "        [12.0453],\n",
      "        [ 9.9439],\n",
      "        [14.6108],\n",
      "        [ 6.2374],\n",
      "        [44.8213],\n",
      "        [ 8.0062],\n",
      "        [10.9959],\n",
      "        [15.4029],\n",
      "        [18.8481]])\n",
      "outputs G_r: tensor([[ -8.2217],\n",
      "        [-19.3643],\n",
      "        [-71.8680],\n",
      "        [-20.5430],\n",
      "        [-38.6313],\n",
      "        [-65.6895],\n",
      "        [-34.3608],\n",
      "        [-30.9503],\n",
      "        [-64.5929],\n",
      "        [  2.1476],\n",
      "        [-34.0614],\n",
      "        [-19.1968],\n",
      "        [-35.7323],\n",
      "        [-46.7255],\n",
      "        [-41.9769],\n",
      "        [-15.7546],\n",
      "        [-21.5707],\n",
      "        [-46.5542],\n",
      "        [-35.1123],\n",
      "        [-49.8673],\n",
      "        [-36.7547],\n",
      "        [-47.7321],\n",
      "        [  1.0507],\n",
      "        [-49.8095],\n",
      "        [-19.5544],\n",
      "        [-61.1828],\n",
      "        [-45.8463],\n",
      "        [ -2.5792],\n",
      "        [-64.0629],\n",
      "        [-67.0318],\n",
      "        [-45.4694],\n",
      "        [-24.0357]])\n",
      "outputs G_act: tensor([[11.7167],\n",
      "        [12.6843],\n",
      "        [20.4257],\n",
      "        [51.4280],\n",
      "        [21.1223],\n",
      "        [29.5794],\n",
      "        [20.1534],\n",
      "        [18.5878],\n",
      "        [28.2614],\n",
      "        [17.9635],\n",
      "        [24.3978],\n",
      "        [10.5828],\n",
      "        [30.2885],\n",
      "        [21.5480],\n",
      "        [20.1964],\n",
      "        [13.3448],\n",
      "        [20.4987],\n",
      "        [22.6853],\n",
      "        [14.0703],\n",
      "        [28.7245],\n",
      "        [38.0071],\n",
      "        [15.6724],\n",
      "        [12.6984],\n",
      "        [23.8590],\n",
      "        [18.0832],\n",
      "        [36.0234],\n",
      "        [23.4053],\n",
      "        [16.8830],\n",
      "        [11.2516],\n",
      "        [27.5685],\n",
      "        [31.6878],\n",
      "        [26.2007]])\n",
      "outputs G_r: tensor([[-48.0422],\n",
      "        [-77.7450],\n",
      "        [ -9.0200],\n",
      "        [  2.0352],\n",
      "        [-54.6339],\n",
      "        [-26.6040],\n",
      "        [-39.0107],\n",
      "        [-20.2221],\n",
      "        [-29.6863],\n",
      "        [-38.1995],\n",
      "        [-31.2707],\n",
      "        [-70.1959],\n",
      "        [-14.5631],\n",
      "        [-34.1147],\n",
      "        [-51.0575],\n",
      "        [-33.6106],\n",
      "        [-40.7141],\n",
      "        [ -9.9841],\n",
      "        [-48.9523],\n",
      "        [-18.4883],\n",
      "        [  8.9384],\n",
      "        [-34.6167],\n",
      "        [-64.4506],\n",
      "        [-27.1948],\n",
      "        [-36.1169],\n",
      "        [-11.3507],\n",
      "        [-13.9946],\n",
      "        [-30.4153],\n",
      "        [-63.5201],\n",
      "        [-12.3809],\n",
      "        [ -3.7798],\n",
      "        [-30.8235]])\n",
      "outputs G_act: tensor([[19.2773],\n",
      "        [21.6453],\n",
      "        [31.1020],\n",
      "        [17.5106],\n",
      "        [16.4033],\n",
      "        [12.3662],\n",
      "        [20.5351],\n",
      "        [32.4344],\n",
      "        [13.6981],\n",
      "        [30.1794],\n",
      "        [12.9938],\n",
      "        [12.0274],\n",
      "        [16.8167],\n",
      "        [14.3976],\n",
      "        [15.9498],\n",
      "        [15.5925],\n",
      "        [18.1217],\n",
      "        [11.4165],\n",
      "        [37.2321],\n",
      "        [33.7271],\n",
      "        [15.3645],\n",
      "        [22.3731],\n",
      "        [21.3085],\n",
      "        [16.1591],\n",
      "        [27.7806],\n",
      "        [18.1488],\n",
      "        [32.7373],\n",
      "        [18.7316],\n",
      "        [25.4208],\n",
      "        [22.6450],\n",
      "        [28.8149],\n",
      "        [14.2822]])\n",
      "outputs G_r: tensor([[-4.6953e+01],\n",
      "        [-2.8739e+01],\n",
      "        [ 9.2896e+00],\n",
      "        [-3.8686e+01],\n",
      "        [-4.2546e+01],\n",
      "        [-6.0086e+01],\n",
      "        [-4.7253e+01],\n",
      "        [-1.6306e+01],\n",
      "        [-6.9204e+01],\n",
      "        [-5.5657e+00],\n",
      "        [-3.0813e+01],\n",
      "        [-4.1074e+01],\n",
      "        [-3.7413e+01],\n",
      "        [-2.7635e+01],\n",
      "        [-3.5984e+01],\n",
      "        [-2.9705e+01],\n",
      "        [-4.2849e+01],\n",
      "        [-4.4309e+01],\n",
      "        [-2.6872e+01],\n",
      "        [ 7.3032e+00],\n",
      "        [-1.3636e+01],\n",
      "        [-1.0675e+01],\n",
      "        [-2.4585e+01],\n",
      "        [-1.9358e+01],\n",
      "        [-7.4308e+00],\n",
      "        [-1.4722e+01],\n",
      "        [ 1.8948e-02],\n",
      "        [-2.5702e+01],\n",
      "        [-5.0784e+00],\n",
      "        [-7.9269e+00],\n",
      "        [-5.3806e+00],\n",
      "        [-3.8575e+01]])\n",
      "outputs G_act: tensor([[12.6032],\n",
      "        [13.3676],\n",
      "        [17.5259],\n",
      "        [17.8277],\n",
      "        [27.6179],\n",
      "        [17.9876],\n",
      "        [18.5503],\n",
      "        [17.1468],\n",
      "        [14.1407],\n",
      "        [ 9.3159],\n",
      "        [10.8330],\n",
      "        [27.7060],\n",
      "        [26.6867],\n",
      "        [16.1676],\n",
      "        [26.2820],\n",
      "        [12.5340],\n",
      "        [28.6763],\n",
      "        [20.8288],\n",
      "        [19.8170],\n",
      "        [23.1196],\n",
      "        [17.7810],\n",
      "        [17.4141],\n",
      "        [65.7344],\n",
      "        [ 8.0504],\n",
      "        [23.0708],\n",
      "        [12.5036],\n",
      "        [27.9306],\n",
      "        [14.5779],\n",
      "        [35.4034],\n",
      "        [12.3742],\n",
      "        [14.9883],\n",
      "        [ 8.4614]])\n",
      "outputs G_r: tensor([[-13.8320],\n",
      "        [-57.3032],\n",
      "        [-39.7875],\n",
      "        [-40.0880],\n",
      "        [  3.8095],\n",
      "        [-72.3624],\n",
      "        [-24.9998],\n",
      "        [-25.0349],\n",
      "        [-51.1229],\n",
      "        [-79.8997],\n",
      "        [-53.0732],\n",
      "        [ -1.4783],\n",
      "        [  7.0213],\n",
      "        [-26.2179],\n",
      "        [-15.9967],\n",
      "        [-29.8592],\n",
      "        [ -0.2374],\n",
      "        [ -8.1536],\n",
      "        [-31.9849],\n",
      "        [-20.3480],\n",
      "        [-36.7929],\n",
      "        [-30.6781],\n",
      "        [ 38.9108],\n",
      "        [-66.9457],\n",
      "        [-33.8240],\n",
      "        [-31.7961],\n",
      "        [  2.4857],\n",
      "        [-29.4987],\n",
      "        [  0.4935],\n",
      "        [-50.1180],\n",
      "        [-30.7932],\n",
      "        [-57.9881]])\n",
      "outputs G_act: tensor([[10.3347],\n",
      "        [28.4392],\n",
      "        [19.1287],\n",
      "        [16.3110],\n",
      "        [34.7089],\n",
      "        [55.7870],\n",
      "        [15.6958],\n",
      "        [11.6950],\n",
      "        [30.7974],\n",
      "        [16.8621],\n",
      "        [21.7894],\n",
      "        [14.7951],\n",
      "        [23.4812],\n",
      "        [22.1461],\n",
      "        [39.9049],\n",
      "        [30.9401],\n",
      "        [14.5118],\n",
      "        [13.5971],\n",
      "        [11.6614],\n",
      "        [14.0842],\n",
      "        [15.1781],\n",
      "        [15.8597],\n",
      "        [17.6383],\n",
      "        [21.1246],\n",
      "        [20.1173],\n",
      "        [26.1840],\n",
      "        [15.2698],\n",
      "        [26.8433],\n",
      "        [13.6256],\n",
      "        [22.8823],\n",
      "        [15.2220],\n",
      "        [18.1307]])\n",
      "outputs G_r: tensor([[-70.7922],\n",
      "        [ -2.7660],\n",
      "        [-24.8927],\n",
      "        [-21.6315],\n",
      "        [ -4.4110],\n",
      "        [ 33.5495],\n",
      "        [-51.6935],\n",
      "        [-36.3216],\n",
      "        [  5.5392],\n",
      "        [-39.9321],\n",
      "        [ -6.0890],\n",
      "        [-37.3180],\n",
      "        [-18.4839],\n",
      "        [-36.8824],\n",
      "        [-11.1920],\n",
      "        [-16.7260],\n",
      "        [-20.6489],\n",
      "        [-55.5091],\n",
      "        [-34.0057],\n",
      "        [-29.2795],\n",
      "        [-44.6720],\n",
      "        [-35.3240],\n",
      "        [-45.7994],\n",
      "        [-17.7556],\n",
      "        [ -9.6829],\n",
      "        [ -9.8337],\n",
      "        [-51.0197],\n",
      "        [  5.7807],\n",
      "        [-42.0810],\n",
      "        [-17.0005],\n",
      "        [-22.9135],\n",
      "        [-33.3870]])\n",
      "outputs G_act: tensor([[27.4705],\n",
      "        [31.9423],\n",
      "        [16.6047],\n",
      "        [28.0826],\n",
      "        [23.4212],\n",
      "        [24.5588],\n",
      "        [26.3959],\n",
      "        [12.8856],\n",
      "        [21.3070],\n",
      "        [ 9.3017],\n",
      "        [14.3740],\n",
      "        [13.9070],\n",
      "        [23.0458],\n",
      "        [27.9607],\n",
      "        [12.2973],\n",
      "        [11.9498],\n",
      "        [20.9368],\n",
      "        [12.0674],\n",
      "        [15.5476],\n",
      "        [15.4453],\n",
      "        [29.2828],\n",
      "        [28.4615],\n",
      "        [24.3421],\n",
      "        [22.0326],\n",
      "        [21.2070],\n",
      "        [13.1996],\n",
      "        [ 5.9310],\n",
      "        [25.8388],\n",
      "        [25.5203],\n",
      "        [26.7374],\n",
      "        [22.8120],\n",
      "        [30.1986]])\n",
      "outputs G_r: tensor([[  0.3390],\n",
      "        [ -3.0725],\n",
      "        [-14.2665],\n",
      "        [-11.2480],\n",
      "        [-10.8476],\n",
      "        [ -6.0635],\n",
      "        [ -9.5546],\n",
      "        [-47.9408],\n",
      "        [ -7.1258],\n",
      "        [-36.0527],\n",
      "        [-66.8085],\n",
      "        [-15.4333],\n",
      "        [-13.0271],\n",
      "        [-17.2166],\n",
      "        [-23.9191],\n",
      "        [-57.8070],\n",
      "        [-30.6666],\n",
      "        [-31.3161],\n",
      "        [-39.2024],\n",
      "        [-57.6404],\n",
      "        [  0.3239],\n",
      "        [-10.7196],\n",
      "        [-27.1209],\n",
      "        [-37.8641],\n",
      "        [-20.5717],\n",
      "        [-39.0064],\n",
      "        [-67.5251],\n",
      "        [  0.8704],\n",
      "        [ -4.2895],\n",
      "        [ 10.5254],\n",
      "        [-46.7748],\n",
      "        [-12.5638]])\n",
      "outputs G_act: tensor([[16.4416],\n",
      "        [11.1300],\n",
      "        [15.0674],\n",
      "        [ 9.6809],\n",
      "        [15.8042],\n",
      "        [19.0365],\n",
      "        [16.5055],\n",
      "        [13.6641],\n",
      "        [31.7522],\n",
      "        [15.9977],\n",
      "        [29.5685],\n",
      "        [16.3578],\n",
      "        [15.0761],\n",
      "        [18.5437],\n",
      "        [20.9318],\n",
      "        [18.9724],\n",
      "        [24.5895],\n",
      "        [14.1339],\n",
      "        [19.7155],\n",
      "        [12.5216],\n",
      "        [19.8979],\n",
      "        [22.6246],\n",
      "        [15.6771],\n",
      "        [35.2833],\n",
      "        [22.5866],\n",
      "        [11.3772],\n",
      "        [ 6.4877],\n",
      "        [22.9009],\n",
      "        [10.5721],\n",
      "        [34.3735],\n",
      "        [16.6856],\n",
      "        [ 9.0314]])\n",
      "outputs G_r: tensor([[-37.1016],\n",
      "        [-16.9963],\n",
      "        [-38.3448],\n",
      "        [-69.9707],\n",
      "        [-35.6355],\n",
      "        [-25.8058],\n",
      "        [-28.6024],\n",
      "        [-35.3933],\n",
      "        [ -8.4670],\n",
      "        [-25.1943],\n",
      "        [-20.5787],\n",
      "        [-18.3407],\n",
      "        [-40.0235],\n",
      "        [-27.4202],\n",
      "        [-62.7981],\n",
      "        [-20.2258],\n",
      "        [-13.1210],\n",
      "        [-47.8422],\n",
      "        [-35.4244],\n",
      "        [-36.8422],\n",
      "        [-30.4639],\n",
      "        [-19.4671],\n",
      "        [-30.6128],\n",
      "        [  6.2167],\n",
      "        [-22.2059],\n",
      "        [-57.5775],\n",
      "        [-52.3721],\n",
      "        [-23.5200],\n",
      "        [-12.4004],\n",
      "        [  4.5403],\n",
      "        [-34.0879],\n",
      "        [-31.0708]])\n",
      "outputs G_act: tensor([[ 3.2036],\n",
      "        [14.1813],\n",
      "        [26.7242],\n",
      "        [22.6416],\n",
      "        [23.4980],\n",
      "        [22.8109],\n",
      "        [14.2855],\n",
      "        [17.7773],\n",
      "        [27.0401],\n",
      "        [23.4633],\n",
      "        [21.4957],\n",
      "        [16.1104],\n",
      "        [15.3521],\n",
      "        [14.2416],\n",
      "        [17.3035],\n",
      "        [27.8292],\n",
      "        [34.4265],\n",
      "        [16.5588],\n",
      "        [13.7952],\n",
      "        [18.8596],\n",
      "        [16.0604],\n",
      "        [11.1886],\n",
      "        [ 9.6588],\n",
      "        [14.1115],\n",
      "        [12.9229],\n",
      "        [29.8578],\n",
      "        [26.4453],\n",
      "        [30.4445],\n",
      "        [12.1603],\n",
      "        [36.2204],\n",
      "        [13.2229],\n",
      "        [21.0016]])\n",
      "outputs G_r: tensor([[-46.6735],\n",
      "        [-37.1548],\n",
      "        [-13.1492],\n",
      "        [-10.3112],\n",
      "        [-51.9249],\n",
      "        [-23.2406],\n",
      "        [-35.0113],\n",
      "        [-38.8068],\n",
      "        [  3.3317],\n",
      "        [-34.9248],\n",
      "        [ -5.7517],\n",
      "        [-32.0363],\n",
      "        [-32.7185],\n",
      "        [-38.3496],\n",
      "        [-28.4582],\n",
      "        [ -3.2931],\n",
      "        [-14.6649],\n",
      "        [-44.4723],\n",
      "        [-24.5084],\n",
      "        [ -8.4934],\n",
      "        [-38.6990],\n",
      "        [-68.9158],\n",
      "        [-46.9372],\n",
      "        [-48.8179],\n",
      "        [-56.2966],\n",
      "        [-13.8503],\n",
      "        [-15.9181],\n",
      "        [-45.3330],\n",
      "        [-27.9566],\n",
      "        [ 12.1810],\n",
      "        [-67.8201],\n",
      "        [-26.7824]])\n",
      "outputs G_act: tensor([[28.8194],\n",
      "        [ 8.0813],\n",
      "        [18.6279],\n",
      "        [21.8668],\n",
      "        [16.3956],\n",
      "        [11.4152],\n",
      "        [16.9045],\n",
      "        [24.7606],\n",
      "        [28.2522],\n",
      "        [29.4004],\n",
      "        [12.7337],\n",
      "        [17.9442],\n",
      "        [12.8871],\n",
      "        [12.5794],\n",
      "        [16.8462],\n",
      "        [38.8018],\n",
      "        [ 6.5227],\n",
      "        [11.3090],\n",
      "        [21.9414],\n",
      "        [13.8237],\n",
      "        [10.0630],\n",
      "        [31.1372],\n",
      "        [22.0121],\n",
      "        [11.6149],\n",
      "        [17.8915],\n",
      "        [32.9869],\n",
      "        [34.4015],\n",
      "        [ 7.1017],\n",
      "        [23.1880],\n",
      "        [ 8.4072],\n",
      "        [19.5538],\n",
      "        [27.0764]])\n",
      "outputs G_r: tensor([[-12.2439],\n",
      "        [-66.5524],\n",
      "        [-34.6742],\n",
      "        [-41.6441],\n",
      "        [-35.8189],\n",
      "        [-65.5034],\n",
      "        [-34.0065],\n",
      "        [-39.2113],\n",
      "        [  5.2870],\n",
      "        [  3.6227],\n",
      "        [-39.6684],\n",
      "        [-39.2589],\n",
      "        [-43.9919],\n",
      "        [-41.3088],\n",
      "        [-24.7210],\n",
      "        [  7.2425],\n",
      "        [-54.0074],\n",
      "        [-20.7707],\n",
      "        [-38.4856],\n",
      "        [-27.5718],\n",
      "        [-42.4929],\n",
      "        [  5.4066],\n",
      "        [ -9.9153],\n",
      "        [-20.8884],\n",
      "        [-28.9679],\n",
      "        [ 12.8491],\n",
      "        [ -2.3449],\n",
      "        [-40.4347],\n",
      "        [-23.7568],\n",
      "        [-28.2917],\n",
      "        [-42.9654],\n",
      "        [-21.6964]])\n",
      "outputs G_act: tensor([[ 6.1078],\n",
      "        [14.5535],\n",
      "        [12.2575],\n",
      "        [17.1968],\n",
      "        [15.7268],\n",
      "        [19.8968],\n",
      "        [21.1436],\n",
      "        [36.3695],\n",
      "        [16.0907],\n",
      "        [12.1650],\n",
      "        [17.5439],\n",
      "        [13.4374],\n",
      "        [12.4340],\n",
      "        [20.1626],\n",
      "        [15.3594],\n",
      "        [24.1129],\n",
      "        [19.8463],\n",
      "        [30.3597],\n",
      "        [22.3420],\n",
      "        [22.2581],\n",
      "        [12.0454],\n",
      "        [20.8167],\n",
      "        [15.3149],\n",
      "        [16.4910],\n",
      "        [16.5203],\n",
      "        [16.7598],\n",
      "        [21.5171],\n",
      "        [11.5355],\n",
      "        [13.5454],\n",
      "        [16.4609],\n",
      "        [13.2581],\n",
      "        [26.0496]])\n",
      "outputs G_r: tensor([[-62.7461],\n",
      "        [-48.3328],\n",
      "        [-37.4564],\n",
      "        [-75.2476],\n",
      "        [-40.7137],\n",
      "        [-35.3897],\n",
      "        [-14.3020],\n",
      "        [ -3.4004],\n",
      "        [-21.5385],\n",
      "        [-54.1798],\n",
      "        [ -7.9940],\n",
      "        [-55.2516],\n",
      "        [-49.1125],\n",
      "        [-38.2655],\n",
      "        [-76.4794],\n",
      "        [-12.5808],\n",
      "        [-24.7710],\n",
      "        [ -1.8846],\n",
      "        [-37.7658],\n",
      "        [-38.7087],\n",
      "        [-39.5866],\n",
      "        [-15.2834],\n",
      "        [-41.3987],\n",
      "        [-35.6961],\n",
      "        [-33.2997],\n",
      "        [-22.1665],\n",
      "        [-23.6283],\n",
      "        [-80.1005],\n",
      "        [-42.1693],\n",
      "        [-32.3383],\n",
      "        [-38.1988],\n",
      "        [ -6.5516]])\n",
      "outputs G_act: tensor([[25.6638],\n",
      "        [17.0123],\n",
      "        [13.3485],\n",
      "        [22.7634],\n",
      "        [25.1424],\n",
      "        [24.7602],\n",
      "        [ 8.5510],\n",
      "        [23.0631],\n",
      "        [15.9161],\n",
      "        [12.2987],\n",
      "        [11.9884],\n",
      "        [28.9591],\n",
      "        [27.6216],\n",
      "        [15.5437],\n",
      "        [ 6.7204],\n",
      "        [15.4993],\n",
      "        [11.1250],\n",
      "        [18.8200],\n",
      "        [15.5254],\n",
      "        [25.1789],\n",
      "        [ 9.7920],\n",
      "        [15.4786],\n",
      "        [47.6011],\n",
      "        [27.4951],\n",
      "        [21.6417],\n",
      "        [21.5866],\n",
      "        [34.6723],\n",
      "        [29.0182],\n",
      "        [23.6303],\n",
      "        [12.9069]])\n",
      "outputs G_r: tensor([[ -5.2602],\n",
      "        [-35.5808],\n",
      "        [-43.2867],\n",
      "        [-16.1941],\n",
      "        [-25.7478],\n",
      "        [ -3.7289],\n",
      "        [-72.5050],\n",
      "        [-14.8451],\n",
      "        [-11.1056],\n",
      "        [-31.3965],\n",
      "        [-50.6306],\n",
      "        [ -3.9591],\n",
      "        [  6.6928],\n",
      "        [-35.8394],\n",
      "        [-66.4226],\n",
      "        [-43.0522],\n",
      "        [-46.1184],\n",
      "        [-27.6685],\n",
      "        [-17.3871],\n",
      "        [ -1.3435],\n",
      "        [-76.1258],\n",
      "        [-42.0049],\n",
      "        [  6.3485],\n",
      "        [-10.3770],\n",
      "        [-14.3880],\n",
      "        [-18.5812],\n",
      "        [  1.4177],\n",
      "        [ -1.0109],\n",
      "        [-23.9358],\n",
      "        [-61.9548]])\n",
      "Average Test Loss G_act: 3.4454138748573535\n",
      "Average Test Loss G_r: 4.155611775138161\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the testing data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss_G_act = 0.0\n",
    "    test_loss_G_r = 0.0\n",
    "\n",
    "    for inputs, targets_G_act, targets_G_r in test_dataloader:\n",
    "        outputs = model(inputs)\n",
    "        outputs_G_act, outputs_G_r = outputs  # Separate the outputs into two tensors\n",
    "\n",
    "        loss_G_act = criterion(outputs_G_act, targets_G_act.unsqueeze(1))\n",
    "        loss_G_r = criterion(outputs_G_r, targets_G_r.unsqueeze(1))\n",
    "\n",
    "        test_loss_G_act += loss_G_act.item()\n",
    "        test_loss_G_r += loss_G_r.item()\n",
    "\n",
    "        print(f\"outputs G_act: {outputs_G_act}\")\n",
    "        print(f\"outputs G_r: {outputs_G_r}\")\n",
    "\n",
    "    average_test_loss_G_act = test_loss_G_act / len(test_dataloader)\n",
    "    average_test_loss_G_r = test_loss_G_r / len(test_dataloader)\n",
    "\n",
    "    print(f\"Average Test Loss G_act: {average_test_loss_G_act}\")\n",
    "    print(f\"Average Test Loss G_r: {average_test_loss_G_r}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For showing how it works, we will use the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G_act 0     15.875896\n",
      "1     15.155477\n",
      "2     18.013824\n",
      "3     23.687079\n",
      "4     23.438094\n",
      "5     20.969801\n",
      "6     11.722625\n",
      "7      7.268044\n",
      "8      6.289673\n",
      "9     16.044475\n",
      "10    13.604005\n",
      "Name: G_act, dtype: float64 G_r 0    -51.881526\n",
      "1    -51.398681\n",
      "2    -66.822349\n",
      "3    -62.481289\n",
      "4    -84.836459\n",
      "5    -57.785046\n",
      "6    -63.721331\n",
      "7    -86.996102\n",
      "8    -86.799671\n",
      "9    -50.409197\n",
      "10   -51.008126\n",
      "Name: G_r, dtype: float64  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read the dataset\n",
    "df = pd.read_csv('debuging_dataset.csv')\n",
    "\n",
    "# Extract the SMILES strings\n",
    "smiles = df['rxn_smiles']\n",
    "\n",
    "print(f\"G_act {df['G_act']} G_r {df['G_r']}  \")\n",
    "\n",
    "# Encode SMILES strings to fingerprints\n",
    "X_fingerprints = []\n",
    "\n",
    "for idx, smiles in enumerate(smiles):\n",
    "    reactants, products = smiles.split('>>')\n",
    "    reactant_fingerprints = [encode_smiles(reactant) for reactant in reactants.split('.')]\n",
    "    product_fingerprints = [encode_smiles(product) for product in products.split('.')]\n",
    "    X_fingerprints.append(reactant_fingerprints + product_fingerprints)\n",
    "\n",
    "# Convert the list of fingerprints and target values to tensors\n",
    "X_test = torch.stack([torch.cat(fingerprints) for fingerprints in X_fingerprints])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for G_act:\n",
      "14.894965\n",
      "15.17654\n",
      "18.077942\n",
      "23.272684\n",
      "21.9313\n",
      "19.457027\n",
      "11.238342\n",
      "6.3206444\n",
      "7.481103\n",
      "15.930947\n",
      "13.589463\n",
      "Predictions for G_r:\n",
      "-50.983437\n",
      "-50.43715\n",
      "-65.683136\n",
      "-61.65767\n",
      "-82.97296\n",
      "-57.48965\n",
      "-64.0065\n",
      "-86.8749\n",
      "-78.12715\n",
      "-49.251316\n",
      "-50.61\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Make predictions on test inputs\n",
    "with torch.no_grad():\n",
    "    test_inputs = X_test  \n",
    "    predictions_G_act, predictions_G_r = model(test_inputs)  # Separate the predictions into two tensors\n",
    "\n",
    "# Convert predictions to numpy arrays\n",
    "predictions_G_act = predictions_G_act.numpy()\n",
    "predictions_G_r = predictions_G_r.numpy()\n",
    "\n",
    "\n",
    "print(\"Predictions for G_act:\")\n",
    "for pred in predictions_G_act:\n",
    "    print(pred[0])\n",
    "\n",
    "print(\"Predictions for G_r:\")\n",
    "for pred in predictions_G_r:\n",
    "    print(pred[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
